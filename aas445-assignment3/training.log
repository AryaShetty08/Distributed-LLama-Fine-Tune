[2024-11-12 12:18:58,603] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
***************************************************************************
                          NOTICE TO USERS

Lawrence Berkeley National Laboratory operates this computer system under 
contract to the U.S. Department of Energy.  This computer system is the 
property of the United States Government and is for authorized use only.
Users (authorized or unauthorized) have no explicit or implicit 
expectation of privacy.

Any or all uses of this system and all files on this system may be
intercepted, monitored, recorded, copied, audited, inspected, and disclosed
to authorized site, Department of Energy, and law enforcement personnel,
as well as authorized officials of other agencies, both domestic and foreign.
By using this system, the user consents to such interception, monitoring,
recording, copying, auditing, inspection, and disclosure at the discretion
of authorized site or Department of Energy personnel.

Unauthorized or improper use of this system may result in administrative
disciplinary action and civil and criminal penalties. By continuing to use
this system you indicate your awareness of and consent to these terms and
conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
stated in this warning.

*****************************************************************************

Login connection to host x1000c7s4b0n1:

[2024-11-12 12:19:04,545] [INFO] [runner.py:496:main] Using IP address of 10.100.0.28 for node nid001241
[2024-11-12 12:19:04,550] [INFO] [multinode_runner.py:81:get_cmd] Running on the following workers: nid001241,nid001244
[2024-11-12 12:19:04,550] [INFO] [runner.py:607:main] cmd = pdsh -S -f 1024 -w nid001241,nid001244 export PYTHONPATH="/pscratch/sd/a/aas445/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning:/opt/nersc/pymon"; export NCCL_NET_GDR_LEVEL="PHB"; export NCCL_SOCKET_IFNAME="hsn";  cd /pscratch/sd/a/aas445/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning; /pscratch/sd/a/aas445/venv/env1/bin/python -u -m deepspeed.launcher.launch --world_info=eyJuaWQwMDEyNDEiOiBbMCwgMSwgMiwgM10sICJuaWQwMDEyNDQiOiBbMCwgMSwgMiwgM119 --node_rank=%n --master_addr=10.100.0.28 --master_port=29500 main_ckpt.py --sft_only_data_path /pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9 --model_name_or_path /pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --max_seq_len 1024 --learning_rate 9.65e-6 --weight_decay 0. --num_train_epochs 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --gradient_checkpointing --zero_stage 3 --deepspeed --lora_dim 0 --lora_module_name layers. --output_dir ./output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847 --print_loss --save_interval 2000 --enable_tensorboard --tensorboard_path ./output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard
nid001241: ***************************************************************************
nid001241:                           NOTICE TO USERS
nid001241: 
nid001241: Lawrence Berkeley National Laboratory operates this computer system under 
nid001241: contract to the U.S. Department of Energy.  This computer system is the 
nid001241: property of the United States Government and is for authorized use only.
nid001241: Users (authorized or unauthorized) have no explicit or implicit 
nid001241: expectation of privacy.
nid001241: 
nid001241: Any or all uses of this system and all files on this system may be
nid001241: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001241: to authorized site, Department of Energy, and law enforcement personnel,
nid001241: as well as authorized officials of other agencies, both domestic and foreign.
nid001241: By using this system, the user consents to such interception, monitoring,
nid001241: recording, copying, auditing, inspection, and disclosure at the discretion
nid001241: of authorized site or Department of Energy personnel.
nid001241: 
nid001241: Unauthorized or improper use of this system may result in administrative
nid001241: disciplinary action and civil and criminal penalties. By continuing to use
nid001241: this system you indicate your awareness of and consent to these terms and
nid001241: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001241: stated in this warning.
nid001241: 
nid001241: *****************************************************************************
nid001241: 
nid001241: Login connection to host x1000c7s4b0n1:
nid001241: 
nid001244: ***************************************************************************
nid001244:                           NOTICE TO USERS
nid001244: 
nid001244: Lawrence Berkeley National Laboratory operates this computer system under 
nid001244: contract to the U.S. Department of Energy.  This computer system is the 
nid001244: property of the United States Government and is for authorized use only.
nid001244: Users (authorized or unauthorized) have no explicit or implicit 
nid001244: expectation of privacy.
nid001244: 
nid001244: Any or all uses of this system and all files on this system may be
nid001244: intercepted, monitored, recorded, copied, audited, inspected, and disclosed
nid001244: to authorized site, Department of Energy, and law enforcement personnel,
nid001244: as well as authorized officials of other agencies, both domestic and foreign.
nid001244: By using this system, the user consents to such interception, monitoring,
nid001244: recording, copying, auditing, inspection, and disclosure at the discretion
nid001244: of authorized site or Department of Energy personnel.
nid001244: 
nid001244: Unauthorized or improper use of this system may result in administrative
nid001244: disciplinary action and civil and criminal penalties. By continuing to use
nid001244: this system you indicate your awareness of and consent to these terms and
nid001244: conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions
nid001244: stated in this warning.
nid001244: 
nid001244: *****************************************************************************
nid001244: 
nid001244: Login connection to host x1000c7s5b0n0:
nid001244: 
nid001241: [2024-11-12 12:19:09,144] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:139:main] 0 NCCL_NET_GDR_LEVEL=PHB
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:139:main] 0 NCCL_SOCKET_IFNAME=hsn
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:146:main] WORLD INFO DICT: {'nid001241': [0, 1, 2, 3], 'nid001244': [0, 1, 2, 3]}
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:152:main] nnodes=2, num_local_procs=4, node_rank=0
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001241': [0, 1, 2, 3], 'nid001244': [4, 5, 6, 7]})
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:164:main] dist_world_size=8
nid001241: [2024-11-12 12:19:10,937] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001241: [2024-11-12 12:19:10,947] [INFO] [launch.py:256:main] process 1885752 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=0', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001241: [2024-11-12 12:19:10,956] [INFO] [launch.py:256:main] process 1885753 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=1', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001241: [2024-11-12 12:19:10,966] [INFO] [launch.py:256:main] process 1885754 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=2', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001241: [2024-11-12 12:19:10,977] [INFO] [launch.py:256:main] process 1885755 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=3', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001244: [2024-11-12 12:19:16,177] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001241: [2024-11-12 12:19:18,149] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001241: [2024-11-12 12:19:18,174] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001241: [2024-11-12 12:19:18,215] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001241: [2024-11-12 12:19:18,215] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:139:main] 1 NCCL_NET_GDR_LEVEL=PHB
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:139:main] 1 NCCL_SOCKET_IFNAME=hsn
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:146:main] WORLD INFO DICT: {'nid001241': [0, 1, 2, 3], 'nid001244': [0, 1, 2, 3]}
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:152:main] nnodes=2, num_local_procs=4, node_rank=1
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'nid001241': [0, 1, 2, 3], 'nid001244': [4, 5, 6, 7]})
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:164:main] dist_world_size=8
nid001244: [2024-11-12 12:19:20,243] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
nid001244: [2024-11-12 12:19:20,256] [INFO] [launch.py:256:main] process 1803887 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=0', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001244: [2024-11-12 12:19:20,268] [INFO] [launch.py:256:main] process 1803888 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=1', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001244: [2024-11-12 12:19:20,279] [INFO] [launch.py:256:main] process 1803889 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=2', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001244: [2024-11-12 12:19:20,290] [INFO] [launch.py:256:main] process 1803890 spawned with command: ['/pscratch/sd/a/aas445/venv/env1/bin/python', '-u', 'main_ckpt.py', '--local_rank=3', '--sft_only_data_path', '/pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9', '--model_name_or_path', '/pscratch/sd/a/aas445/hf_cache/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--max_seq_len', '1024', '--learning_rate', '9.65e-6', '--weight_decay', '0.', '--num_train_epochs', '1', '--gradient_accumulation_steps', '1', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '0', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--lora_dim', '0', '--lora_module_name', 'layers.', '--output_dir', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847', '--print_loss', '--save_interval', '2000', '--enable_tensorboard', '--tensorboard_path', './output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard']
nid001241: [2024-11-12 12:19:20,314] [INFO] [comm.py:652:init_distributed] cdb=None
nid001241: [2024-11-12 12:19:20,314] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
nid001241: [2024-11-12 12:19:20,930] [INFO] [comm.py:652:init_distributed] cdb=None
nid001241: [2024-11-12 12:19:20,931] [INFO] [comm.py:652:init_distributed] cdb=None
nid001241: [2024-11-12 12:19:20,932] [INFO] [comm.py:652:init_distributed] cdb=None
nid001244: [2024-11-12 12:19:27,518] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001244: [2024-11-12 12:19:27,533] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001244: [2024-11-12 12:19:27,573] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001244: [2024-11-12 12:19:27,576] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
nid001244: [2024-11-12 12:19:29,597] [INFO] [comm.py:652:init_distributed] cdb=None
nid001244: [2024-11-12 12:19:30,367] [INFO] [comm.py:652:init_distributed] cdb=None
nid001244: [2024-11-12 12:19:30,370] [INFO] [comm.py:652:init_distributed] cdb=None
nid001244: [2024-11-12 12:19:30,375] [INFO] [comm.py:652:init_distributed] cdb=None
nid001244: [2024-11-12 12:19:33,113] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: [2024-11-12 12:19:33,113] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: [2024-11-12 12:19:33,113] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: [2024-11-12 12:19:33,113] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:19:33,125] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:19:33,125] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:19:33,125] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:19:33,125] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:19:34,678] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
nid001244: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.33s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.53s/it]
nid001244: Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.53s/it]
nid001244: Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.53s/it]
nid001244: Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.54s/it]
nid001241: Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:33, 11.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:12,  6.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:21<00:07,  7.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.55s/it]
nid001241: Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.55s/it]
nid001241: Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.55s/it]
nid001241: Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  7.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.49s/it]
nid001241: Creating prompt dataset ['Dahoas/rm-static'], reload=False
nid001241: HELPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
nid001241: Dahoas/rm-static
nid001244: Creating prompt dataset ['Dahoas/rm-static'], reload=False
nid001244: HELPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
nid001244: Dahoas/rm-static
nid001241: ended
nid001244: ended
nid001241: Creating dataset Dahoas_rm_static for train_phase=1 size=15252
nid001244: Creating dataset Dahoas_rm_static for train_phase=1 size=15252
nid001241: Creating dataset Dahoas_rm_static for train_phase=1 size=1021
nid001241: HELPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
nid001241: /pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9
nid001244: Creating dataset Dahoas_rm_static for train_phase=1 size=1021
nid001244: HELPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP
nid001244: /pscratch/sd/a/aas445/hf_cache/hub/datasets--Dahoas--rm-static/snapshots/64fd53cc91f7cb73b283a6e4f661205e277d23c9
nid001244: ended
nid001241: ended
nid001241: Creating dataset Dahoas_rm_static for train_phase=1 size=76256
nid001244: Creating dataset Dahoas_rm_static for train_phase=1 size=76256
nid001241: Creating dataset Dahoas_rm_static for train_phase=1 size=5103
nid001244: Creating dataset Dahoas_rm_static for train_phase=1 size=5103
nid001241: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001241: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001241: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001244: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001244: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001244: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001241: Detected CUDA files, patching ldflags
nid001241: Emitting ninja build file /global/u2/a/aas445/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
nid001241: Building extension module fused_adam...
nid001241: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001241: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001241: ninja: no work to do.
nid001241: Loading extension module fused_adam...
nid001241: Time to load fused_adam op: 1.322155475616455 seconds
nid001241: [2024-11-12 12:22:17,906] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: Loading extension module fused_adam...
nid001241: Time to load fused_adam op: 0.2071998119354248 seconds
nid001241: [2024-11-12 12:22:17,909] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown
nid001241: [2024-11-12 12:22:17,909] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
nid001241: [2024-11-12 12:22:17,909] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: Loading extension module fused_adam...
nid001244: Loading extension module fused_adam...
nid001241: Loading extension module fused_adam...
nid001241: Time to load fused_adam op: 1.2189586162567139 seconds
nid001241: [2024-11-12 12:22:17,935] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: Time to load fused_adam op: 0.5317761898040771 seconds
nid001244: Time to load fused_adam op: 1.034038782119751 seconds
nid001244: [2024-11-12 12:22:17,937] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: [2024-11-12 12:22:17,937] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001244: Loading extension module fused_adam...
nid001244: Time to load fused_adam op: 0.8127472400665283 seconds
nid001244: [2024-11-12 12:22:17,959] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: Loading extension module fused_adam...
nid001241: Time to load fused_adam op: 1.3169162273406982 seconds
nid001241: [2024-11-12 12:22:17,977] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:22:18,190] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
nid001241: [2024-11-12 12:22:18,191] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
nid001241: [2024-11-12 12:22:18,191] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
nid001241: [2024-11-12 12:22:18,200] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
nid001241: [2024-11-12 12:22:18,200] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
nid001241: [2024-11-12 12:22:18,200] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
nid001241: [2024-11-12 12:22:18,200] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
nid001241: [2024-11-12 12:22:18,735] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
nid001241: [2024-11-12 12:22:18,737] [INFO] [utils.py:782:see_memory_usage] MA 3.58 GB         Max_MA 5.78 GB         CA 17.94 GB         Max_CA 18 GB 
nid001241: [2024-11-12 12:22:18,737] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.58 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:18,739] [INFO] [stage3.py:166:__init__] Reduce bucket size 500000000
nid001241: [2024-11-12 12:22:18,739] [INFO] [stage3.py:167:__init__] Prefetch bucket size 30000000
nid001244: Using /global/u2/a/aas445/.cache/torch_extensions/py39_cu121 as PyTorch extensions root...
nid001241: [2024-11-12 12:22:19,237] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
nid001241: [2024-11-12 12:22:19,237] [INFO] [utils.py:782:see_memory_usage] MA 3.58 GB         Max_MA 3.58 GB         CA 17.94 GB         Max_CA 18 GB 
nid001241: [2024-11-12 12:22:19,237] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.6 GB, percent = 46.4%
nid001241: Parameter Offload: Total persistent parameters: 266240 in 65 params
nid001244: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
nid001244: To disable this warning, you can either:
nid001244: 	- Avoid using `tokenizers` before the fork if possible
nid001244: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
nid001241: [2024-11-12 12:22:19,771] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
nid001241: [2024-11-12 12:22:19,772] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 3.71 GB         CA 17.94 GB         Max_CA 18 GB 
nid001241: [2024-11-12 12:22:19,772] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.73 GB, percent = 46.4%
nid001244: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
nid001244: To disable this warning, you can either:
nid001244: 	- Avoid using `tokenizers` before the fork if possible
nid001244: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
nid001241: [2024-11-12 12:22:20,281] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
nid001241: [2024-11-12 12:22:20,282] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 17.94 GB         Max_CA 18 GB 
nid001241: [2024-11-12 12:22:20,282] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.73 GB, percent = 46.4%
nid001244: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
nid001244: To disable this warning, you can either:
nid001244: 	- Avoid using `tokenizers` before the fork if possible
nid001244: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
nid001244: Detected CUDA files, patching ldflags
nid001244: Emitting ninja build file /global/u2/a/aas445/.cache/torch_extensions/py39_cu121/fused_adam/build.ninja...
nid001244: Building extension module fused_adam...
nid001244: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
nid001244: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
nid001244: To disable this warning, you can either:
nid001244: 	- Avoid using `tokenizers` before the fork if possible
nid001244: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
nid001244: ninja: no work to do.
nid001244: Loading extension module fused_adam...
nid001244: Time to load fused_adam op: 2.278517723083496 seconds
nid001244: [2024-11-12 12:22:21,501] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8
nid001241: [2024-11-12 12:22:23,786] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
nid001241: [2024-11-12 12:22:23,787] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 1.87 GB         Max_CA 18 GB 
nid001241: [2024-11-12 12:22:23,787] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.65 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:24,284] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
nid001241: [2024-11-12 12:22:24,285] [INFO] [utils.py:782:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 1.87 GB         Max_CA 2 GB 
nid001241: [2024-11-12 12:22:24,285] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.65 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:24,790] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
nid001241: [2024-11-12 12:22:24,791] [INFO] [utils.py:782:see_memory_usage] MA 5.61 GB         Max_MA 7.48 GB         CA 7.48 GB         Max_CA 7 GB 
nid001241: [2024-11-12 12:22:24,791] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.65 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:25,287] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
nid001241: [2024-11-12 12:22:25,288] [INFO] [utils.py:782:see_memory_usage] MA 5.61 GB         Max_MA 5.61 GB         CA 7.48 GB         Max_CA 7 GB 
nid001241: [2024-11-12 12:22:25,288] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.65 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:25,787] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
nid001241: [2024-11-12 12:22:25,787] [INFO] [utils.py:782:see_memory_usage] MA 5.61 GB         Max_MA 9.35 GB         CA 11.22 GB         Max_CA 11 GB 
nid001241: [2024-11-12 12:22:25,788] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.65 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:25,788] [INFO] [stage3.py:521:_setup_for_real_optimizer] optimizer state initialized
nid001241: 0,192
nid001241: 0,192
nid001241: 0,192
nid001244: 0,192
nid001244: 0,192
nid001244: 0,192
nid001244: 0,192
nid001241: [2024-11-12 12:22:28,696] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
nid001241: [2024-11-12 12:22:28,696] [INFO] [utils.py:782:see_memory_usage] MA 8.41 GB         Max_MA 10.37 GB         CA 17.99 GB         Max_CA 18 GB 
nid001241: [2024-11-12 12:22:28,697] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 116.56 GB, percent = 46.4%
nid001241: [2024-11-12 12:22:28,697] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
nid001241: [2024-11-12 12:22:28,697] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
nid001241: [2024-11-12 12:22:28,697] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f5d880aedc0>
nid001241: [2024-11-12 12:22:28,697] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[9.65e-06, 9.65e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:22:28,698] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
nid001241:     "partition_activations": false, 
nid001241:     "contiguous_memory_optimization": false, 
nid001241:     "cpu_checkpointing": false, 
nid001241:     "number_checkpoints": null, 
nid001241:     "synchronize_checkpoint_boundary": false, 
nid001241:     "profile": false
nid001241: }
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   amp_enabled .................. False
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   amp_params ................... False
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   autotuning_config ............ {
nid001241:     "enabled": false, 
nid001241:     "start_step": null, 
nid001241:     "end_step": null, 
nid001241:     "metric_path": null, 
nid001241:     "arg_mappings": null, 
nid001241:     "metric": "throughput", 
nid001241:     "model_info": null, 
nid001241:     "results_dir": "autotuning_results", 
nid001241:     "exps_dir": "autotuning_exps", 
nid001241:     "overwrite": true, 
nid001241:     "fast": true, 
nid001241:     "start_profile_step": 3, 
nid001241:     "end_profile_step": 5, 
nid001241:     "tuner_type": "gridsearch", 
nid001241:     "tuner_early_stopping": 5, 
nid001241:     "tuner_num_trials": 50, 
nid001241:     "model_info_path": null, 
nid001241:     "mp_size": 1, 
nid001241:     "max_train_batch_size": null, 
nid001241:     "min_train_batch_size": 1, 
nid001241:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
nid001241:     "min_train_micro_batch_size_per_gpu": 1, 
nid001241:     "num_tuning_micro_batch_sizes": 3
nid001241: }
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
nid001241: [2024-11-12 12:22:28,699] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5d880aefd0>
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   communication_data_type ...... None
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   disable_allgather ............ False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   dump_state ................... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
nid001241:     "enabled": false, 
nid001241:     "recompute_fwd_factor": 0.0, 
nid001241:     "profile_step": 1, 
nid001241:     "module_depth": -1, 
nid001241:     "top_modules": 1, 
nid001241:     "detailed": true, 
nid001241:     "output_file": null
nid001241: }
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   fp16_enabled ................. True
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   global_rank .................. 0
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   graph_harvesting ............. False
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
nid001241: [2024-11-12 12:22:28,700] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   loss_scale ................... 0
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   memory_breakdown ............. False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='./output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard/ds_tensorboard_logs/', job_name='step1_model_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   nebula_config ................ {
nid001241:     "enabled": false, 
nid001241:     "persistent_storage_path": null, 
nid001241:     "persistent_time_interval": 100, 
nid001241:     "num_of_version_in_retention": 2, 
nid001241:     "enable_nebula_load": true, 
nid001241:     "load_path": null
nid001241: }
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   optimizer_name ............... None
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   optimizer_params ............. None
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   pld_enabled .................. False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   pld_params ................... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   prescale_gradients ........... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   scheduler_name ............... None
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   scheduler_params ............. None
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   sparse_attention ............. None
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   steps_per_print .............. 10
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   train_batch_size ............. 32
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  4
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   weight_quantization_config ... None
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   world_size ................... 8
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   zero_enabled ................. True
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
nid001241: [2024-11-12 12:22:28,701] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
nid001241: [2024-11-12 12:22:28,702] [INFO] [config.py:989:print_user_config]   json = {
nid001241:     "train_batch_size": 32, 
nid001241:     "train_micro_batch_size_per_gpu": 4, 
nid001241:     "steps_per_print": 10, 
nid001241:     "zero_optimization": {
nid001241:         "stage": 3, 
nid001241:         "overlap_comm": true, 
nid001241:         "offload_param": {
nid001241:             "device": "none"
nid001241:         }, 
nid001241:         "offload_optimizer": {
nid001241:             "device": "none"
nid001241:         }, 
nid001241:         "stage3_param_persistence_threshold": 1.000000e+04, 
nid001241:         "stage3_max_live_parameters": 3.000000e+07, 
nid001241:         "stage3_prefetch_bucket_size": 3.000000e+07, 
nid001241:         "memory_efficient_linear": false
nid001241:     }, 
nid001241:     "fp16": {
nid001241:         "enabled": true, 
nid001241:         "loss_scale_window": 100
nid001241:     }, 
nid001241:     "gradient_clipping": 1.0, 
nid001241:     "prescale_gradients": false, 
nid001241:     "wall_clock_breakdown": false, 
nid001241:     "hybrid_engine": {
nid001241:         "enabled": false, 
nid001241:         "max_out_tokens": 512, 
nid001241:         "inference_tp_size": 1, 
nid001241:         "release_inference_cache": false, 
nid001241:         "pin_parameters": true, 
nid001241:         "tp_gather_partition_size": 8
nid001241:     }, 
nid001241:     "tensorboard": {
nid001241:         "enabled": true, 
nid001241:         "output_path": "./output_Llama-3.1-8B-Instruct_epoch1_train_batch_size4_seq1024_lora0_zero3_20241112-121847/step1_tensorboard/ds_tensorboard_logs/", 
nid001241:         "job_name": "step1_model_tensorboard"
nid001241:     }
nid001241: }
nid001241: ***** Running training *****
nid001241: ***** Evaluating perplexity, Epoch 0/1 *****
nid001241: 0,192
nid001244: 1,192
nid001244: 1,192
nid001244: 1,192
nid001244: 1,192
nid001241: 1,192
nid001241: 1,192
nid001241: 1,192
nid001241: 1,192
nid001244: 2,192
nid001244: 2,1922,192
nid001244: 
nid001244: 2,192
nid001241: 2,192
nid001241: 2,192
nid001241: 2,192
nid001241: 2,192
nid001244: 3,192
nid001244: 3,192
nid001241: 3,192
nid001244: 3,192
nid001241: 3,192
nid001244: 3,192
nid001241: 3,192
nid001241: 3,192
nid001241: 4,192
nid001244: 4,192
nid001241: 4,192
nid001244: 4,192
nid001244: 4,192
nid001244: 4,192
nid001241: 4,192
nid001241: 4,192
nid001241: 5,192
nid001244: 5,192
nid001241: 5,192
nid001244: 5,192
nid001241: 5,192
nid001241: 5,192
nid001244: 5,192
nid001244: 5,192
nid001244: 6,192
nid001244: 6,192
nid001241: 6,192
nid001244: 6,192
nid001244: 6,192
nid001241: 6,192
nid001241: 6,192
nid001241: 6,192
nid001244: 7,192
nid001241: 7,192
nid001241: 7,192
nid001244: 7,192
nid001244: 7,192
nid001244: 7,192
nid001241: 7,192
nid001241: 7,192
nid001241: 8,192
nid001244: 8,192
nid001241: 8,192
nid001244: 8,192
nid001244: 8,192
nid001241: 8,192
nid001244: 8,192
nid001241: 8,192
nid001241: 9,192
nid001244: 9,192
nid001241: 9,192
nid001244: 9,192
nid001241: 9,192
nid001244: 9,192
nid001244: 9,192
nid001241: 9,192
nid001244: 10,192
nid001244: 10,192
nid001241: 10,192
nid001244: 10,192
nid001241: 10,192
nid001244: 10,192
nid001241: 10,192
nid001241: 10,192
nid001244: 11,192
nid001241: 11,192
nid001244: 11,192
nid001241: 11,192
nid001241: 11,192
nid001244: 11,192
nid001244: 11,192
nid001241: 11,192
nid001244: 12,192
nid001241: 12,192
nid001241: 12,192
nid001244: 12,192
nid001241: 12,192
nid001241: 12,192
nid001244: 12,192
nid001244: 12,192
nid001244: 13,192
nid001244: 13,192
nid001241: 13,192
nid001241: 13,192
nid001241: 13,192
nid001241: 13,192
nid001244: 13,192
nid001244: 13,192
nid001241: 14,192
nid001241: 14,192
nid001244: 14,192
nid001244: 14,192
nid001241: 14,192
nid001244: 14,192
nid001241: 14,192
nid001244: 14,192
nid001244: 15,192
nid001244: 15,192
nid001244: 15,192
nid001241: 15,192
nid001241: 15,192
nid001241: 15,192
nid001241: 15,192
nid001244: 15,192
nid001241: 16,192
nid001244: 16,192
nid001244: 16,192
nid001241: 16,192
nid001241: 16,192
nid001241: 16,192
nid001244: 16,192
nid001244: 16,192
nid001244: 17,192
nid001241: 17,192
nid001244: 17,192
nid001241: 17,192
nid001241: 17,192
nid001241: 17,192
nid001244: 17,192
nid001244: 17,192
nid001241: 18,192
nid001244: 18,192
nid001244: 18,192
nid001241: 18,192
nid001241: 18,192
nid001241: 18,192
nid001244: 18,192
nid001244: 18,192
nid001241: 19,192
nid001244: 19,192
nid001241: 19,192
nid001244: 19,192
nid001241: 19,192
nid001244: 19,192
nid001241: 19,192
nid001244: 19,192
nid001241: 20,192
nid001244: 20,192
nid001241: 20,192
nid001244: 20,192
nid001244: 20,192
nid001244: 20,192
nid001241: 20,192
nid001241: 20,192
nid001244: 21,192
nid001241: 21,192
nid001241: 21,192
nid001244: 21,192
nid001244: 21,192
nid001241: 21,192
nid001241: 21,192
nid001244: 21,192
nid001244: 22,192
nid001244: 22,192
nid001241: 22,192
nid001241: 22,192
nid001241: 22,192
nid001241: 22,192
nid001244: 22,192
nid001244: 22,192
nid001244: 23,192
nid001244: 23,192
nid001241: 23,192
nid001241: 23,192
nid001244: 23,192
nid001244: 23,192
nid001241: 23,19223,192
nid001241: 
nid001244: 24,192
nid001241: 24,192
nid001241: 24,192
nid001244: 24,192
nid001244: 24,192
nid001244: 24,192
nid001241: 24,192
nid001241: 24,192
nid001244: 25,192
nid001241: 25,192
nid001244: 25,192
nid001244: 25,192
nid001244: 25,192
nid001241: 25,192
nid001241: 25,192
nid001241: 25,192
nid001241: 26,192
nid001241: 26,192
nid001244: 26,192
nid001244: 26,192
nid001244: 26,192
nid001244: 26,192
nid001241: 26,192
nid001241: 26,192
nid001244: 27,192
nid001244: 27,192
nid001241: 27,192
nid001241: 27,192
nid001244: 27,192
nid001241: 27,192
nid001244: 27,192
nid001241: 27,192
nid001244: 28,192
nid001241: 28,192
nid001244: 28,192
nid001241: 28,192
nid001241: 28,192
nid001244: 28,192
nid001244: 28,192
nid001241: 28,192
nid001244: 29,192
nid001244: 29,192
nid001241: 29,192
nid001241: 29,192
nid001241: 29,192
nid001244: 29,192
nid001244: 29,192
nid001241: 29,192
nid001244: 30,192
nid001241: 30,192
nid001241: 30,192
nid001244: 30,192
nid001244: 30,192
nid001241: 30,192
nid001244: 30,192
nid001241: 30,192
nid001244: 31,192
nid001241: 31,192
nid001241: 31,192
nid001244: 31,192
nid001241: 31,192
nid001241: 31,192
nid001244: 31,192
nid001244: 31,192
nid001244: 32,192
nid001244: 32,192
nid001241: 32,192
nid001241: 32,192
nid001241: 32,192
nid001241: 32,192
nid001244: 32,192
nid001244: 32,192
nid001244: 33,192
nid001244: 33,192
nid001244: 33,192
nid001244: 33,192
nid001241: 33,192
nid001241: 33,192
nid001241: 33,192
nid001241: 33,192
nid001244: 34,192
nid001241: 34,192
nid001244: 34,192
nid001241: 34,192
nid001244: 34,192
nid001244: 34,192
nid001241: 34,192
nid001241: 34,192
nid001244: 35,192
nid001241: 35,192
nid001244: 35,192
nid001241: 35,192
nid001241: 35,192
nid001244: 35,192
nid001244: 35,192
nid001241: 35,192
nid001241: 36,192
nid001241: 36,192
nid001244: 36,192
nid001241: 36,192
nid001244: 36,192
nid001241: 36,192
nid001244: 36,192
nid001244: 36,192
nid001241: 37,192
nid001244: 37,192
nid001241: 37,192
nid001244: 37,192
nid001244: 37,192
nid001244: 37,192
nid001241: 37,192
nid001241: 37,192
nid001241: 38,192
nid001241: 38,192
nid001244: 38,192
nid001244: 38,192
nid001244: 38,192
nid001244: 38,192
nid001241: 38,192
nid001241: 38,192
nid001244: 39,192
nid001244: 39,192
nid001241: 39,192
nid001241: 39,192
nid001244: 39,192
nid001244: 39,192
nid001241: 39,192
nid001241: 39,192
nid001241: 40,192
nid001244: 40,192
nid001244: 40,192
nid001241: 40,192
nid001241: 40,192
nid001241: 40,192
nid001244: 40,192
nid001244: 40,192
nid001241: 41,192
nid001244: 41,192
nid001241: 41,192
nid001241: 41,192
nid001241: 41,192
nid001244: 41,192
nid001244: 41,192
nid001244: 41,192
nid001241: 42,192
nid001241: 42,192
nid001244: 42,192
nid001241: 42,192
nid001241: 42,192
nid001244: 42,192
nid001244: 42,192
nid001244: 42,192
nid001244: 43,192
nid001244: 43,192
nid001244: 43,192
nid001241: 43,192
nid001244: 43,192
nid001241: 43,192
nid001241: 43,192
nid001241: 43,192
nid001244: 44,192
nid001244: 44,192
nid001241: 44,192
nid001244: 44,192
nid001244: 44,192
nid001241: 44,192
nid001241: 44,192
nid001241: 44,192
nid001244: 45,192
nid001244: 45,192
nid001244: 45,192
nid001241: 45,192
nid001241: 45,192
nid001244: 45,192
nid001241: 45,192
nid001241: 45,192
nid001244: 46,192
nid001241: 46,192
nid001244: 46,192
nid001244: 46,192
nid001241: 46,192
nid001244: 46,192
nid001241: 46,192
nid001241: 46,192
nid001241: 47,19247,192
nid001241: 
nid001244: 47,192
nid001244: 47,192
nid001244: 47,192
nid001241: 47,192
nid001244: 47,192
nid001241: 47,192
nid001244: 48,192
nid001244: 48,192
nid001241: 48,192
nid001241: 48,192
nid001244: 48,192
nid001241: 48,192
nid001244: 48,192
nid001241: 48,192
nid001244: 49,192
nid001244: 49,192
nid001241: 49,192
nid001241: 49,192
nid001244: 49,192
nid001244: 49,192
nid001241: 49,192
nid001241: 49,192
nid001244: 50,192
nid001244: 50,192
nid001241: 50,192
nid001244: 50,192
nid001244: 50,192
nid001241: 50,192
nid001241: 50,192
nid001241: 50,192
nid001241: 51,192
nid001244: 51,192
nid001244: 51,192
nid001244: 51,192
nid001241: 51,192
nid001244: 51,192
nid001241: 51,192
nid001241: 51,192
nid001244: 52,192
nid001241: 52,192
nid001244: 52,192
nid001241: 52,192
nid001244: 52,192
nid001244: 52,192
nid001241: 52,192
nid001241: 52,192
nid001244: 53,192
nid001241: 53,192
nid001241: 53,192
nid001244: 53,192
nid001244: 53,192
nid001244: 53,192
nid001241: 53,192
nid001241: 53,192
nid001241: 54,192
nid001244: 54,192
nid001244: 54,192
nid001241: 54,192
nid001244: 54,192
nid001244: 54,192
nid001241: 54,192
nid001241: 54,192
nid001244: 55,192
nid001241: 55,192
nid001244: 55,192
nid001244: 55,192
nid001241: 55,192
nid001244: 55,192
nid001241: 55,192
nid001241: 55,192
nid001241: 56,192
nid001241: 56,192
nid001244: 56,192
nid001244: 56,192
nid001244: 56,192
nid001244: 56,192
nid001241: 56,192
nid001241: 56,192
nid001244: 57,192
nid001241: 57,192
nid001244: 57,192
nid001244: 57,192
nid001241: 57,192
nid001244: 57,192
nid001241: 57,192
nid001241: 57,192
nid001244: 58,192
nid001244: 58,192
nid001241: 58,192
nid001244: 58,192
nid001241: 58,192
nid001244: 58,192
nid001241: 58,192
nid001241: 58,192
nid001244: 59,192
nid001241: 59,192
nid001244: 59,192
nid001241: 59,192
nid001244: 59,192
nid001244: 59,192
nid001241: 59,192
nid001241: 59,192
nid001244: 60,192
nid001241: 60,192
nid001244: 60,192
nid001241: 60,192
nid001244: 60,192
nid001241: 60,192
nid001244: 60,192
nid001241: 60,192
nid001244: 61,192
nid001241: 61,192
nid001241: 61,192
nid001244: 61,192
nid001241: 61,192
nid001244: 61,192
nid001244: 61,192
nid001241: 61,192
nid001244: 62,192
nid001244: 62,192
nid001241: 62,192
nid001241: 62,192
nid001244: 62,192
nid001244: 62,192
nid001241: 62,192
nid001241: 62,192
nid001241: 63,192
nid001244: 63,192
nid001241: 63,192
nid001244: 63,192
nid001244: 63,192
nid001241: 63,192
nid001244: 63,192
nid001241: 63,192
nid001241: 64,192
nid001241: 64,192
nid001244: 64,192
nid001244: 64,192
nid001244: 64,192
nid001244: 64,192
nid001241: 64,192
nid001241: 64,192
nid001241: 65,192
nid001241: 65,192
nid001244: 65,192
nid001244: 65,192
nid001244: 65,192
nid001244: 65,192
nid001241: 65,192
nid001241: 65,192
nid001244: 66,192
nid001241: 66,192
nid001244: 66,192
nid001241: 66,192
nid001244: 66,192
nid001241: 66,192
nid001244: 66,192
nid001241: 66,192
nid001244: 67,192
nid001244: 67,192
nid001244: 67,192
nid001241: 67,192
nid001244: 67,192
nid001241: 67,192
nid001241: 67,192
nid001241: 67,192
nid001244: 68,192
nid001241: 68,192
nid001241: 68,192
nid001244: 68,192
nid001244: 68,192
nid001244: 68,192
nid001241: 68,192
nid001241: 68,192
nid001244: 69,192
nid001244: 69,192
nid001244: 69,192
nid001241: 69,192
nid001244: 69,192
nid001241: 69,192
nid001241: 69,19269,192
nid001241: 
nid001244: 70,192
nid001244: 70,192
nid001241: 70,192
nid001244: 70,192
nid001241: 70,192
nid001244: 70,192
nid001241: 70,192
nid001241: 70,192
nid001241: 71,192
nid001241: 71,192
nid001244: 71,192
nid001244: 71,192
nid001241: 71,192
nid001244: 71,192
nid001244: 71,192
nid001241: 71,192
nid001241: 72,192
nid001244: 72,192
nid001244: 72,192
nid001241: 72,192
nid001244: 72,192
nid001244: 72,192
nid001241: 72,192
nid001241: 72,192
nid001244: 73,192
nid001241: 73,192
nid001244: 73,192
nid001244: 73,192
nid001244: 73,192
nid001241: 73,192
nid001241: 73,192
nid001241: 73,192
nid001241: 74,192
nid001244: 74,192
nid001244: 74,192
nid001241: 74,192
nid001244: 74,192
nid001244: 74,192
nid001241: 74,192
nid001241: 74,192
nid001241: 75,192
nid001244: 75,192
nid001244: 75,192
nid001244: 75,192
nid001244: 75,192
nid001241: 75,192
nid001241: 75,192
nid001241: 75,192
nid001244: 76,192
nid001241: 76,192
nid001241: 76,192
nid001244: 76,192
nid001244: 76,192
nid001244: 76,192
nid001241: 76,192
nid001241: 76,192
nid001241: 77,192
nid001241: 77,192
nid001244: 77,192
nid001244: 77,192
nid001244: 77,192
nid001241: 77,192
nid001244: 77,192
nid001241: 77,192
nid001244: 78,192
nid001244: 78,192
nid001241: 78,192
nid001241: 78,192
nid001244: 78,192
nid001244: 78,192
nid001241: 78,192
nid001241: 78,192
nid001244: 79,192
nid001244: 79,192
nid001241: 79,192
nid001241: 79,192
nid001244: 79,192
nid001244: 79,192
nid001241: 79,19279,192
nid001241: 
nid001244: 80,192
nid001244: 80,192
nid001244: 80,192
nid001241: 80,192
nid001241: 80,192
nid001244: 80,192
nid001241: 80,192
nid001241: 80,192
nid001244: 81,192
nid001244: 81,192
nid001241: 81,192
nid001244: 81,192
nid001241: 81,192
nid001244: 81,192
nid001241: 81,192
nid001241: 81,192
nid001241: 82,192
nid001244: 82,192
nid001244: 82,192
nid001244: 82,192
nid001244: 82,192
nid001241: 82,192
nid001241: 82,192
nid001241: 82,192
nid001241: 83,192
nid001244: 83,192
nid001241: 83,192
nid001244: 83,192
nid001244: 83,192
nid001244: 83,192
nid001241: 83,192
nid001241: 83,192
nid001244: 84,192
nid001244: 84,192
nid001241: 84,192
nid001241: 84,192
nid001244: 84,192
nid001244: 84,192
nid001241: 84,192
nid001241: 84,192
nid001241: 85,192
nid001241: 85,192
nid001244: 85,192
nid001244: 85,192
nid001244: 85,192
nid001244: 85,192
nid001241: 85,192
nid001241: 85,192
nid001241: 86,192
nid001244: 86,192
nid001241: 86,192
nid001244: 86,192
nid001244: 86,192
nid001241: 86,192
nid001244: 86,192
nid001241: 86,192
nid001241: 87,192
nid001241: 87,192
nid001244: 87,192
nid001244: 87,192
nid001244: 87,192
nid001244: 87,192
nid001241: 87,192
nid001241: 87,192
nid001244: 88,192
nid001241: 88,192
nid001244: 88,192
nid001241: 88,192
nid001244: 88,192
nid001244: 88,192
nid001241: 88,192
nid001241: 88,192
nid001241: 89,192
nid001244: 89,192
nid001244: 89,192
nid001241: 89,192
nid001244: 89,192
nid001244: 89,192
nid001241: 89,192
nid001241: 89,192
nid001241: 90,192
nid001241: 90,192
nid001244: 90,192
nid001244: 90,192
nid001244: 90,192
nid001244: 90,192
nid001241: 90,192
nid001241: 90,192
nid001244: 91,192
nid001244: 91,192
nid001244: 91,192
nid001244: 91,192
nid001241: 91,192
nid001241: 91,192
nid001241: 91,192
nid001241: 91,192
nid001244: 92,192
nid001244: 92,192
nid001241: 92,192
nid001244: 92,192
nid001241: 92,192
nid001244: 92,192
nid001241: 92,192
nid001241: 92,192
nid001241: 93,192
nid001244: 93,192
nid001241: 93,192
nid001241: 93,192
nid001244: 93,192
nid001244: 93,192
nid001241: 93,192
nid001244: 93,192
nid001244: 94,192
nid001244: 94,192
nid001244: 94,192
nid001241: 94,192
nid001244: 94,192
nid001241: 94,192
nid001241: 94,192
nid001241: 94,192
nid001241: 95,192
nid001244: 95,192
nid001244: 95,192
nid001241: 95,192
nid001241: 95,192
nid001244: 95,192
nid001244: 95,192
nid001241: 95,192
nid001244: 96,192
nid001241: 96,192
nid001241: 96,192
nid001241: 96,192
nid001244: 96,192
nid001244: 96,192
nid001244: 96,192
nid001241: 96,192
nid001244: 97,192
nid001244: 97,192
nid001244: 97,192
nid001244: 97,192
nid001241: 97,192
nid001241: 97,192
nid001241: 97,19297,192
nid001241: 
nid001244: 98,192
nid001241: 98,192
nid001244: 98,192
nid001244: 98,192
nid001241: 98,192
nid001244: 98,192
nid001241: 98,192
nid001241: 98,192
nid001244: 99,192
nid001241: 99,192
nid001241: 99,192
nid001244: 99,192
nid001244: 99,192
nid001241: 99,192
nid001241: 99,192
nid001244: 99,192
nid001244: 100,192
nid001241: 100,192
nid001244: 100,192
nid001244: 100,192
nid001241: 100,192
nid001244: 100,192
nid001241: 100,192
nid001241: 100,192
nid001241: 101,192
nid001241: 101,192
nid001244: 101,192
nid001244: 101,192
nid001244: 101,192
nid001244: 101,192
nid001241: 101,192
nid001241: 101,192
nid001244: 102,192
nid001241: 102,192
nid001241: 102,192
nid001244: 102,192
nid001241: 102,192
nid001241: 102,192
nid001244: 102,192
nid001244: 102,192
nid001241: 103,192
nid001244: 103,192
nid001244: 103,192
nid001241: 103,192
nid001244: 103,192
nid001241: 103,192
nid001244: 103,192
nid001241: 103,192
nid001244: 104,192
nid001244: 104,192
nid001244: 104,192
nid001241: 104,192
nid001244: 104,192
nid001241: 104,192
nid001241: 104,192
nid001241: 104,192
nid001244: 105,192
nid001244: 105,192
nid001241: 105,192
nid001244: 105,192
nid001241: 105,192
nid001244: 105,192
nid001241: 105,192
nid001241: 105,192
nid001241: 106,192
nid001244: 106,192
nid001244: 106,192
nid001244: 106,192
nid001241: 106,192106,192
nid001241: 
nid001244: 106,192
nid001241: 106,192
nid001244: 107,192
nid001244: 107,192
nid001241: 107,192
nid001244: 107,192
nid001244: 107,192
nid001241: 107,192
nid001241: 107,192
nid001241: 107,192
nid001244: 108,192
nid001241: 108,192
nid001244: 108,192
nid001244: 108,192
nid001244: 108,192
nid001241: 108,192
nid001241: 108,192
nid001241: 108,192
nid001241: 109,192
nid001244: 109,192
nid001244: 109,192
nid001244: 109,192
nid001244: 109,192
nid001241: 109,192
nid001241: 109,192
nid001241: 109,192
nid001244: 110,192
nid001244: 110,192
nid001241: 110,192
nid001244: 110,192
nid001241: 110,192
nid001244: 110,192
nid001241: 110,192
nid001241: 110,192
nid001244: 111,192
nid001244: 111,192
nid001241: 111,192
nid001244: 111,192
nid001244: 111,192
nid001241: 111,192
nid001241: 111,192
nid001241: 111,192
nid001244: 112,192
nid001244: 112,192
nid001241: 112,192
nid001244: 112,192
nid001244: 112,192
nid001241: 112,192
nid001241: 112,192
nid001241: 112,192
nid001244: 113,192
nid001241: 113,192
nid001244: 113,192
nid001241: 113,192
nid001244: 113,192
nid001241: 113,192
nid001244: 113,192
nid001241: 113,192
nid001244: 114,192
nid001241: 114,192
nid001244: 114,192
nid001241: 114,192
nid001244: 114,192
nid001244: 114,192
nid001241: 114,192
nid001241: 114,192
nid001244: 115,192
nid001244: 115,192
nid001244: 115,192
nid001241: 115,192
nid001241: 115,192
nid001244: 115,192
nid001241: 115,192
nid001241: 115,192
nid001241: 116,192
nid001244: 116,192
nid001241: 116,192
nid001244: 116,192
nid001241: 116,192
nid001244: 116,192
nid001241: 116,192
nid001244: 116,192
nid001241: 117,192
nid001244: 117,192
nid001244: 117,192
nid001244: 117,192
nid001241: 117,192
nid001244: 117,192
nid001241: 117,192
nid001241: 117,192
nid001244: 118,192
nid001244: 118,192
nid001244: 118,192
nid001241: 118,192
nid001244: 118,192
nid001241: 118,192
nid001241: 118,192
nid001241: 118,192
nid001241: 119,192
nid001244: 119,192
nid001244: 119,192
nid001241: 119,192
nid001244: 119,192
nid001244: 119,192
nid001241: 119,192
nid001241: 119,192
nid001244: 120,192
nid001241: 120,192
nid001244: 120,192
nid001244: 120,192
nid001241: 120,192
nid001244: 120,192
nid001241: 120,192
nid001241: 120,192
nid001244: 121,192
nid001244: 121,192
nid001241: 121,192
nid001241: 121,192
nid001244: 121,192
nid001244: 121,192
nid001241: 121,192
nid001241: 121,192
nid001241: 122,192
nid001244: 122,192
nid001244: 122,192
nid001244: 122,192
nid001244: 122,192
nid001241: 122,192
nid001241: 122,192
nid001241: 122,192
nid001244: 123,192
nid001241: 123,192
nid001241: 123,192
nid001244: 123,192
nid001244: 123,192
nid001244: 123,192
nid001241: 123,192
nid001241: 123,192
nid001244: 124,192
nid001244: 124,192
nid001241: 124,192
nid001241: 124,192
nid001244: 124,192
nid001244: 124,192
nid001241: 124,192
nid001241: 124,192
nid001244: 125,192
nid001241: 125,192
nid001244: 125,192
nid001241: 125,192
nid001244: 125,192
nid001244: 125,192
nid001241: 125,192
nid001241: 125,192
nid001241: 126,192
nid001241: 126,192
nid001241: 126,192
nid001244: 126,192
nid001244: 126,192126,192
nid001244: 
nid001241: 126,192
nid001244: 126,192
nid001244: 127,192
nid001244: 127,192
nid001244: 127,192
nid001244: 127,192
nid001241: 127,192
nid001241: 127,192
nid001241: 127,192
nid001241: 127,192
nid001241: 128,192
nid001244: 128,192
nid001244: 128,192
nid001241: 128,192
nid001244: 128,192
nid001244: 128,192
nid001241: 128,192
nid001241: 128,192
nid001241: 129,192
nid001244: 129,192
nid001241: 129,192
nid001244: 129,192
nid001244: 129,192
nid001241: 129,192
nid001241: 129,192
nid001244: 129,192
nid001241: 130,192
nid001244: 130,192
nid001241: 130,192
nid001244: 130,192
nid001244: 130,192
nid001244: 130,192
nid001241: 130,192
nid001241: 130,192
nid001244: 131,192
nid001244: 131,192
nid001244: 131,192
nid001241: 131,192
nid001244: 131,192
nid001241: 131,192
nid001241: 131,192
nid001241: 131,192
nid001244: 132,192
nid001244: 132,192
nid001244: 132,192
nid001241: 132,192
nid001244: 132,192
nid001241: 132,192
nid001241: 132,192
nid001241: 132,192
nid001244: 133,192
nid001241: 133,192
nid001244: 133,192
nid001241: 133,192
nid001244: 133,192
nid001244: 133,192
nid001241: 133,192
nid001241: 133,192
nid001241: 134,192
nid001244: 134,192
nid001244: 134,192
nid001244: 134,192
nid001241: 134,192
nid001244: 134,192
nid001241: 134,192
nid001241: 134,192
nid001241: 135,192
nid001241: 135,192
nid001244: 135,192
nid001244: 135,192
nid001244: 135,192
nid001244: 135,192
nid001241: 135,192
nid001241: 135,192
nid001244: 136,192
nid001241: 136,192
nid001244: 136,192
nid001244: 136,192
nid001241: 136,192
nid001241: 136,192
nid001244: 136,192
nid001241: 136,192
nid001244: 137,192
nid001241: 137,192
nid001244: 137,192
nid001244: 137,192
nid001244: 137,192
nid001241: 137,192
nid001241: 137,192
nid001241: 137,192
nid001241: 138,192
nid001244: 138,192
nid001241: 138,192
nid001244: 138,192
nid001244: 138,192
nid001244: 138,192
nid001241: 138,192
nid001241: 138,192
nid001241: 139,192
nid001241: 139,192
nid001244: 139,192
nid001244: 139,192
nid001244: 139,192
nid001244: 139,192
nid001241: 139,192
nid001241: 139,192
nid001244: 140,192
nid001241: 140,192
nid001244: 140,192
nid001244: 140,192
nid001241: 140,192
nid001241: 140,192
nid001244: 140,192
nid001241: 140,192
nid001244: 141,192
nid001244: 141,192
nid001244: 141,192
nid001244: 141,192
nid001241: 141,192
nid001241: 141,192
nid001241: 141,192
nid001241: 141,192
nid001244: 142,192
nid001241: 142,192
nid001241: 142,192
nid001241: 142,192
nid001241: 142,192
nid001244: 142,192
nid001244: 142,192
nid001244: 142,192
nid001244: 143,192
nid001244: 143,192
nid001241: 143,192
nid001244: 143,192
nid001244: 143,192
nid001241: 143,192
nid001241: 143,192
nid001241: 143,192
nid001241: 144,192
nid001244: 144,192
nid001244: 144,192
nid001241: 144,192
nid001241: 144,192
nid001244: 144,192
nid001241: 144,192
nid001244: 144,192
nid001244: 145,192
nid001244: 145,192
nid001241: 145,192
nid001241: 145,192
nid001244: 145,192
nid001244: 145,192
nid001241: 145,192
nid001241: 145,192
nid001244: 146,192
nid001244: 146,192
nid001244: 146,192
nid001244: 146,192
nid001241: 146,192
nid001241: 146,192
nid001241: 146,192
nid001241: 146,192
nid001244: 147,192
nid001241: 147,192
nid001241: 147,192
nid001241: 147,192
nid001244: 147,192
nid001244: 147,192
nid001244: 147,192
nid001241: 147,192
nid001241: 148,192
nid001244: 148,192
nid001244: 148,192
nid001244: 148,192
nid001244: 148,192
nid001241: 148,192
nid001241: 148,192
nid001241: 148,192
nid001241: 149,192
nid001244: 149,192
nid001244: 149,192
nid001244: 149,192
nid001241: 149,192
nid001244: 149,192
nid001241: 149,192
nid001241: 149,192
nid001241: 150,192
nid001244: 150,192
nid001244: 150,192
nid001244: 150,192
nid001244: 150,192
nid001241: 150,192
nid001241: 150,192
nid001241: 150,192
nid001241: 151,192
nid001244: 151,192
nid001244: 151,192
nid001244: 151,192
nid001241: 151,192
nid001241: 151,192
nid001241: 151,192
nid001244: 151,192
nid001244: 152,192
nid001241: 152,192
nid001244: 152,192
nid001241: 152,192
nid001244: 152,192
nid001241: 152,192
nid001244: 152,192
nid001241: 152,192
nid001241: 153,192
nid001244: 153,192
nid001244: 153,192
nid001241: 153,192
nid001244: 153,192
nid001244: 153,192
nid001241: 153,192
nid001241: 153,192
nid001244: 154,192
nid001241: 154,192
nid001244: 154,192
nid001244: 154,192
nid001244: 154,192
nid001241: 154,192
nid001241: 154,192
nid001241: 154,192
nid001241: 155,192
nid001244: 155,192
nid001241: 155,192
nid001244: 155,192
nid001244: 155,192
nid001244: 155,192
nid001241: 155,192
nid001241: 155,192
nid001241: 156,192
nid001241: 156,192
nid001244: 156,192
nid001244: 156,192
nid001244: 156,192
nid001244: 156,192
nid001241: 156,192
nid001241: 156,192
nid001244: 157,192
nid001244: 157,192
nid001244: 157,192
nid001244: 157,192
nid001241: 157,192
nid001241: 157,192
nid001241: 157,192
nid001241: 157,192
nid001241: 158,192
nid001244: 158,192
nid001244: 158,192
nid001244: 158,192
nid001244: 158,192
nid001241: 158,192
nid001241: 158,192
nid001241: 158,192
nid001241: 159,192
nid001244: 159,192
nid001244: 159,192
nid001244: 159,192
nid001244: 159,192
nid001241: 159,192
nid001241: 159,192
nid001241: 159,192
nid001244: 160,192
nid001241: 160,192
nid001244: 160,192
nid001241: 160,192
nid001244: 160,192
nid001244: 160,192
nid001241: 160,192
nid001241: 160,192
nid001244: 161,192
nid001244: 161,192
nid001241: 161,192
nid001244: 161,192
nid001241: 161,192
nid001244: 161,192
nid001241: 161,192
nid001241: 161,192
nid001244: 162,192
nid001244: 162,192
nid001244: 162,192
nid001241: 162,192
nid001241: 162,192
nid001244: 162,192
nid001241: 162,192
nid001241: 162,192
nid001244: 163,192
nid001244: 163,192
nid001244: 163,192
nid001241: 163,192
nid001241: 163,192
nid001244: 163,192
nid001241: 163,192
nid001241: 163,192
nid001244: 164,192
nid001241: 164,192
nid001241: 164,192
nid001241: 164,192
nid001244: 164,192
nid001244: 164,192
nid001244: 164,192
nid001241: 164,192
nid001241: 165,192
nid001244: 165,192
nid001244: 165,192
nid001244: 165,192
nid001244: 165,192
nid001241: 165,192
nid001241: 165,192
nid001241: 165,192
nid001241: 166,192
nid001244: 166,192
nid001244: 166,192
nid001244: 166,192
nid001241: 166,192
nid001244: 166,192
nid001241: 166,192
nid001241: 166,192
nid001244: 167,192
nid001241: 167,192
nid001241: 167,192
nid001241: 167,192
nid001241: 167,192
nid001244: 167,192
nid001244: 167,192
nid001244: 167,192
nid001241: 168,192
nid001244: 168,192
nid001244: 168,192
nid001244: 168,192
nid001244: 168,192
nid001241: 168,192
nid001241: 168,192
nid001241: 168,192
nid001244: 169,192
nid001241: 169,192
nid001241: 169,192
nid001244: 169,192
nid001244: 169,192
nid001244: 169,192
nid001241: 169,192
nid001241: 169,192
nid001241: 170,192
nid001241: 170,192
nid001244: 170,192
nid001244: 170,192
nid001244: 170,192
nid001244: 170,192
nid001241: 170,192
nid001241: 170,192
nid001244: 171,192
nid001244: 171,192
nid001244: 171,192
nid001241: 171,192
nid001241: 171,192
nid001244: 171,192
nid001241: 171,192
nid001241: 171,192
nid001241: 172,192
nid001244: 172,192
nid001244: 172,192
nid001244: 172,192
nid001241: 172,192
nid001244: 172,192
nid001241: 172,192
nid001241: 172,192
nid001241: 173,192
nid001241: 173,192
nid001244: 173,192
nid001244: 173,192
nid001244: 173,192
nid001244: 173,192
nid001241: 173,192
nid001241: 173,192
nid001241: 174,192
nid001244: 174,192
nid001244: 174,192
nid001244: 174,192
nid001241: 174,192
nid001244: 174,192
nid001241: 174,192
nid001241: 174,192
nid001241: 175,192
nid001244: 175,192
nid001244: 175,192
nid001241: 175,192
nid001244: 175,192
nid001244: 175,192
nid001241: 175,192
nid001241: 175,192
nid001244: 176,192
nid001241: 176,192
nid001241: 176,192
nid001244: 176,192
nid001244: 176,192
nid001244: 176,192
nid001241: 176,192176,192
nid001241: 
nid001241: 177,192
nid001244: 177,192
nid001244: 177,192
nid001241: 177,192
nid001244: 177,192
nid001244: 177,192
nid001241: 177,192
nid001241: 177,192
nid001241: 178,192
nid001244: 178,192
nid001241: 178,192
nid001244: 178,192178,192
nid001244: 
nid001244: 178,192
nid001241: 178,192
nid001241: 178,192
nid001244: 179,192
nid001241: 179,192
nid001241: 179,192
nid001244: 179,192
nid001244: 179,192
nid001244: 179,192
nid001241: 179,192
nid001241: 179,192
nid001244: 180,192
nid001241: 180,192
nid001244: 180,192
nid001241: 180,192
nid001244: 180,192
nid001244: 180,192
nid001241: 180,192
nid001241: 180,192
nid001241: 181,192
nid001244: 181,192
nid001244: 181,192
nid001244: 181,192
nid001241: 181,192
nid001244: 181,192
nid001241: 181,192
nid001241: 181,192
nid001244: 182,192
nid001241: 182,192
nid001244: 182,192
nid001241: 182,192
nid001244: 182,192
nid001241: 182,192
nid001244: 182,192
nid001241: 182,192
nid001244: 183,192
nid001241: 183,192
nid001241: 183,192
nid001244: 183,192
nid001244: 183,192
nid001244: 183,192
nid001241: 183,192
nid001241: 183,192
nid001241: 184,192
nid001244: 184,192
nid001244: 184,192
nid001244: 184,192
nid001241: 184,192
nid001244: 184,192
nid001241: 184,192
nid001241: 184,192
nid001241: 185,192
nid001244: 185,192
nid001241: 185,192
nid001244: 185,192
nid001244: 185,192
nid001244: 185,192
nid001241: 185,192
nid001241: 185,192
nid001244: 186,192
nid001241: 186,192
nid001244: 186,192
nid001244: 186,192
nid001241: 186,192
nid001244: 186,192
nid001241: 186,192
nid001241: 186,192
nid001244: 187,192
nid001244: 187,192
nid001241: 187,192
nid001244: 187,192
nid001244: 187,192
nid001241: 187,192
nid001241: 187,192
nid001241: 187,192
nid001241: 188,192
nid001244: 188,192
nid001244: 188,192
nid001241: 188,192
nid001244: 188,192
nid001244: 188,192
nid001241: 188,192
nid001241: 188,192
nid001244: 189,192
nid001244: 189,192
nid001244: 189,192
nid001244: 189,192
nid001241: 189,192
nid001241: 189,192
nid001241: 189,192
nid001241: 189,192
nid001244: 190,192
nid001244: 190,192
nid001244: 190,192
nid001241: 190,192
nid001241: 190,192
nid001241: 190,192
nid001241: 190,192
nid001244: 190,192
nid001241: 191,192
nid001244: 191,192
nid001244: 191,192
nid001244: 191,192
nid001244: 191,192
nid001241: 191,192
nid001241: 191,192
nid001241: 191,192
nid001241: ppl: 10.33669376373291, loss: 2.335700035095215
nid001241: Beginning of Epoch 1/1, Total Micro Batches 2860
nid001244: Epoch: 0, Step: 0, Rank: 7, loss = 2.320115327835083
nid001244: Epoch: 0, Step: 0, Rank: 4, loss = 2.74989914894104
nid001244: Epoch: 0, Step: 0, Rank: 5, loss = 2.1699635982513428
nid001241: Epoch: 0, Step: 0, Rank: 0, loss = 2.098331928253174
nid001244: Epoch: 0, Step: 0, Rank: 6, loss = 2.3483622074127197
nid001241: Epoch: 0, Step: 0, Rank: 3, loss = 2.325697422027588
nid001241: Epoch: 0, Step: 0, Rank: 1, loss = 2.4105734825134277
nid001241: Epoch: 0, Step: 0, Rank: 2, loss = 2.109689712524414
nid001241: Invalidate trace cache @ step 422 and module 0: cache has only 422 modules
nid001241: [2024-11-12 12:32:31,642] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
nid001241: Model Parameters: 8.030 B, Latency: 10.73s, TFLOPs: 2.71, Samples/sec: 0.37, Time/seq 2.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Step 0: GPU Memory Usage
nid001241: GPU 0 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  31882 MB
nid001241:   Free:  9077 MB
nid001241:   Usage: 31882/40960 MB (77.84%)
nid001241: 
nid001241: GPU 1 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  31882 MB
nid001241:   Free:  9077 MB
nid001241:   Usage: 31882/40960 MB (77.84%)
nid001241: 
nid001241: GPU 2 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  31906 MB
nid001241:   Free:  9053 MB
nid001241:   Usage: 31906/40960 MB (77.90%)
nid001241: 
nid001241: GPU 3 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  31858 MB
nid001241:   Free:  9101 MB
nid001241:   Usage: 31858/40960 MB (77.78%)
nid001241: 
nid001244: Epoch: 0, Step: 1, Rank: 7, loss = 2.1534481048583984
nid001244: Epoch: 0, Step: 1, Rank: 6, loss = 2.018853187561035
nid001241: Epoch: 0, Step: 1, Rank: 3, loss = 2.1772494316101074
nid001244: Epoch: 0, Step: 1, Rank: 5, loss = 2.537731885910034
nid001241: Epoch: 0, Step: 1, Rank: 2, loss = 2.3630313873291016
nid001241: Epoch: 0, Step: 1, Rank: 1, loss = 2.295055627822876
nid001241: Epoch: 0, Step: 1, Rank: 0, loss = 2.2721738815307617
nid001244: Epoch: 0, Step: 1, Rank: 4, loss = 2.201922655105591
nid001241: [2024-11-12 12:32:41,721] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
nid001241: Model Parameters: 8.030 B, Latency: 10.08s, TFLOPs: 2.89, Samples/sec: 0.40, Time/seq 2.52s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 2, Rank: 0, loss = 2.109560489654541
nid001241: Epoch: 0, Step: 2, Rank: 3, loss = 2.0017051696777344
nid001244: Epoch: 0, Step: 2, Rank: 4, loss = 3.2846827507019043
nid001244: Epoch: 0, Step: 2, Rank: 7, loss = 2.372044324874878
nid001241: Epoch: 0, Step: 2, Rank: 1, loss = 2.5107176303863525
nid001241: Epoch: 0, Step: 2, Rank: 2, loss = 2.6228601932525635
nid001244: Epoch: 0, Step: 2, Rank: 5, loss = 2.3184633255004883
nid001244: Epoch: 0, Step: 2, Rank: 6, loss = 2.031869411468506
nid001241: [2024-11-12 12:32:48,301] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
nid001241: Model Parameters: 8.030 B, Latency: 6.58s, TFLOPs: 4.42, Samples/sec: 0.61, Time/seq 1.64s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 3, Rank: 6, loss = 1.9104021787643433
nid001241: Epoch: 0, Step: 3, Rank: 2, loss = 2.4836862087249756
nid001244: Epoch: 0, Step: 3, Rank: 7, loss = 2.355832576751709
nid001241: Epoch: 0, Step: 3, Rank: 3, loss = 2.1201155185699463
nid001241: Epoch: 0, Step: 3, Rank: 0, loss = 2.256472110748291
nid001241: Epoch: 0, Step: 3, Rank: 1, loss = 2.453141212463379
nid001244: Epoch: 0, Step: 3, Rank: 5, loss = 2.723647356033325
nid001244: Epoch: 0, Step: 3, Rank: 4, loss = 2.314072370529175
nid001241: [2024-11-12 12:32:55,359] [WARNING] [stage3.py:2114:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
nid001241: Model Parameters: 8.030 B, Latency: 7.06s, TFLOPs: 4.12, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 4, Rank: 0, loss = 2.156801462173462
nid001241: Epoch: 0, Step: 4, Rank: 3, loss = 2.069082736968994
nid001241: Epoch: 0, Step: 4, Rank: 1, loss = 1.6326414346694946
nid001244: Epoch: 0, Step: 4, Rank: 4, loss = 1.724843144416809
nid001244: Epoch: 0, Step: 4, Rank: 7, loss = 2.017444372177124
nid001241: Epoch: 0, Step: 4, Rank: 2, loss = 1.9933463335037231
nid001244: Epoch: 0, Step: 4, Rank: 5, loss = 1.6501370668411255
nid001244: Epoch: 0, Step: 4, Rank: 6, loss = 2.0836169719696045
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 5, Rank: 6, loss = 1.8895646333694458
nid001244: Epoch: 0, Step: 5, Rank: 7, loss = 1.991033911705017
nid001241: Epoch: 0, Step: 5, Rank: 0, loss = 1.6992601156234741
nid001241: Epoch: 0, Step: 5, Rank: 3, loss = 2.0333054065704346
nid001241: Epoch: 0, Step: 5, Rank: 1, loss = 1.944691777229309
nid001244: Epoch: 0, Step: 5, Rank: 4, loss = 1.7605600357055664
nid001244: Epoch: 0, Step: 5, Rank: 5, loss = 1.633601427078247
nid001241: Epoch: 0, Step: 5, Rank: 2, loss = 1.8545854091644287
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 6, Rank: 0, loss = 1.9535784721374512
nid001241: Epoch: 0, Step: 6, Rank: 3, loss = 1.8098695278167725
nid001241: Epoch: 0, Step: 6, Rank: 1, loss = 1.7007054090499878
nid001244: Epoch: 0, Step: 6, Rank: 4, loss = 2.024787425994873
nid001244: Epoch: 0, Step: 6, Rank: 7, loss = 2.0753042697906494
nid001241: Epoch: 0, Step: 6, Rank: 2, loss = 1.6780892610549927
nid001244: Epoch: 0, Step: 6, Rank: 5, loss = 2.1653859615325928
nid001244: Epoch: 0, Step: 6, Rank: 6, loss = 1.876876711845398
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 7, Rank: 3, loss = 1.9027560949325562
nid001241: Epoch: 0, Step: 7, Rank: 2, loss = 1.8903740644454956
nid001244: Epoch: 0, Step: 7, Rank: 7, loss = 1.5441902875900269
nid001241: Epoch: 0, Step: 7, Rank: 1, loss = 1.6046181917190552
nid001244: Epoch: 0, Step: 7, Rank: 6, loss = 1.6298370361328125
nid001244: Epoch: 0, Step: 7, Rank: 5, loss = 1.8025187253952026
nid001244: Epoch: 0, Step: 7, Rank: 4, loss = 1.7616039514541626
nid001241: Epoch: 0, Step: 7, Rank: 0, loss = 2.0088367462158203
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 8, Rank: 4, loss = 1.9285563230514526
nid001244: Epoch: 0, Step: 8, Rank: 7, loss = 1.9286634922027588
nid001241: Epoch: 0, Step: 8, Rank: 0, loss = 1.91252863407135
nid001241: Epoch: 0, Step: 8, Rank: 3, loss = 1.8210803270339966
nid001244: Epoch: 0, Step: 8, Rank: 5, loss = 1.8006447553634644
nid001244: Epoch: 0, Step: 8, Rank: 6, loss = 1.7622441053390503
nid001241: Epoch: 0, Step: 8, Rank: 1, loss = 2.150655508041382
nid001241: Epoch: 0, Step: 8, Rank: 2, loss = 1.9468371868133545
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 9, Rank: 0, loss = 1.5174140930175781
nid001241: Epoch: 0, Step: 9, Rank: 1, loss = 1.8485161066055298
nid001241: Epoch: 0, Step: 9, Rank: 2, loss = 1.9252609014511108
nid001244: Epoch: 0, Step: 9, Rank: 4, loss = 1.9083507061004639
nid001241: Epoch: 0, Step: 9, Rank: 3, loss = 1.6675522327423096
nid001244: Epoch: 0, Step: 9, Rank: 7, loss = 1.866565465927124
nid001244: Epoch: 0, Step: 9, Rank: 5, loss = 1.6417902708053589
nid001244: Epoch: 0, Step: 9, Rank: 6, loss = 1.839079737663269
nid001241: [2024-11-12 12:33:36,592] [INFO] [logging.py:128:log_dist] [Rank 0] step=10, skipped=3, lr=[9.649857364068918e-06, 9.649857364068918e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:33:36,593] [INFO] [timer.py:264:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=4.668958084816771, CurrSamplesPerSec=4.648324249235024, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 10, Rank: 4, loss = 1.6339869499206543
nid001244: Epoch: 0, Step: 10, Rank: 7, loss = 1.8030587434768677
nid001241: Epoch: 0, Step: 10, Rank: 0, loss = 1.7495547533035278
nid001241: Epoch: 0, Step: 10, Rank: 3, loss = 1.9620640277862549
nid001244: Epoch: 0, Step: 10, Rank: 5, loss = 1.7213389873504639
nid001241: Epoch: 0, Step: 10, Rank: 1, loss = 1.8434112071990967
nid001241: Epoch: 0, Step: 10, Rank: 2, loss = 1.8013219833374023
nid001244: Epoch: 0, Step: 10, Rank: 6, loss = 1.612537145614624
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 11, Rank: 6, loss = 1.6471315622329712
nid001244: Epoch: 0, Step: 11, Rank: 7, loss = 1.5157216787338257
nid001244: Epoch: 0, Step: 11, Rank: 4, loss = 1.7440160512924194
nid001241: Epoch: 0, Step: 11, Rank: 3, loss = 1.6026374101638794
nid001244: Epoch: 0, Step: 11, Rank: 5, loss = 1.9135102033615112
nid001241: Epoch: 0, Step: 11, Rank: 0, loss = 1.9827042818069458
nid001241: Epoch: 0, Step: 11, Rank: 2, loss = 1.806033968925476
nid001241: Epoch: 0, Step: 11, Rank: 1, loss = 1.6907525062561035
nid001241: Model Parameters: 8.030 B, Latency: 6.68s, TFLOPs: 4.35, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 12, Rank: 2, loss = 1.5199604034423828
nid001241: Epoch: 0, Step: 12, Rank: 3, loss = 1.9197038412094116
nid001244: Epoch: 0, Step: 12, Rank: 6, loss = 1.59633469581604
nid001241: Epoch: 0, Step: 12, Rank: 0, loss = 1.6728228330612183
nid001244: Epoch: 0, Step: 12, Rank: 7, loss = 1.9299572706222534
nid001244: Epoch: 0, Step: 12, Rank: 4, loss = 1.7134662866592407
nid001244: Epoch: 0, Step: 12, Rank: 5, loss = 1.8040103912353516
nid001241: Epoch: 0, Step: 12, Rank: 1, loss = 1.7733592987060547
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 13, Rank: 6, loss = 1.4011657238006592
nid001244: Epoch: 0, Step: 13, Rank: 7, loss = 1.7957355976104736
nid001244: Epoch: 0, Step: 13, Rank: 4, loss = 1.678314447402954
nid001241: Epoch: 0, Step: 13, Rank: 3, loss = 1.620082139968872
nid001241: Epoch: 0, Step: 13, Rank: 0, loss = 1.495595932006836
nid001244: Epoch: 0, Step: 13, Rank: 5, loss = 1.837205171585083
nid001241: Epoch: 0, Step: 13, Rank: 1, loss = 2.006469964981079
nid001241: Epoch: 0, Step: 13, Rank: 2, loss = 1.8923269510269165
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 14, Rank: 4, loss = 1.703648328781128
nid001244: Epoch: 0, Step: 14, Rank: 7, loss = 1.6937483549118042
nid001241: Epoch: 0, Step: 14, Rank: 0, loss = 1.806947112083435
nid001244: Epoch: 0, Step: 14, Rank: 5, loss = 1.9582760334014893
nid001241: Epoch: 0, Step: 14, Rank: 3, loss = 1.6967523097991943
nid001241: Epoch: 0, Step: 14, Rank: 1, loss = 1.7898811101913452
nid001241: Epoch: 0, Step: 14, Rank: 2, loss = 1.4630908966064453
nid001244: Epoch: 0, Step: 14, Rank: 6, loss = 1.650403618812561
nid001241: Model Parameters: 8.030 B, Latency: 6.69s, TFLOPs: 4.35, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 15, Rank: 2, loss = 1.8381538391113281
nid001241: Epoch: 0, Step: 15, Rank: 3, loss = 1.7363755702972412
nid001244: Epoch: 0, Step: 15, Rank: 6, loss = 1.6855816841125488
nid001241: Epoch: 0, Step: 15, Rank: 0, loss = 1.8067121505737305
nid001244: Epoch: 0, Step: 15, Rank: 7, loss = 1.7464953660964966
nid001244: Epoch: 0, Step: 15, Rank: 4, loss = 1.965375542640686
nid001244: Epoch: 0, Step: 15, Rank: 5, loss = 1.794453740119934
nid001241: Epoch: 0, Step: 15, Rank: 1, loss = 1.7654616832733154
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 16, Rank: 2, loss = 1.5386302471160889
nid001244: Epoch: 0, Step: 16, Rank: 7, loss = 1.65113365650177
nid001244: Epoch: 0, Step: 16, Rank: 6, loss = 1.8438079357147217
nid001241: Epoch: 0, Step: 16, Rank: 3, loss = 1.5457617044448853
nid001244: Epoch: 0, Step: 16, Rank: 4, loss = 1.5746415853500366
nid001241: Epoch: 0, Step: 16, Rank: 0, loss = 1.777245283126831
nid001241: Epoch: 0, Step: 16, Rank: 1, loss = 1.942551612854004
nid001244: Epoch: 0, Step: 16, Rank: 5, loss = 1.8583590984344482
nid001241: Model Parameters: 8.030 B, Latency: 6.66s, TFLOPs: 4.37, Samples/sec: 0.60, Time/seq 1.66s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 17, Rank: 4, loss = 1.7986161708831787
nid001244: Epoch: 0, Step: 17, Rank: 7, loss = 1.8579598665237427
nid001244: Epoch: 0, Step: 17, Rank: 6, loss = 1.7614985704421997
nid001241: Epoch: 0, Step: 17, Rank: 1, loss = 1.9116060733795166
nid001244: Epoch: 0, Step: 17, Rank: 5, loss = 1.9280930757522583
nid001241: Epoch: 0, Step: 17, Rank: 0, loss = 1.5567377805709839
nid001241: Epoch: 0, Step: 17, Rank: 2, loss = 1.3232295513153076
nid001241: Epoch: 0, Step: 17, Rank: 3, loss = 1.7605160474777222
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 18, Rank: 1, loss = 1.6858183145523071
nid001241: Epoch: 0, Step: 18, Rank: 0, loss = 1.8896304368972778
nid001241: Epoch: 0, Step: 18, Rank: 2, loss = 1.634925365447998
nid001244: Epoch: 0, Step: 18, Rank: 5, loss = 1.5171303749084473
nid001244: Epoch: 0, Step: 18, Rank: 4, loss = 1.6730059385299683
nid001244: Epoch: 0, Step: 18, Rank: 6, loss = 1.8976937532424927
nid001244: Epoch: 0, Step: 18, Rank: 7, loss = 1.8569802045822144
nid001241: Epoch: 0, Step: 18, Rank: 3, loss = 1.6967462301254272
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 19, Rank: 1, loss = 1.3620326519012451
nid001241: Epoch: 0, Step: 19, Rank: 2, loss = 1.5502547025680542
nid001241: Epoch: 0, Step: 19, Rank: 0, loss = 1.491636872291565
nid001244: Epoch: 0, Step: 19, Rank: 5, loss = 1.7020275592803955
nid001244: Epoch: 0, Step: 19, Rank: 4, loss = 1.4433486461639404
nid001241: Epoch: 0, Step: 19, Rank: 3, loss = 1.2194377183914185
nid001244: Epoch: 0, Step: 19, Rank: 6, loss = 1.8506650924682617
nid001244: Epoch: 0, Step: 19, Rank: 7, loss = 1.866767406463623
nid001241: [2024-11-12 12:34:44,313] [INFO] [logging.py:128:log_dist] [Rank 0] step=20, skipped=3, lr=[9.649158759401679e-06, 9.649158759401679e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:34:44,313] [INFO] [timer.py:264:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=4.702019585034434, CurrSamplesPerSec=4.670918999645717, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 20, Rank: 2, loss = 1.618803858757019
nid001241: Epoch: 0, Step: 20, Rank: 3, loss = 1.8766056299209595
nid001241: Epoch: 0, Step: 20, Rank: 0, loss = 1.6882027387619019
nid001244: Epoch: 0, Step: 20, Rank: 6, loss = 1.709374189376831
nid001244: Epoch: 0, Step: 20, Rank: 7, loss = 1.5728411674499512
nid001241: Epoch: 0, Step: 20, Rank: 1, loss = 1.956700086593628
nid001244: Epoch: 0, Step: 20, Rank: 4, loss = 1.6647602319717407
nid001244: Epoch: 0, Step: 20, Rank: 5, loss = 1.7971267700195312
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 21, Rank: 3, loss = 1.387978434562683
nid001241: Epoch: 0, Step: 21, Rank: 2, loss = 1.3259508609771729
nid001244: Epoch: 0, Step: 21, Rank: 6, loss = 1.637296438217163
nid001241: Epoch: 0, Step: 21, Rank: 0, loss = 1.8891464471817017
nid001244: Epoch: 0, Step: 21, Rank: 7, loss = 1.5856596231460571
nid001244: Epoch: 0, Step: 21, Rank: 5, loss = 1.6498323678970337
nid001244: Epoch: 0, Step: 21, Rank: 4, loss = 1.7084144353866577
nid001241: Epoch: 0, Step: 21, Rank: 1, loss = 1.5709227323532104
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 22, Rank: 2, loss = 1.5757795572280884
nid001241: Epoch: 0, Step: 22, Rank: 3, loss = 1.571916937828064
nid001244: Epoch: 0, Step: 22, Rank: 6, loss = 1.6480498313903809
nid001244: Epoch: 0, Step: 22, Rank: 7, loss = 1.6066092252731323
nid001241: Epoch: 0, Step: 22, Rank: 0, loss = 1.5064655542373657
nid001244: Epoch: 0, Step: 22, Rank: 5, loss = 1.631861686706543
nid001244: Epoch: 0, Step: 22, Rank: 4, loss = 1.681144118309021
nid001241: Epoch: 0, Step: 22, Rank: 1, loss = 1.4966925382614136
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 23, Rank: 3, loss = 1.593869924545288
nid001241: Epoch: 0, Step: 23, Rank: 2, loss = 1.8798836469650269
nid001244: Epoch: 0, Step: 23, Rank: 4, loss = 1.8068807125091553
nid001241: Epoch: 0, Step: 23, Rank: 0, loss = 1.9682432413101196
nid001241: Epoch: 0, Step: 23, Rank: 1, loss = 1.9115644693374634
nid001244: Epoch: 0, Step: 23, Rank: 7, loss = 1.8418947458267212
nid001244: Epoch: 0, Step: 23, Rank: 5, loss = 1.696337342262268
nid001244: Epoch: 0, Step: 23, Rank: 6, loss = 1.946786642074585
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 24, Rank: 0, loss = 1.7803549766540527
nid001244: Epoch: 0, Step: 24, Rank: 7, loss = 1.53506600856781
nid001244: Epoch: 0, Step: 24, Rank: 4, loss = 1.82339346408844
nid001241: Epoch: 0, Step: 24, Rank: 3, loss = 1.7853610515594482
nid001244: Epoch: 0, Step: 24, Rank: 5, loss = 2.0467026233673096
nid001241: Epoch: 0, Step: 24, Rank: 1, loss = 1.8314487934112549
nid001244: Epoch: 0, Step: 24, Rank: 6, loss = 1.6518481969833374
nid001241: Epoch: 0, Step: 24, Rank: 2, loss = 1.7662792205810547
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 25, Rank: 7, loss = 1.560024619102478
nid001244: Epoch: 0, Step: 25, Rank: 6, loss = 1.6531171798706055
nid001241: Epoch: 0, Step: 25, Rank: 3, loss = 1.8990321159362793
nid001244: Epoch: 0, Step: 25, Rank: 5, loss = 1.350953459739685
nid001241: Epoch: 0, Step: 25, Rank: 2, loss = 1.030142068862915
nid001244: Epoch: 0, Step: 25, Rank: 4, loss = 1.533660650253296
nid001241: Epoch: 0, Step: 25, Rank: 1, loss = 1.874720573425293
nid001241: Epoch: 0, Step: 25, Rank: 0, loss = 1.6432647705078125
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 26, Rank: 2, loss = 1.953459620475769
nid001241: Epoch: 0, Step: 26, Rank: 3, loss = 1.7990624904632568
nid001241: Epoch: 0, Step: 26, Rank: 0, loss = 1.745887279510498
nid001244: Epoch: 0, Step: 26, Rank: 6, loss = 1.6392375230789185
nid001244: Epoch: 0, Step: 26, Rank: 7, loss = 1.8016809225082397
nid001244: Epoch: 0, Step: 26, Rank: 4, loss = 1.6393498182296753
nid001244: Epoch: 0, Step: 26, Rank: 5, loss = 1.5858707427978516
nid001241: Epoch: 0, Step: 26, Rank: 1, loss = 1.2822836637496948
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 27, Rank: 1, loss = 1.8503572940826416
nid001241: Epoch: 0, Step: 27, Rank: 0, loss = 1.5176784992218018
nid001244: Epoch: 0, Step: 27, Rank: 7, loss = 1.6145857572555542
nid001244: Epoch: 0, Step: 27, Rank: 6, loss = 1.7621618509292603
nid001241: Epoch: 0, Step: 27, Rank: 2, loss = 1.701505422592163
nid001241: Epoch: 0, Step: 27, Rank: 3, loss = 1.824496865272522
nid001244: Epoch: 0, Step: 27, Rank: 5, loss = 1.669119954109192
nid001244: Epoch: 0, Step: 27, Rank: 4, loss = 1.8077243566513062
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 28, Rank: 2, loss = 1.9000012874603271
nid001241: Epoch: 0, Step: 28, Rank: 3, loss = 1.8376039266586304
nid001244: Epoch: 0, Step: 28, Rank: 6, loss = 1.6396957635879517
nid001244: Epoch: 0, Step: 28, Rank: 7, loss = 1.9540125131607056
nid001241: Epoch: 0, Step: 28, Rank: 0, loss = 2.0024185180664062
nid001244: Epoch: 0, Step: 28, Rank: 4, loss = 1.5433824062347412
nid001244: Epoch: 0, Step: 28, Rank: 5, loss = 1.897348403930664
nid001241: Epoch: 0, Step: 28, Rank: 1, loss = 1.8917673826217651
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 29, Rank: 2, loss = 1.7184122800827026
nid001244: Epoch: 0, Step: 29, Rank: 6, loss = 1.617080569267273
nid001244: Epoch: 0, Step: 29, Rank: 7, loss = 1.671871542930603
nid001241: Epoch: 0, Step: 29, Rank: 3, loss = 1.832762360572815
nid001244: Epoch: 0, Step: 29, Rank: 4, loss = 1.7140617370605469
nid001244: Epoch: 0, Step: 29, Rank: 5, loss = 1.764888882637024
nid001241: Epoch: 0, Step: 29, Rank: 0, loss = 1.9345409870147705
nid001241: Epoch: 0, Step: 29, Rank: 1, loss = 1.7854418754577637
nid001241: [2024-11-12 12:35:52,654] [INFO] [logging.py:128:log_dist] [Rank 0] step=30, skipped=3, lr=[9.647878071750232e-06, 9.647878071750232e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:35:52,655] [INFO] [timer.py:264:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=4.696250425252502, CurrSamplesPerSec=4.729819950549852, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 30, Rank: 2, loss = 1.884425163269043
nid001241: Epoch: 0, Step: 30, Rank: 0, loss = 1.7211483716964722
nid001241: Epoch: 0, Step: 30, Rank: 3, loss = 1.9511886835098267
nid001244: Epoch: 0, Step: 30, Rank: 6, loss = 1.681159496307373
nid001241: Epoch: 0, Step: 30, Rank: 1, loss = 1.664206862449646
nid001244: Epoch: 0, Step: 30, Rank: 7, loss = 1.716982126235962
nid001244: Epoch: 0, Step: 30, Rank: 4, loss = 1.6762038469314575
nid001244: Epoch: 0, Step: 30, Rank: 5, loss = 1.5432424545288086
nid001241: Model Parameters: 8.030 B, Latency: 6.95s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 31, Rank: 2, loss = 1.993701457977295
nid001241: Epoch: 0, Step: 31, Rank: 0, loss = 1.437511920928955
nid001244: Epoch: 0, Step: 31, Rank: 6, loss = 1.3983231782913208
nid001244: Epoch: 0, Step: 31, Rank: 7, loss = 1.6641772985458374
nid001241: Epoch: 0, Step: 31, Rank: 3, loss = 1.506961703300476
nid001241: Epoch: 0, Step: 31, Rank: 1, loss = 1.6759201288223267
nid001244: Epoch: 0, Step: 31, Rank: 5, loss = 1.4471282958984375
nid001244: Epoch: 0, Step: 31, Rank: 4, loss = 1.7061753273010254
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 32, Rank: 7, loss = 1.4610676765441895
nid001244: Epoch: 0, Step: 32, Rank: 6, loss = 1.777339220046997
nid001244: Epoch: 0, Step: 32, Rank: 5, loss = 1.6186206340789795
nid001241: Epoch: 0, Step: 32, Rank: 3, loss = 1.733412265777588
nid001241: Epoch: 0, Step: 32, Rank: 2, loss = 1.645135521888733
nid001241: Epoch: 0, Step: 32, Rank: 1, loss = 1.6608933210372925
nid001244: Epoch: 0, Step: 32, Rank: 4, loss = 1.544870376586914
nid001241: Epoch: 0, Step: 32, Rank: 0, loss = 1.8155977725982666
nid001241: Model Parameters: 8.030 B, Latency: 7.32s, TFLOPs: 3.97, Samples/sec: 0.55, Time/seq 1.83s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 33, Rank: 7, loss = 1.8630985021591187
nid001244: Epoch: 0, Step: 33, Rank: 6, loss = 1.5389304161071777
nid001244: Epoch: 0, Step: 33, Rank: 5, loss = 1.5850754976272583
nid001241: Epoch: 0, Step: 33, Rank: 2, loss = 2.0241000652313232
nid001241: Epoch: 0, Step: 33, Rank: 3, loss = 1.8864129781723022
nid001244: Epoch: 0, Step: 33, Rank: 4, loss = 1.8840374946594238
nid001241: Epoch: 0, Step: 33, Rank: 1, loss = 1.306049108505249
nid001241: Epoch: 0, Step: 33, Rank: 0, loss = 1.902357816696167
nid001241: Model Parameters: 8.030 B, Latency: 7.13s, TFLOPs: 4.08, Samples/sec: 0.56, Time/seq 1.78s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 34, Rank: 7, loss = 1.6187469959259033
nid001244: Epoch: 0, Step: 34, Rank: 4, loss = 1.8902075290679932
nid001244: Epoch: 0, Step: 34, Rank: 5, loss = 1.5979125499725342
nid001241: Epoch: 0, Step: 34, Rank: 0, loss = 1.6932262182235718
nid001244: Epoch: 0, Step: 34, Rank: 6, loss = 1.4384492635726929
nid001241: Epoch: 0, Step: 34, Rank: 3, loss = 1.674184799194336
nid001241: Epoch: 0, Step: 34, Rank: 1, loss = 1.8090763092041016
nid001241: Epoch: 0, Step: 34, Rank: 2, loss = 1.8139593601226807
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 35, Rank: 6, loss = 1.5426677465438843
nid001244: Epoch: 0, Step: 35, Rank: 7, loss = 1.7438327074050903
nid001241: Epoch: 0, Step: 35, Rank: 2, loss = 1.5051511526107788
nid001244: Epoch: 0, Step: 35, Rank: 4, loss = 1.6747822761535645
nid001241: Epoch: 0, Step: 35, Rank: 3, loss = 1.9358603954315186Epoch: 0, Step: 35, Rank: 0, loss = 1.7202481031417847
nid001241: 
nid001241: Epoch: 0, Step: 35, Rank: 1, loss = 1.7312272787094116
nid001244: Epoch: 0, Step: 35, Rank: 5, loss = 1.6652154922485352
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 36, Rank: 6, loss = 1.7968019247055054
nid001244: Epoch: 0, Step: 36, Rank: 7, loss = 1.880204439163208
nid001241: Epoch: 0, Step: 36, Rank: 3, loss = 1.5744976997375488
nid001244: Epoch: 0, Step: 36, Rank: 5, loss = 1.6535724401474
nid001241: Epoch: 0, Step: 36, Rank: 2, loss = 1.8148807287216187
nid001241: Epoch: 0, Step: 36, Rank: 1, loss = 1.881722092628479
nid001241: Epoch: 0, Step: 36, Rank: 0, loss = 1.7272279262542725
nid001244: Epoch: 0, Step: 36, Rank: 4, loss = 1.8149744272232056
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 37, Rank: 6, loss = 1.8094960451126099
nid001244: Epoch: 0, Step: 37, Rank: 7, loss = 1.3472412824630737
nid001241: Epoch: 0, Step: 37, Rank: 2, loss = 1.5155879259109497
nid001241: Epoch: 0, Step: 37, Rank: 3, loss = 1.352190613746643
nid001244: Epoch: 0, Step: 37, Rank: 4, loss = 1.8787999153137207
nid001241: Epoch: 0, Step: 37, Rank: 0, loss = 1.8066579103469849
nid001241: Epoch: 0, Step: 37, Rank: 1, loss = 1.5557045936584473
nid001244: Epoch: 0, Step: 37, Rank: 5, loss = 1.6692159175872803
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 38, Rank: 4, loss = 1.8325942754745483
nid001241: Epoch: 0, Step: 38, Rank: 0, loss = 1.4194328784942627
nid001244: Epoch: 0, Step: 38, Rank: 7, loss = 1.6831848621368408
nid001241: Epoch: 0, Step: 38, Rank: 3, loss = 1.448622465133667
nid001241: Epoch: 0, Step: 38, Rank: 1, loss = 1.800399661064148
nid001241: Epoch: 0, Step: 38, Rank: 2, loss = 1.3981969356536865
nid001244: Epoch: 0, Step: 38, Rank: 5, loss = 1.7818747758865356
nid001244: Epoch: 0, Step: 38, Rank: 6, loss = 1.658508062362671
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 39, Rank: 7, loss = 1.8754169940948486
nid001241: Epoch: 0, Step: 39, Rank: 3, loss = 1.6636788845062256
nid001244: Epoch: 0, Step: 39, Rank: 5, loss = 1.6296249628067017
nid001241: Epoch: 0, Step: 39, Rank: 2, loss = 1.7884231805801392
nid001244: Epoch: 0, Step: 39, Rank: 4, loss = 1.5783969163894653
nid001244: Epoch: 0, Step: 39, Rank: 6, loss = 1.8519939184188843
nid001241: Epoch: 0, Step: 39, Rank: 0, loss = 1.733513355255127
nid001241: Epoch: 0, Step: 39, Rank: 1, loss = 1.6166017055511475
nid001241: [2024-11-12 12:37:01,951] [INFO] [logging.py:128:log_dist] [Rank 0] step=40, skipped=3, lr=[9.646015455642344e-06, 9.646015455642344e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:37:01,951] [INFO] [timer.py:264:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=4.676342118562294, CurrSamplesPerSec=4.691602224965994, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 40, Rank: 7, loss = 1.7893099784851074
nid001244: Epoch: 0, Step: 40, Rank: 4, loss = 1.8173693418502808
nid001241: Epoch: 0, Step: 40, Rank: 0, loss = 1.6710623502731323
nid001244: Epoch: 0, Step: 40, Rank: 5, loss = 1.5809729099273682
nid001241: Epoch: 0, Step: 40, Rank: 1, loss = 1.8419777154922485
nid001241: Epoch: 0, Step: 40, Rank: 3, loss = 1.7029664516448975
nid001244: Epoch: 0, Step: 40, Rank: 6, loss = 1.279073715209961
nid001241: Epoch: 0, Step: 40, Rank: 2, loss = 1.4674742221832275
nid001241: Model Parameters: 8.030 B, Latency: 6.66s, TFLOPs: 4.37, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 41, Rank: 5, loss = 1.7730319499969482
nid001244: Epoch: 0, Step: 41, Rank: 4, loss = 1.6259406805038452
nid001241: Epoch: 0, Step: 41, Rank: 0, loss = 1.5752549171447754
nid001241: Epoch: 0, Step: 41, Rank: 1, loss = 1.7418829202651978
nid001241: Epoch: 0, Step: 41, Rank: 2, loss = 1.5859404802322388
nid001244: Epoch: 0, Step: 41, Rank: 6, loss = 1.7010722160339355
nid001241: Epoch: 0, Step: 41, Rank: 3, loss = 1.9418824911117554
nid001244: Epoch: 0, Step: 41, Rank: 7, loss = 1.50262451171875
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 42, Rank: 7, loss = 1.5223057270050049
nid001244: Epoch: 0, Step: 42, Rank: 6, loss = 1.6027759313583374
nid001244: Epoch: 0, Step: 42, Rank: 4, loss = 1.5852372646331787
nid001241: Epoch: 0, Step: 42, Rank: 0, loss = 1.450339913368225
nid001244: Epoch: 0, Step: 42, Rank: 5, loss = 1.8936840295791626
nid001241: Epoch: 0, Step: 42, Rank: 3, loss = 1.8922057151794434
nid001241: Epoch: 0, Step: 42, Rank: 1, loss = 1.6121248006820679
nid001241: Epoch: 0, Step: 42, Rank: 2, loss = 1.8408077955245972
nid001241: Model Parameters: 8.030 B, Latency: 6.73s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 43, Rank: 7, loss = 1.555382490158081
nid001244: Epoch: 0, Step: 43, Rank: 4, loss = 1.6908305883407593
nid001241: Epoch: 0, Step: 43, Rank: 0, loss = 1.7798856496810913
nid001244: Epoch: 0, Step: 43, Rank: 5, loss = 2.011896848678589
nid001241: Epoch: 0, Step: 43, Rank: 3, loss = 1.7892403602600098
nid001241: Epoch: 0, Step: 43, Rank: 1, loss = 1.9913188219070435
nid001244: Epoch: 0, Step: 43, Rank: 6, loss = 1.512969970703125
nid001241: Epoch: 0, Step: 43, Rank: 2, loss = 1.8604917526245117
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 44, Rank: 6, loss = 1.9609211683273315
nid001241: Epoch: 0, Step: 44, Rank: 3, loss = 1.6198288202285767
nid001241: Epoch: 0, Step: 44, Rank: 0, loss = 1.6279505491256714
nid001241: Epoch: 0, Step: 44, Rank: 1, loss = 1.6384705305099487
nid001244: Epoch: 0, Step: 44, Rank: 7, loss = 1.8138549327850342
nid001241: Epoch: 0, Step: 44, Rank: 2, loss = 1.6534322500228882
nid001244: Epoch: 0, Step: 44, Rank: 4, loss = 1.9794572591781616
nid001244: Epoch: 0, Step: 44, Rank: 5, loss = 1.7640141248703003
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 45, Rank: 4, loss = 1.8008418083190918
nid001241: Epoch: 0, Step: 45, Rank: 0, loss = 2.0247750282287598
nid001241: Epoch: 0, Step: 45, Rank: 3, loss = 1.95419180393219
nid001244: Epoch: 0, Step: 45, Rank: 5, loss = 1.623367190361023
nid001244: Epoch: 0, Step: 45, Rank: 7, loss = 1.8840336799621582
nid001241: Epoch: 0, Step: 45, Rank: 1, loss = 1.8401466608047485
nid001244: Epoch: 0, Step: 45, Rank: 6, loss = 1.6546542644500732
nid001241: Epoch: 0, Step: 45, Rank: 2, loss = 1.8850057125091553
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 46, Rank: 3, loss = 1.621034860610962
nid001244: Epoch: 0, Step: 46, Rank: 7, loss = 1.69350266456604
nid001241: Epoch: 0, Step: 46, Rank: 0, loss = 1.5181740522384644
nid001244: Epoch: 0, Step: 46, Rank: 6, loss = 1.4801081418991089
nid001241: Epoch: 0, Step: 46, Rank: 1, loss = 1.4885236024856567
nid001241: Epoch: 0, Step: 46, Rank: 2, loss = 1.7847381830215454
nid001244: Epoch: 0, Step: 46, Rank: 4, loss = 1.5282442569732666
nid001244: Epoch: 0, Step: 46, Rank: 5, loss = 1.8568811416625977
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 47, Rank: 6, loss = 1.4370760917663574
nid001244: Epoch: 0, Step: 47, Rank: 4, loss = 1.4694983959197998
nid001244: Epoch: 0, Step: 47, Rank: 7, loss = 1.8979755640029907
nid001241: Epoch: 0, Step: 47, Rank: 2, loss = 1.781699776649475
nid001241: Epoch: 0, Step: 47, Rank: 3, loss = 1.5171183347702026
nid001241: Epoch: 0, Step: 47, Rank: 0, loss = 1.5165138244628906
nid001244: Epoch: 0, Step: 47, Rank: 5, loss = 1.7547270059585571
nid001241: Epoch: 0, Step: 47, Rank: 1, loss = 1.6314316987991333
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 48, Rank: 3, loss = 1.48957097530365
nid001241: Epoch: 0, Step: 48, Rank: 2, loss = 1.7530438899993896
nid001244: Epoch: 0, Step: 48, Rank: 7, loss = 1.6547722816467285
nid001241: Epoch: 0, Step: 48, Rank: 1, loss = 1.8743816614151
nid001244: Epoch: 0, Step: 48, Rank: 6, loss = 1.3770841360092163
nid001241: Epoch: 0, Step: 48, Rank: 0, loss = 1.5270546674728394
nid001244: Epoch: 0, Step: 48, Rank: 5, loss = 1.7899094820022583
nid001244: Epoch: 0, Step: 48, Rank: 4, loss = 1.8245999813079834
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 49, Rank: 7, loss = 1.682305097579956
nid001241: Epoch: 0, Step: 49, Rank: 0, loss = 1.62959623336792
nid001241: Epoch: 0, Step: 49, Rank: 3, loss = 1.8282785415649414
nid001244: Epoch: 0, Step: 49, Rank: 6, loss = 1.5458283424377441
nid001241: Epoch: 0, Step: 49, Rank: 1, loss = 1.6319763660430908
nid001241: Epoch: 0, Step: 49, Rank: 2, loss = 1.5371226072311401
nid001244: Epoch: 0, Step: 49, Rank: 5, loss = 2.1073310375213623
nid001244: Epoch: 0, Step: 49, Rank: 4, loss = 1.6946231126785278
nid001241: [2024-11-12 12:38:09,893] [INFO] [logging.py:128:log_dist] [Rank 0] step=50, skipped=3, lr=[9.643571135821272e-06, 9.643571135821272e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:38:09,893] [INFO] [timer.py:264:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=4.684010657180002, CurrSamplesPerSec=4.755406916098824, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.73s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 50, Rank: 6, loss = 1.4889682531356812
nid001244: Epoch: 0, Step: 50, Rank: 7, loss = 1.492826223373413
nid001241: Epoch: 0, Step: 50, Rank: 2, loss = 1.597564458847046
nid001241: Epoch: 0, Step: 50, Rank: 3, loss = 1.778550386428833
nid001244: Epoch: 0, Step: 50, Rank: 4, loss = 1.7894941568374634
nid001241: Epoch: 0, Step: 50, Rank: 1, loss = 1.5570324659347534
nid001241: Epoch: 0, Step: 50, Rank: 0, loss = 1.752943992614746
nid001244: Epoch: 0, Step: 50, Rank: 5, loss = 1.562041163444519
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 51, Rank: 6, loss = 1.5161380767822266
nid001244: Epoch: 0, Step: 51, Rank: 7, loss = 1.4977422952651978
nid001241: Epoch: 0, Step: 51, Rank: 2, loss = 1.47199285030365
nid001241: Epoch: 0, Step: 51, Rank: 3, loss = 1.4875544309616089
nid001241: Epoch: 0, Step: 51, Rank: 0, loss = 1.9185720682144165
nid001241: Epoch: 0, Step: 51, Rank: 1, loss = 1.8956472873687744
nid001244: Epoch: 0, Step: 51, Rank: 4, loss = 1.7458854913711548
nid001244: Epoch: 0, Step: 51, Rank: 5, loss = 1.7410550117492676
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 52, Rank: 7, loss = 1.7992581129074097
nid001244: Epoch: 0, Step: 52, Rank: 5, loss = 1.575286865234375
nid001244: Epoch: 0, Step: 52, Rank: 6, loss = 1.5843828916549683
nid001241: Epoch: 0, Step: 52, Rank: 3, loss = 1.7197246551513672
nid001244: Epoch: 0, Step: 52, Rank: 4, loss = 1.8699530363082886
nid001241: Epoch: 0, Step: 52, Rank: 2, loss = 1.8207720518112183
nid001241: Epoch: 0, Step: 52, Rank: 1, loss = 1.7326023578643799
nid001241: Epoch: 0, Step: 52, Rank: 0, loss = 1.7839726209640503
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 53, Rank: 7, loss = 1.8873628377914429
nid001244: Epoch: 0, Step: 53, Rank: 6, loss = 1.8115037679672241
nid001241: Epoch: 0, Step: 53, Rank: 2, loss = 1.613061547279358
nid001241: Epoch: 0, Step: 53, Rank: 3, loss = 1.536734700202942
nid001244: Epoch: 0, Step: 53, Rank: 5, loss = 1.780265212059021
nid001241: Epoch: 0, Step: 53, Rank: 1, loss = 1.7888239622116089
nid001241: Epoch: 0, Step: 53, Rank: 0, loss = 1.9346177577972412
nid001244: Epoch: 0, Step: 53, Rank: 4, loss = 1.707903504371643
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.14, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 54, Rank: 3, loss = 1.821820855140686
nid001241: Epoch: 0, Step: 54, Rank: 1, loss = 1.594739317893982
nid001241: Epoch: 0, Step: 54, Rank: 2, loss = 1.5562283992767334
nid001244: Epoch: 0, Step: 54, Rank: 7, loss = 1.5135982036590576
nid001241: Epoch: 0, Step: 54, Rank: 0, loss = 1.5889475345611572
nid001244: Epoch: 0, Step: 54, Rank: 6, loss = 1.382895827293396
nid001244: Epoch: 0, Step: 54, Rank: 5, loss = 1.907946228981018
nid001244: Epoch: 0, Step: 54, Rank: 4, loss = 1.5289162397384644
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 55, Rank: 6, loss = 1.8173191547393799
nid001244: Epoch: 0, Step: 55, Rank: 7, loss = 1.8775042295455933
nid001241: Epoch: 0, Step: 55, Rank: 2, loss = 1.3395599126815796
nid001241: Epoch: 0, Step: 55, Rank: 3, loss = 1.4213672876358032
nid001241: Epoch: 0, Step: 55, Rank: 0, loss = 1.9393516778945923Epoch: 0, Step: 55, Rank: 1, loss = 1.4108959436416626
nid001241: 
nid001244: Epoch: 0, Step: 55, Rank: 4, loss = 1.9161243438720703
nid001244: Epoch: 0, Step: 55, Rank: 5, loss = 1.4399034976959229
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 56, Rank: 6, loss = 1.4091827869415283
nid001244: Epoch: 0, Step: 56, Rank: 7, loss = 1.768633484840393
nid001241: Epoch: 0, Step: 56, Rank: 2, loss = 1.6474069356918335
nid001241: Epoch: 0, Step: 56, Rank: 3, loss = 1.9303948879241943Epoch: 0, Step: 56, Rank: 0, loss = 1.4950292110443115
nid001241: 
nid001244: Epoch: 0, Step: 56, Rank: 4, loss = 1.8851555585861206
nid001241: Epoch: 0, Step: 56, Rank: 1, loss = 1.5745532512664795
nid001244: Epoch: 0, Step: 56, Rank: 5, loss = 2.0454185009002686
nid001241: Model Parameters: 8.030 B, Latency: 6.71s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 57, Rank: 6, loss = 1.7044754028320312
nid001244: Epoch: 0, Step: 57, Rank: 7, loss = 1.661666750907898
nid001241: Epoch: 0, Step: 57, Rank: 0, loss = 1.6455074548721313
nid001244: Epoch: 0, Step: 57, Rank: 4, loss = 1.6673082113265991
nid001244: Epoch: 0, Step: 57, Rank: 5, loss = 1.7560560703277588
nid001241: Epoch: 0, Step: 57, Rank: 3, loss = 1.5350068807601929
nid001241: Epoch: 0, Step: 57, Rank: 1, loss = 1.7119455337524414
nid001241: Epoch: 0, Step: 57, Rank: 2, loss = 1.7173389196395874
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 58, Rank: 3, loss = 1.5541433095932007
nid001241: Epoch: 0, Step: 58, Rank: 2, loss = 1.6175401210784912
nid001244: Epoch: 0, Step: 58, Rank: 7, loss = 1.7847678661346436
nid001241: Epoch: 0, Step: 58, Rank: 1, loss = 1.7393423318862915
nid001244: Epoch: 0, Step: 58, Rank: 6, loss = 1.5339488983154297
nid001241: Epoch: 0, Step: 58, Rank: 0, loss = 1.6870462894439697
nid001244: Epoch: 0, Step: 58, Rank: 5, loss = 1.783910870552063
nid001244: Epoch: 0, Step: 58, Rank: 4, loss = 1.4398579597473145
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 59, Rank: 6, loss = 1.7347331047058105
nid001241: Epoch: 0, Step: 59, Rank: 0, loss = 2.088395833969116
nid001241: Epoch: 0, Step: 59, Rank: 3, loss = 1.7720457315444946
nid001244: Epoch: 0, Step: 59, Rank: 7, loss = 1.5771796703338623
nid001241: Epoch: 0, Step: 59, Rank: 1, loss = 1.8619961738586426
nid001241: Epoch: 0, Step: 59, Rank: 2, loss = 1.4616429805755615
nid001244: Epoch: 0, Step: 59, Rank: 5, loss = 1.760806918144226
nid001244: Epoch: 0, Step: 59, Rank: 4, loss = 1.912833333015442
nid001241: [2024-11-12 12:39:18,088] [INFO] [logging.py:128:log_dist] [Rank 0] step=60, skipped=3, lr=[9.640545407218638e-06, 9.640545407218638e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:39:18,088] [INFO] [timer.py:264:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=4.686056025319809, CurrSamplesPerSec=4.775067190665889, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.71s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 60, Rank: 4, loss = 1.809502124786377
nid001244: Epoch: 0, Step: 60, Rank: 7, loss = 1.7084474563598633
nid001241: Epoch: 0, Step: 60, Rank: 0, loss = 1.5560377836227417
nid001241: Epoch: 0, Step: 60, Rank: 3, loss = 1.5474443435668945
nid001241: Epoch: 0, Step: 60, Rank: 1, loss = 1.7757855653762817
nid001244: Epoch: 0, Step: 60, Rank: 5, loss = 1.475883960723877
nid001241: Epoch: 0, Step: 60, Rank: 2, loss = 1.7131561040878296
nid001244: Epoch: 0, Step: 60, Rank: 6, loss = 1.7471460103988647
nid001241: Model Parameters: 8.030 B, Latency: 6.66s, TFLOPs: 4.37, Samples/sec: 0.60, Time/seq 1.66s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 61, Rank: 6, loss = 1.7645467519760132
nid001244: Epoch: 0, Step: 61, Rank: 7, loss = 1.7438652515411377
nid001244: Epoch: 0, Step: 61, Rank: 5, loss = 1.7036792039871216
nid001241: Epoch: 0, Step: 61, Rank: 2, loss = 1.5691514015197754
nid001244: Epoch: 0, Step: 61, Rank: 4, loss = 1.6998591423034668
nid001241: Epoch: 0, Step: 61, Rank: 3, loss = 1.5844004154205322
nid001241: Epoch: 0, Step: 61, Rank: 0, loss = 1.6298216581344604
nid001241: Epoch: 0, Step: 61, Rank: 1, loss = 1.701716423034668
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 62, Rank: 4, loss = 1.3851468563079834
nid001244: Epoch: 0, Step: 62, Rank: 5, loss = 1.450055480003357
nid001244: Epoch: 0, Step: 62, Rank: 7, loss = 1.9280526638031006
nid001241: Epoch: 0, Step: 62, Rank: 0, loss = 1.8741964101791382
nid001241: Epoch: 0, Step: 62, Rank: 3, loss = 1.6620537042617798
nid001244: Epoch: 0, Step: 62, Rank: 6, loss = 1.532735824584961
nid001241: Epoch: 0, Step: 62, Rank: 1, loss = 1.6882838010787964
nid001241: Epoch: 0, Step: 62, Rank: 2, loss = 1.7191176414489746
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 63, Rank: 0, loss = 1.7139300107955933
nid001241: Epoch: 0, Step: 63, Rank: 3, loss = 1.581499695777893
nid001241: Epoch: 0, Step: 63, Rank: 1, loss = 1.5004987716674805
nid001244: Epoch: 0, Step: 63, Rank: 4, loss = 1.7775944471359253
nid001244: Epoch: 0, Step: 63, Rank: 7, loss = 1.671740174293518
nid001241: Epoch: 0, Step: 63, Rank: 2, loss = 1.6263632774353027
nid001244: Epoch: 0, Step: 63, Rank: 5, loss = 1.6333065032958984
nid001244: Epoch: 0, Step: 63, Rank: 6, loss = 1.3805574178695679
nid001241: Model Parameters: 8.030 B, Latency: 6.95s, TFLOPs: 4.18, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 64, Rank: 4, loss = 1.7007372379302979
nid001244: Epoch: 0, Step: 64, Rank: 7, loss = 1.6116071939468384
nid001241: Epoch: 0, Step: 64, Rank: 0, loss = 1.7360937595367432
nid001241: Epoch: 0, Step: 64, Rank: 3, loss = 1.781116247177124
nid001241: Epoch: 0, Step: 64, Rank: 1, loss = 1.5972087383270264
nid001241: Epoch: 0, Step: 64, Rank: 2, loss = 1.720625877380371
nid001244: Epoch: 0, Step: 64, Rank: 5, loss = 1.7226921319961548
nid001244: Epoch: 0, Step: 64, Rank: 6, loss = 1.5661216974258423
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 65, Rank: 7, loss = 1.6214805841445923
nid001244: Epoch: 0, Step: 65, Rank: 6, loss = 1.8258157968521118
nid001241: Epoch: 0, Step: 65, Rank: 2, loss = 1.9463404417037964
nid001241: Epoch: 0, Step: 65, Rank: 3, loss = 1.6129432916641235
nid001244: Epoch: 0, Step: 65, Rank: 4, loss = 1.6315150260925293
nid001244: Epoch: 0, Step: 65, Rank: 5, loss = 1.4827717542648315
nid001241: Epoch: 0, Step: 65, Rank: 0, loss = 1.7019963264465332
nid001241: Epoch: 0, Step: 65, Rank: 1, loss = 1.7227543592453003
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 66, Rank: 7, loss = 1.6659475564956665
nid001241: Epoch: 0, Step: 66, Rank: 0, loss = 1.895576000213623
nid001244: Epoch: 0, Step: 66, Rank: 6, loss = 1.7843143939971924
nid001241: Epoch: 0, Step: 66, Rank: 1, loss = 1.8597497940063477
nid001241: Epoch: 0, Step: 66, Rank: 3, loss = 1.4555689096450806
nid001241: Epoch: 0, Step: 66, Rank: 2, loss = 1.9126684665679932
nid001244: Epoch: 0, Step: 66, Rank: 5, loss = 1.7697333097457886
nid001244: Epoch: 0, Step: 66, Rank: 4, loss = 1.3818355798721313
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 67, Rank: 6, loss = 1.5905230045318604
nid001244: Epoch: 0, Step: 67, Rank: 7, loss = 1.5857487916946411
nid001244: Epoch: 0, Step: 67, Rank: 4, loss = 1.3675477504730225
nid001241: Epoch: 0, Step: 67, Rank: 2, loss = 1.545475959777832
nid001241: Epoch: 0, Step: 67, Rank: 3, loss = 1.8111422061920166
nid001244: Epoch: 0, Step: 67, Rank: 5, loss = 1.7466903924942017
nid001241: Epoch: 0, Step: 67, Rank: 0, loss = 1.8136199712753296
nid001241: Epoch: 0, Step: 67, Rank: 1, loss = 1.8874914646148682
nid001241: Model Parameters: 8.030 B, Latency: 7.06s, TFLOPs: 4.12, Samples/sec: 0.57, Time/seq 1.77s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 68, Rank: 2, loss = 1.3031647205352783
nid001241: Epoch: 0, Step: 68, Rank: 3, loss = 1.81037175655365
nid001241: Epoch: 0, Step: 68, Rank: 0, loss = 1.966767430305481
nid001244: Epoch: 0, Step: 68, Rank: 6, loss = 1.433221697807312
nid001244: Epoch: 0, Step: 68, Rank: 7, loss = 1.727596640586853
nid001244: Epoch: 0, Step: 68, Rank: 4, loss = 1.9075219631195068
nid001241: Epoch: 0, Step: 68, Rank: 1, loss = 1.710239291191101
nid001244: Epoch: 0, Step: 68, Rank: 5, loss = 1.7559030055999756
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 69, Rank: 2, loss = 1.6115270853042603
nid001241: Epoch: 0, Step: 69, Rank: 3, loss = 1.8722169399261475
nid001241: Epoch: 0, Step: 69, Rank: 0, loss = 1.721880316734314
nid001244: Epoch: 0, Step: 69, Rank: 7, loss = 1.425372838973999
nid001244: Epoch: 0, Step: 69, Rank: 6, loss = 1.77243173122406
nid001241: Epoch: 0, Step: 69, Rank: 1, loss = 1.9171011447906494
nid001244: Epoch: 0, Step: 69, Rank: 5, loss = 1.9006366729736328
nid001244: Epoch: 0, Step: 69, Rank: 4, loss = 1.8356847763061523
nid001241: [2024-11-12 12:40:26,694] [INFO] [logging.py:128:log_dist] [Rank 0] step=70, skipped=3, lr=[9.636938634918846e-06, 9.636938634918846e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:40:26,694] [INFO] [timer.py:264:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=4.683374741886711, CurrSamplesPerSec=4.751334209658058, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 70, Rank: 7, loss = 1.3656681776046753
nid001244: Epoch: 0, Step: 70, Rank: 6, loss = 1.0897040367126465
nid001241: Epoch: 0, Step: 70, Rank: 3, loss = 1.692686915397644
nid001244: Epoch: 0, Step: 70, Rank: 5, loss = 1.6387977600097656
nid001241: Epoch: 0, Step: 70, Rank: 2, loss = 1.3794556856155396
nid001241: Epoch: 0, Step: 70, Rank: 1, loss = 1.6591949462890625
nid001241: Epoch: 0, Step: 70, Rank: 0, loss = 1.5557072162628174
nid001244: Epoch: 0, Step: 70, Rank: 4, loss = 1.6354459524154663
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 71, Rank: 6, loss = 1.6321873664855957
nid001244: Epoch: 0, Step: 71, Rank: 7, loss = 2.004417657852173
nid001244: Epoch: 0, Step: 71, Rank: 4, loss = 1.6649249792099
nid001241: Epoch: 0, Step: 71, Rank: 2, loss = 1.7999249696731567
nid001244: Epoch: 0, Step: 71, Rank: 5, loss = 1.3920743465423584
nid001241: Epoch: 0, Step: 71, Rank: 3, loss = 1.8669466972351074
nid001241: Epoch: 0, Step: 71, Rank: 0, loss = 1.8976033926010132
nid001241: Epoch: 0, Step: 71, Rank: 1, loss = 1.4681386947631836
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 72, Rank: 7, loss = 1.7695904970169067
nid001244: Epoch: 0, Step: 72, Rank: 6, loss = 1.5804738998413086
nid001244: Epoch: 0, Step: 72, Rank: 5, loss = 1.6700018644332886
nid001241: Epoch: 0, Step: 72, Rank: 3, loss = 1.556876540184021
nid001241: Epoch: 0, Step: 72, Rank: 2, loss = 1.7586405277252197
nid001241: Epoch: 0, Step: 72, Rank: 1, loss = 1.8134130239486694
nid001241: Epoch: 0, Step: 72, Rank: 0, loss = 1.8664984703063965
nid001244: Epoch: 0, Step: 72, Rank: 4, loss = 1.6089885234832764
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 73, Rank: 3, loss = 1.6871684789657593
nid001241: Epoch: 0, Step: 73, Rank: 0, loss = 1.5722073316574097
nid001244: Epoch: 0, Step: 73, Rank: 4, loss = 1.6557413339614868
nid001241: Epoch: 0, Step: 73, Rank: 1, loss = 1.830706000328064
nid001241: Epoch: 0, Step: 73, Rank: 2, loss = 1.9092870950698853
nid001244: Epoch: 0, Step: 73, Rank: 7, loss = 1.6375203132629395
nid001244: Epoch: 0, Step: 73, Rank: 5, loss = 1.6240675449371338
nid001244: Epoch: 0, Step: 73, Rank: 6, loss = 1.9089292287826538
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 74, Rank: 7, loss = 1.5525232553482056
nid001244: Epoch: 0, Step: 74, Rank: 6, loss = 1.5430574417114258
nid001244: Epoch: 0, Step: 74, Rank: 5, loss = 2.008155345916748
nid001241: Epoch: 0, Step: 74, Rank: 3, loss = 1.6556700468063354
nid001241: Epoch: 0, Step: 74, Rank: 2, loss = 1.8731173276901245
nid001244: Epoch: 0, Step: 74, Rank: 4, loss = 1.6337326765060425
nid001241: Epoch: 0, Step: 74, Rank: 1, loss = 1.7567058801651
nid001241: Epoch: 0, Step: 74, Rank: 0, loss = 1.8405300378799438
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 75, Rank: 7, loss = 1.667645812034607
nid001244: Epoch: 0, Step: 75, Rank: 4, loss = 1.8901911973953247
nid001244: Epoch: 0, Step: 75, Rank: 5, loss = 1.791977047920227
nid001241: Epoch: 0, Step: 75, Rank: 0, loss = 1.6831793785095215
nid001244: Epoch: 0, Step: 75, Rank: 6, loss = 1.7643325328826904
nid001241: Epoch: 0, Step: 75, Rank: 3, loss = 2.0882320404052734
nid001241: Epoch: 0, Step: 75, Rank: 1, loss = 1.7053210735321045
nid001241: Epoch: 0, Step: 75, Rank: 2, loss = 1.6098997592926025
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 76, Rank: 4, loss = 1.775960087776184
nid001244: Epoch: 0, Step: 76, Rank: 7, loss = 1.8631370067596436
nid001241: Epoch: 0, Step: 76, Rank: 0, loss = 1.6869620084762573
nid001241: Epoch: 0, Step: 76, Rank: 3, loss = 1.8235363960266113
nid001244: Epoch: 0, Step: 76, Rank: 5, loss = 1.7220832109451294
nid001241: Epoch: 0, Step: 76, Rank: 1, loss = 1.689658284187317
nid001241: Epoch: 0, Step: 76, Rank: 2, loss = 1.3962905406951904
nid001244: Epoch: 0, Step: 76, Rank: 6, loss = 1.4199949502944946
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 77, Rank: 7, loss = 1.6081409454345703
nid001244: Epoch: 0, Step: 77, Rank: 6, loss = 1.3782087564468384
nid001241: Epoch: 0, Step: 77, Rank: 3, loss = 1.6503913402557373
nid001241: Epoch: 0, Step: 77, Rank: 2, loss = 1.564225435256958
nid001241: Epoch: 0, Step: 77, Rank: 1, loss = 1.533806324005127
nid001241: Epoch: 0, Step: 77, Rank: 0, loss = 1.8921453952789307
nid001244: Epoch: 0, Step: 77, Rank: 5, loss = 1.4415178298950195
nid001244: Epoch: 0, Step: 77, Rank: 4, loss = 1.660370945930481
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 78, Rank: 6, loss = 1.7693675756454468
nid001244: Epoch: 0, Step: 78, Rank: 7, loss = 1.4520424604415894
nid001244: Epoch: 0, Step: 78, Rank: 4, loss = 1.305690884590149
nid001241: Epoch: 0, Step: 78, Rank: 2, loss = 1.962319254875183
nid001241: Epoch: 0, Step: 78, Rank: 0, loss = 1.983557105064392
nid001241: Epoch: 0, Step: 78, Rank: 3, loss = 1.7393282651901245
nid001241: Epoch: 0, Step: 78, Rank: 1, loss = 1.9623733758926392
nid001244: Epoch: 0, Step: 78, Rank: 5, loss = 1.5209468603134155
nid001241: Model Parameters: 8.030 B, Latency: 7.00s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 79, Rank: 6, loss = 1.5205413103103638
nid001244: Epoch: 0, Step: 79, Rank: 7, loss = 1.7216408252716064
nid001241: Epoch: 0, Step: 79, Rank: 2, loss = 1.7068309783935547
nid001244: Epoch: 0, Step: 79, Rank: 4, loss = 2.040393829345703
nid001241: Epoch: 0, Step: 79, Rank: 3, loss = 1.729630470275879
nid001241: Epoch: 0, Step: 79, Rank: 0, loss = 1.7622110843658447
nid001241: Epoch: 0, Step: 79, Rank: 1, loss = 1.547158122062683
nid001244: Epoch: 0, Step: 79, Rank: 5, loss = 1.9502226114273071
nid001241: [2024-11-12 12:41:35,527] [INFO] [logging.py:128:log_dist] [Rank 0] step=80, skipped=3, lr=[9.632751254115037e-06, 9.632751254115037e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:41:35,527] [INFO] [timer.py:264:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=4.6793735041881295, CurrSamplesPerSec=4.6128438740951365, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 80, Rank: 6, loss = 1.7269954681396484
nid001244: Epoch: 0, Step: 80, Rank: 7, loss = 1.7685235738754272
nid001244: Epoch: 0, Step: 80, Rank: 4, loss = 1.872595191001892
nid001241: Epoch: 0, Step: 80, Rank: 2, loss = 1.6130681037902832
nid001244: Epoch: 0, Step: 80, Rank: 5, loss = 1.912990689277649
nid001241: Epoch: 0, Step: 80, Rank: 3, loss = 1.4637916088104248
nid001241: Epoch: 0, Step: 80, Rank: 0, loss = 1.9541808366775513
nid001241: Epoch: 0, Step: 80, Rank: 1, loss = 1.5540049076080322
nid001241: Model Parameters: 8.030 B, Latency: 6.97s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 81, Rank: 4, loss = 1.8945609331130981
nid001244: Epoch: 0, Step: 81, Rank: 7, loss = 1.73661470413208
nid001244: Epoch: 0, Step: 81, Rank: 5, loss = 1.9992643594741821
nid001244: Epoch: 0, Step: 81, Rank: 6, loss = 1.2692211866378784
nid001241: Epoch: 0, Step: 81, Rank: 0, loss = 1.5801494121551514
nid001241: Epoch: 0, Step: 81, Rank: 3, loss = 1.598222017288208
nid001241: Epoch: 0, Step: 81, Rank: 1, loss = 1.6204379796981812
nid001241: Epoch: 0, Step: 81, Rank: 2, loss = 1.7113661766052246
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 82, Rank: 7, loss = 1.7046254873275757
nid001241: Epoch: 0, Step: 82, Rank: 3, loss = 1.5778608322143555
nid001241: Epoch: 0, Step: 82, Rank: 2, loss = 1.829057216644287
nid001244: Epoch: 0, Step: 82, Rank: 5, loss = 2.0826382637023926
nid001244: Epoch: 0, Step: 82, Rank: 4, loss = 1.7987233400344849
nid001241: Epoch: 0, Step: 82, Rank: 0, loss = 1.9049285650253296
nid001244: Epoch: 0, Step: 82, Rank: 6, loss = 1.4903953075408936
nid001241: Epoch: 0, Step: 82, Rank: 1, loss = 1.6821143627166748
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 83, Rank: 6, loss = 1.7709368467330933
nid001244: Epoch: 0, Step: 83, Rank: 7, loss = 1.8607447147369385
nid001244: Epoch: 0, Step: 83, Rank: 4, loss = 1.1632198095321655
nid001241: Epoch: 0, Step: 83, Rank: 2, loss = 1.7143932580947876
nid001244: Epoch: 0, Step: 83, Rank: 5, loss = 1.6053775548934937
nid001241: Epoch: 0, Step: 83, Rank: 3, loss = 1.944412112236023
nid001241: Epoch: 0, Step: 83, Rank: 0, loss = 1.8322190046310425
nid001241: Epoch: 0, Step: 83, Rank: 1, loss = 1.6615245342254639
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 84, Rank: 3, loss = 1.664528250694275
nid001241: Epoch: 0, Step: 84, Rank: 2, loss = 1.4731957912445068
nid001244: Epoch: 0, Step: 84, Rank: 7, loss = 1.7830783128738403
nid001244: Epoch: 0, Step: 84, Rank: 6, loss = 2.0498416423797607
nid001241: Epoch: 0, Step: 84, Rank: 1, loss = 1.8127292394638062
nid001241: Epoch: 0, Step: 84, Rank: 0, loss = 1.814281702041626
nid001244: Epoch: 0, Step: 84, Rank: 5, loss = 1.8200756311416626
nid001244: Epoch: 0, Step: 84, Rank: 4, loss = 1.7428679466247559
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 85, Rank: 6, loss = 1.5121347904205322
nid001244: Epoch: 0, Step: 85, Rank: 4, loss = 1.8546242713928223
nid001244: Epoch: 0, Step: 85, Rank: 7, loss = 1.5623966455459595
nid001241: Epoch: 0, Step: 85, Rank: 2, loss = 1.7867050170898438
nid001241: Epoch: 0, Step: 85, Rank: 3, loss = 1.7363972663879395
nid001244: Epoch: 0, Step: 85, Rank: 5, loss = 1.5862962007522583
nid001241: Epoch: 0, Step: 85, Rank: 0, loss = 1.7779173851013184
nid001241: Epoch: 0, Step: 85, Rank: 1, loss = 1.6113526821136475
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 86, Rank: 6, loss = 2.035853147506714
nid001244: Epoch: 0, Step: 86, Rank: 7, loss = 1.8598856925964355
nid001241: Epoch: 0, Step: 86, Rank: 2, loss = 1.658279538154602
nid001244: Epoch: 0, Step: 86, Rank: 4, loss = 1.4647942781448364
nid001241: Epoch: 0, Step: 86, Rank: 3, loss = 1.6348236799240112
nid001241: Epoch: 0, Step: 86, Rank: 0, loss = 1.5800360441207886
nid001241: Epoch: 0, Step: 86, Rank: 1, loss = 1.1418895721435547
nid001244: Epoch: 0, Step: 86, Rank: 5, loss = 1.741520643234253
nid001241: Model Parameters: 8.030 B, Latency: 7.10s, TFLOPs: 4.10, Samples/sec: 0.56, Time/seq 1.77s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 87, Rank: 7, loss = 1.7308529615402222
nid001244: Epoch: 0, Step: 87, Rank: 6, loss = 1.788886547088623
nid001244: Epoch: 0, Step: 87, Rank: 5, loss = 1.6897709369659424
nid001241: Epoch: 0, Step: 87, Rank: 2, loss = 1.681527018547058
nid001241: Epoch: 0, Step: 87, Rank: 3, loss = 1.6682167053222656
nid001244: Epoch: 0, Step: 87, Rank: 4, loss = 2.1183888912200928
nid001241: Epoch: 0, Step: 87, Rank: 1, loss = 1.7273987531661987
nid001241: Epoch: 0, Step: 87, Rank: 0, loss = 1.5600440502166748
nid001241: Model Parameters: 8.030 B, Latency: 7.36s, TFLOPs: 3.95, Samples/sec: 0.54, Time/seq 1.84s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 88, Rank: 7, loss = 1.6145532131195068
nid001244: Epoch: 0, Step: 88, Rank: 6, loss = 1.6392439603805542
nid001244: Epoch: 0, Step: 88, Rank: 5, loss = 1.3139851093292236
nid001241: Epoch: 0, Step: 88, Rank: 3, loss = 1.6416922807693481
nid001241: Epoch: 0, Step: 88, Rank: 2, loss = 1.644727110862732
nid001241: Epoch: 0, Step: 88, Rank: 1, loss = 1.724038004875183
nid001244: Epoch: 0, Step: 88, Rank: 4, loss = 1.7711639404296875
nid001241: Epoch: 0, Step: 88, Rank: 0, loss = 1.645572304725647
nid001241: Model Parameters: 8.030 B, Latency: 7.17s, TFLOPs: 4.06, Samples/sec: 0.56, Time/seq 1.79s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 89, Rank: 7, loss = 1.7173576354980469
nid001244: Epoch: 0, Step: 89, Rank: 6, loss = 1.8859655857086182
nid001241: Epoch: 0, Step: 89, Rank: 3, loss = 1.9481323957443237
nid001244: Epoch: 0, Step: 89, Rank: 5, loss = 1.6387470960617065
nid001241: Epoch: 0, Step: 89, Rank: 2, loss = 1.6310715675354004
nid001241: Epoch: 0, Step: 89, Rank: 1, loss = 1.524688959121704
nid001241: Epoch: 0, Step: 89, Rank: 0, loss = 1.8291409015655518
nid001244: Epoch: 0, Step: 89, Rank: 4, loss = 1.7377452850341797
nid001241: [2024-11-12 12:42:45,404] [INFO] [logging.py:128:log_dist] [Rank 0] step=90, skipped=3, lr=[9.627983770056567e-06, 9.627983770056567e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:42:45,405] [INFO] [timer.py:264:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=4.66818991672098, CurrSamplesPerSec=4.538915454692949, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 7.05s, TFLOPs: 4.13, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 90, Rank: 7, loss = 1.6465020179748535
nid001244: Epoch: 0, Step: 90, Rank: 4, loss = 1.4945658445358276
nid001244: Epoch: 0, Step: 90, Rank: 5, loss = 1.5074657201766968
nid001241: Epoch: 0, Step: 90, Rank: 0, loss = 1.6306121349334717
nid001241: Epoch: 0, Step: 90, Rank: 3, loss = 1.7590852975845337
nid001244: Epoch: 0, Step: 90, Rank: 6, loss = 1.8102091550827026
nid001241: Epoch: 0, Step: 90, Rank: 1, loss = 1.5828977823257446
nid001241: Epoch: 0, Step: 90, Rank: 2, loss = 1.3837180137634277
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 91, Rank: 4, loss = 1.4853579998016357
nid001244: Epoch: 0, Step: 91, Rank: 7, loss = 1.6339060068130493
nid001241: Epoch: 0, Step: 91, Rank: 0, loss = 1.705905556678772
nid001241: Epoch: 0, Step: 91, Rank: 3, loss = 1.8824738264083862
nid001244: Epoch: 0, Step: 91, Rank: 5, loss = 1.500828504562378
nid001244: Epoch: 0, Step: 91, Rank: 6, loss = 1.7097080945968628
nid001241: Epoch: 0, Step: 91, Rank: 1, loss = 1.7850055694580078
nid001241: Epoch: 0, Step: 91, Rank: 2, loss = 1.162409782409668
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 92, Rank: 3, loss = 1.7439467906951904
nid001241: Epoch: 0, Step: 92, Rank: 2, loss = 1.8255176544189453
nid001241: Epoch: 0, Step: 92, Rank: 1, loss = 1.7086597681045532
nid001244: Epoch: 0, Step: 92, Rank: 7, loss = 1.475216031074524
nid001244: Epoch: 0, Step: 92, Rank: 6, loss = 1.9496127367019653
nid001241: Epoch: 0, Step: 92, Rank: 0, loss = 1.4268497228622437
nid001244: Epoch: 0, Step: 92, Rank: 5, loss = 1.5266329050064087
nid001244: Epoch: 0, Step: 92, Rank: 4, loss = 1.7216007709503174
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 93, Rank: 7, loss = 1.355004906654358
nid001241: Epoch: 0, Step: 93, Rank: 0, loss = 2.037525177001953
nid001244: Epoch: 0, Step: 93, Rank: 4, loss = 1.812400460243225
nid001244: Epoch: 0, Step: 93, Rank: 6, loss = 1.699342131614685
nid001241: Epoch: 0, Step: 93, Rank: 1, loss = 1.4316980838775635
nid001241: Epoch: 0, Step: 93, Rank: 2, loss = 1.542738437652588
nid001244: Epoch: 0, Step: 93, Rank: 5, loss = 1.676352858543396
nid001241: Epoch: 0, Step: 93, Rank: 3, loss = 1.284701943397522
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 94, Rank: 4, loss = 1.5108767747879028
nid001244: Epoch: 0, Step: 94, Rank: 7, loss = 1.8363171815872192
nid001241: Epoch: 0, Step: 94, Rank: 0, loss = 1.4119117259979248
nid001241: Epoch: 0, Step: 94, Rank: 3, loss = 1.5313777923583984
nid001244: Epoch: 0, Step: 94, Rank: 5, loss = 1.7758686542510986
nid001244: Epoch: 0, Step: 94, Rank: 6, loss = 1.5612002611160278
nid001241: Epoch: 0, Step: 94, Rank: 1, loss = 1.4755901098251343
nid001241: Epoch: 0, Step: 94, Rank: 2, loss = 1.674311876296997
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 95, Rank: 2, loss = 1.6217598915100098
nid001244: Epoch: 0, Step: 95, Rank: 6, loss = 1.7585755586624146
nid001241: Epoch: 0, Step: 95, Rank: 3, loss = 1.6401889324188232
nid001244: Epoch: 0, Step: 95, Rank: 7, loss = 1.7242528200149536
nid001244: Epoch: 0, Step: 95, Rank: 5, loss = 1.6170787811279297
nid001244: Epoch: 0, Step: 95, Rank: 4, loss = 1.5312819480895996
nid001241: Epoch: 0, Step: 95, Rank: 0, loss = 1.5807064771652222
nid001241: Epoch: 0, Step: 95, Rank: 1, loss = 1.5858975648880005
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 96, Rank: 4, loss = 1.846052646636963
nid001244: Epoch: 0, Step: 96, Rank: 7, loss = 1.8887039422988892
nid001244: Epoch: 0, Step: 96, Rank: 5, loss = 1.66254460811615
nid001241: Epoch: 0, Step: 96, Rank: 0, loss = 1.9085313081741333
nid001241: Epoch: 0, Step: 96, Rank: 3, loss = 1.7529890537261963
nid001244: Epoch: 0, Step: 96, Rank: 6, loss = 1.8855414390563965
nid001241: Epoch: 0, Step: 96, Rank: 1, loss = 1.672020435333252
nid001241: Epoch: 0, Step: 96, Rank: 2, loss = 2.0175790786743164
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 97, Rank: 2, loss = 1.7538306713104248
nid001241: Epoch: 0, Step: 97, Rank: 3, loss = 1.3969755172729492
nid001244: Epoch: 0, Step: 97, Rank: 4, loss = 1.6862659454345703
nid001241: Epoch: 0, Step: 97, Rank: 0, loss = 1.625167727470398
nid001241: Epoch: 0, Step: 97, Rank: 1, loss = 1.752872109413147
nid001244: Epoch: 0, Step: 97, Rank: 7, loss = 1.644342064857483
nid001244: Epoch: 0, Step: 97, Rank: 5, loss = 1.4463119506835938
nid001244: Epoch: 0, Step: 97, Rank: 6, loss = 1.7410842180252075
nid001241: Model Parameters: 8.030 B, Latency: 6.73s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 98, Rank: 1, loss = 1.5111098289489746
nid001241: Epoch: 0, Step: 98, Rank: 0, loss = 1.7677291631698608
nid001244: Epoch: 0, Step: 98, Rank: 5, loss = 1.3369804620742798
nid001241: Epoch: 0, Step: 98, Rank: 2, loss = 1.590899109840393
nid001244: Epoch: 0, Step: 98, Rank: 6, loss = 1.484947681427002
nid001244: Epoch: 0, Step: 98, Rank: 4, loss = 1.641197681427002
nid001244: Epoch: 0, Step: 98, Rank: 7, loss = 1.4533518552780151
nid001241: Epoch: 0, Step: 98, Rank: 3, loss = 1.7696946859359741
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 99, Rank: 1, loss = 1.5944596529006958
nid001241: Epoch: 0, Step: 99, Rank: 0, loss = 1.6138262748718262
nid001241: Epoch: 0, Step: 99, Rank: 2, loss = 1.3947066068649292
nid001244: Epoch: 0, Step: 99, Rank: 5, loss = 1.2308979034423828
nid001244: Epoch: 0, Step: 99, Rank: 4, loss = 1.7928178310394287
nid001244: Epoch: 0, Step: 99, Rank: 6, loss = 1.5074241161346436
nid001244: Epoch: 0, Step: 99, Rank: 7, loss = 1.8074158430099487
nid001241: Epoch: 0, Step: 99, Rank: 3, loss = 1.6809417009353638
nid001241: [2024-11-12 12:43:53,668] [INFO] [logging.py:128:log_dist] [Rank 0] step=100, skipped=3, lr=[9.62263675798805e-06, 9.62263675798805e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:43:53,669] [INFO] [timer.py:264:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=4.67051450354684, CurrSamplesPerSec=4.6701492758561205, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 100, Rank: 1, loss = 1.842671275138855
nid001241: Epoch: 0, Step: 100, Rank: 0, loss = 1.4385662078857422
nid001244: Epoch: 0, Step: 100, Rank: 5, loss = 1.5801604986190796
nid001241: Epoch: 0, Step: 100, Rank: 2, loss = 1.7923394441604614
nid001244: Epoch: 0, Step: 100, Rank: 4, loss = 1.9357068538665771
nid001244: Epoch: 0, Step: 100, Rank: 6, loss = 1.6331795454025269
nid001244: Epoch: 0, Step: 100, Rank: 7, loss = 1.7009023427963257
nid001241: Epoch: 0, Step: 100, Rank: 3, loss = 1.6259835958480835
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Step 100: GPU Memory Usage
nid001241: GPU 0 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36858 MB
nid001241:   Free:  4101 MB
nid001241:   Usage: 36858/40960 MB (89.99%)
nid001241: 
nid001241: GPU 1 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36860 MB
nid001241:   Free:  4099 MB
nid001241:   Usage: 36860/40960 MB (89.99%)
nid001241: 
nid001241: GPU 2 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36884 MB
nid001241:   Free:  4075 MB
nid001241:   Usage: 36884/40960 MB (90.05%)
nid001241: 
nid001241: GPU 3 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36836 MB
nid001241:   Free:  4123 MB
nid001241:   Usage: 36836/40960 MB (89.93%)
nid001241: 
nid001241: Epoch: 0, Step: 101, Rank: 3, loss = 1.6744825839996338
nid001241: Epoch: 0, Step: 101, Rank: 2, loss = 1.3245543241500854
nid001241: Epoch: 0, Step: 101, Rank: 1, loss = 1.9225895404815674
nid001244: Epoch: 0, Step: 101, Rank: 7, loss = 1.7479912042617798
nid001244: Epoch: 0, Step: 101, Rank: 6, loss = 1.777023434638977
nid001244: Epoch: 0, Step: 101, Rank: 5, loss = 1.671964406967163
nid001241: Epoch: 0, Step: 101, Rank: 0, loss = 1.5293140411376953
nid001244: Epoch: 0, Step: 101, Rank: 4, loss = 1.6091276407241821
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 102, Rank: 3, loss = 1.5815842151641846
nid001241: Epoch: 0, Step: 102, Rank: 0, loss = 1.7671220302581787
nid001241: Epoch: 0, Step: 102, Rank: 1, loss = 1.8330118656158447
nid001244: Epoch: 0, Step: 102, Rank: 7, loss = 1.5001275539398193
nid001244: Epoch: 0, Step: 102, Rank: 4, loss = 1.4767820835113525
nid001244: Epoch: 0, Step: 102, Rank: 5, loss = 1.1728633642196655
nid001244: Epoch: 0, Step: 102, Rank: 6, loss = 1.6465164422988892
nid001241: Epoch: 0, Step: 102, Rank: 2, loss = 1.8029370307922363
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 103, Rank: 2, loss = 1.7728354930877686
nid001241: Epoch: 0, Step: 103, Rank: 3, loss = 1.5873113870620728
nid001244: Epoch: 0, Step: 103, Rank: 6, loss = 2.0824573040008545
nid001244: Epoch: 0, Step: 103, Rank: 7, loss = 1.5973559617996216
nid001241: Epoch: 0, Step: 103, Rank: 0, loss = 1.877567172050476
nid001241: Epoch: 0, Step: 103, Rank: 1, loss = 1.7885087728500366
nid001244: Epoch: 0, Step: 103, Rank: 4, loss = 1.5977303981781006
nid001244: Epoch: 0, Step: 103, Rank: 5, loss = 1.7893048524856567
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 104, Rank: 3, loss = 1.7034324407577515
nid001244: Epoch: 0, Step: 104, Rank: 7, loss = 1.5689592361450195
nid001244: Epoch: 0, Step: 104, Rank: 6, loss = 1.794971227645874
nid001241: Epoch: 0, Step: 104, Rank: 1, loss = 2.074934959411621
nid001241: Epoch: 0, Step: 104, Rank: 0, loss = 1.9344911575317383
nid001244: Epoch: 0, Step: 104, Rank: 4, loss = 1.6240862607955933
nid001241: Epoch: 0, Step: 104, Rank: 2, loss = 2.135392427444458
nid001244: Epoch: 0, Step: 104, Rank: 5, loss = 2.0127127170562744
nid001241: Model Parameters: 8.030 B, Latency: 6.73s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 105, Rank: 3, loss = 1.7064688205718994
nid001241: Epoch: 0, Step: 105, Rank: 2, loss = 1.627134084701538
nid001241: Epoch: 0, Step: 105, Rank: 1, loss = 1.9493600130081177
nid001244: Epoch: 0, Step: 105, Rank: 7, loss = 1.6182653903961182
nid001244: Epoch: 0, Step: 105, Rank: 6, loss = 1.4458866119384766
nid001241: Epoch: 0, Step: 105, Rank: 0, loss = 1.847609281539917
nid001244: Epoch: 0, Step: 105, Rank: 5, loss = 2.0166916847229004
nid001244: Epoch: 0, Step: 105, Rank: 4, loss = 1.4917504787445068
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 106, Rank: 2, loss = 1.808874487876892
nid001244: Epoch: 0, Step: 106, Rank: 7, loss = 1.584209680557251
nid001244: Epoch: 0, Step: 106, Rank: 4, loss = 1.6709175109863281
nid001244: Epoch: 0, Step: 106, Rank: 5, loss = 1.8262157440185547
nid001241: Epoch: 0, Step: 106, Rank: 3, loss = 1.8217517137527466
nid001244: Epoch: 0, Step: 106, Rank: 6, loss = 1.443354606628418
nid001241: Epoch: 0, Step: 106, Rank: 0, loss = 1.3680378198623657
nid001241: Epoch: 0, Step: 106, Rank: 1, loss = 1.4875867366790771
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 107, Rank: 0, loss = 2.0450849533081055
nid001241: Epoch: 0, Step: 107, Rank: 3, loss = 1.7414530515670776
nid001241: Epoch: 0, Step: 107, Rank: 1, loss = 2.031660318374634
nid001241: Epoch: 0, Step: 107, Rank: 2, loss = 1.4630110263824463
nid001244: Epoch: 0, Step: 107, Rank: 4, loss = 1.7034770250320435
nid001244: Epoch: 0, Step: 107, Rank: 7, loss = 1.559810996055603
nid001244: Epoch: 0, Step: 107, Rank: 5, loss = 1.664530634880066
nid001244: Epoch: 0, Step: 107, Rank: 6, loss = 1.7438689470291138
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 108, Rank: 7, loss = 1.350123405456543
nid001244: Epoch: 0, Step: 108, Rank: 4, loss = 1.6204220056533813
nid001244: Epoch: 0, Step: 108, Rank: 6, loss = 1.7711442708969116
nid001241: Epoch: 0, Step: 108, Rank: 3, loss = 1.5854498147964478
nid001244: Epoch: 0, Step: 108, Rank: 5, loss = 1.5700095891952515
nid001241: Epoch: 0, Step: 108, Rank: 1, loss = 1.6813522577285767
nid001241: Epoch: 0, Step: 108, Rank: 0, loss = 1.716202735900879
nid001241: Epoch: 0, Step: 108, Rank: 2, loss = 1.4951045513153076
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 109, Rank: 0, loss = 1.655480980873108
nid001241: Epoch: 0, Step: 109, Rank: 1, loss = 1.6454235315322876
nid001241: Epoch: 0, Step: 109, Rank: 2, loss = 1.5152891874313354
nid001244: Epoch: 0, Step: 109, Rank: 6, loss = 1.3946479558944702
nid001241: Epoch: 0, Step: 109, Rank: 3, loss = 1.5892013311386108
nid001244: Epoch: 0, Step: 109, Rank: 7, loss = 1.5304687023162842
nid001244: Epoch: 0, Step: 109, Rank: 4, loss = 1.697094440460205
nid001244: Epoch: 0, Step: 109, Rank: 5, loss = 1.6104153394699097
nid001241: [2024-11-12 12:45:01,718] [INFO] [logging.py:128:log_dist] [Rank 0] step=110, skipped=3, lr=[9.616710863079949e-06, 9.616710863079949e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:45:01,719] [INFO] [timer.py:264:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=4.673772336811019, CurrSamplesPerSec=4.677201819630319, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 110, Rank: 2, loss = 1.557054042816162
nid001241: Epoch: 0, Step: 110, Rank: 0, loss = 1.72321355342865
nid001241: Epoch: 0, Step: 110, Rank: 3, loss = 1.7908151149749756
nid001244: Epoch: 0, Step: 110, Rank: 4, loss = 1.6650466918945312
nid001241: Epoch: 0, Step: 110, Rank: 1, loss = 1.6983245611190796
nid001244: Epoch: 0, Step: 110, Rank: 7, loss = 1.7585794925689697
nid001244: Epoch: 0, Step: 110, Rank: 5, loss = 1.549381971359253
nid001244: Epoch: 0, Step: 110, Rank: 6, loss = 1.287691593170166
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 111, Rank: 3, loss = 1.4403644800186157
nid001244: Epoch: 0, Step: 111, Rank: 7, loss = 1.6622782945632935
nid001241: Epoch: 0, Step: 111, Rank: 2, loss = 1.6324940919876099
nid001244: Epoch: 0, Step: 111, Rank: 6, loss = 1.1603666543960571
nid001241: Epoch: 0, Step: 111, Rank: 1, loss = 1.7635900974273682
nid001244: Epoch: 0, Step: 111, Rank: 5, loss = 1.542576789855957
nid001241: Epoch: 0, Step: 111, Rank: 0, loss = 1.6458971500396729
nid001244: Epoch: 0, Step: 111, Rank: 4, loss = 1.8058512210845947
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 112, Rank: 5, loss = 1.5179423093795776
nid001244: Epoch: 0, Step: 112, Rank: 4, loss = 1.7908296585083008
nid001241: Epoch: 0, Step: 112, Rank: 1, loss = 1.445556879043579
nid001241: Epoch: 0, Step: 112, Rank: 0, loss = 1.828237771987915
nid001241: Epoch: 0, Step: 112, Rank: 2, loss = 1.6029335260391235
nid001244: Epoch: 0, Step: 112, Rank: 6, loss = 1.5611952543258667
nid001241: Epoch: 0, Step: 112, Rank: 3, loss = 1.6129008531570435
nid001244: Epoch: 0, Step: 112, Rank: 7, loss = 1.70236337184906
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 113, Rank: 2, loss = 1.4615451097488403
nid001241: Epoch: 0, Step: 113, Rank: 3, loss = 1.666895866394043
nid001244: Epoch: 0, Step: 113, Rank: 6, loss = 1.7477940320968628
nid001241: Epoch: 0, Step: 113, Rank: 0, loss = 1.699558138847351
nid001244: Epoch: 0, Step: 113, Rank: 7, loss = 1.7485228776931763
nid001241: Epoch: 0, Step: 113, Rank: 1, loss = 1.7008848190307617
nid001244: Epoch: 0, Step: 113, Rank: 5, loss = 1.5932393074035645
nid001244: Epoch: 0, Step: 113, Rank: 4, loss = 1.73847234249115
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 114, Rank: 7, loss = 1.4189029932022095
nid001244: Epoch: 0, Step: 114, Rank: 5, loss = 1.810044765472412
nid001244: Epoch: 0, Step: 114, Rank: 6, loss = 1.5354841947555542
nid001241: Epoch: 0, Step: 114, Rank: 3, loss = 1.8041473627090454
nid001241: Epoch: 0, Step: 114, Rank: 2, loss = 1.5875400304794312
nid001241: Epoch: 0, Step: 114, Rank: 1, loss = 1.6051923036575317
nid001244: Epoch: 0, Step: 114, Rank: 4, loss = 1.480027198791504
nid001241: Epoch: 0, Step: 114, Rank: 0, loss = 1.5889058113098145
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 115, Rank: 6, loss = 1.6430110931396484
nid001244: Epoch: 0, Step: 115, Rank: 7, loss = 1.6537941694259644
nid001241: Epoch: 0, Step: 115, Rank: 2, loss = 1.9630593061447144
nid001244: Epoch: 0, Step: 115, Rank: 4, loss = 1.4906567335128784
nid001241: Epoch: 0, Step: 115, Rank: 3, loss = 1.8735337257385254
nid001244: Epoch: 0, Step: 115, Rank: 5, loss = 1.8151946067810059
nid001241: Epoch: 0, Step: 115, Rank: 0, loss = 1.7170333862304688
nid001241: Epoch: 0, Step: 115, Rank: 1, loss = 1.6156178712844849
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 116, Rank: 4, loss = 1.697947382926941
nid001244: Epoch: 0, Step: 116, Rank: 7, loss = 1.599995493888855
nid001244: Epoch: 0, Step: 116, Rank: 5, loss = 1.747361183166504
nid001241: Epoch: 0, Step: 116, Rank: 0, loss = 1.200554370880127
nid001244: Epoch: 0, Step: 116, Rank: 6, loss = 1.4280552864074707
nid001241: Epoch: 0, Step: 116, Rank: 3, loss = 1.5969185829162598
nid001241: Epoch: 0, Step: 116, Rank: 1, loss = 1.2690987586975098
nid001241: Epoch: 0, Step: 116, Rank: 2, loss = 1.6429952383041382
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 117, Rank: 6, loss = 1.333184838294983
nid001244: Epoch: 0, Step: 117, Rank: 7, loss = 1.553576946258545
nid001241: Epoch: 0, Step: 117, Rank: 2, loss = 1.5202387571334839
nid001241: Epoch: 0, Step: 117, Rank: 3, loss = 1.5261272192001343
nid001241: Epoch: 0, Step: 117, Rank: 0, loss = 1.576622486114502
nid001241: Epoch: 0, Step: 117, Rank: 1, loss = 1.5245451927185059
nid001244: Epoch: 0, Step: 117, Rank: 4, loss = 1.8108941316604614
nid001244: Epoch: 0, Step: 117, Rank: 5, loss = 1.7366787195205688
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 118, Rank: 3, loss = 1.638598084449768
nid001241: Epoch: 0, Step: 118, Rank: 1, loss = 1.5786237716674805
nid001241: Epoch: 0, Step: 118, Rank: 2, loss = 1.6183902025222778
nid001244: Epoch: 0, Step: 118, Rank: 7, loss = 1.8771227598190308
nid001241: Epoch: 0, Step: 118, Rank: 0, loss = 1.5871531963348389
nid001244: Epoch: 0, Step: 118, Rank: 6, loss = 1.5110934972763062
nid001244: Epoch: 0, Step: 118, Rank: 5, loss = 1.6673195362091064
nid001244: Epoch: 0, Step: 118, Rank: 4, loss = 1.5757187604904175
nid001241: Model Parameters: 8.030 B, Latency: 7.01s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 119, Rank: 4, loss = 1.1657124757766724
nid001244: Epoch: 0, Step: 119, Rank: 6, loss = 1.1563540697097778
nid001244: Epoch: 0, Step: 119, Rank: 7, loss = 1.0450741052627563
nid001244: Epoch: 0, Step: 119, Rank: 5, loss = 1.608688473701477
nid001241: Epoch: 0, Step: 119, Rank: 2, loss = 1.824208378791809
nid001241: Epoch: 0, Step: 119, Rank: 3, loss = 1.9005177021026611
nid001241: Epoch: 0, Step: 119, Rank: 0, loss = 1.8120156526565552
nid001241: Epoch: 0, Step: 119, Rank: 1, loss = 1.6076580286026
nid001241: [2024-11-12 12:46:10,478] [INFO] [logging.py:128:log_dist] [Rank 0] step=120, skipped=3, lr=[9.610206800350732e-06, 9.610206800350732e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:46:10,478] [INFO] [timer.py:264:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=4.67237774619742, CurrSamplesPerSec=4.687278039438659, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 120, Rank: 3, loss = 1.652377963066101
nid001241: Epoch: 0, Step: 120, Rank: 2, loss = 1.7314409017562866
nid001244: Epoch: 0, Step: 120, Rank: 7, loss = 1.9981651306152344
nid001241: Epoch: 0, Step: 120, Rank: 1, loss = 1.933066725730896
nid001244: Epoch: 0, Step: 120, Rank: 6, loss = 1.59340238571167
nid001244: Epoch: 0, Step: 120, Rank: 5, loss = 1.4971718788146973
nid001244: Epoch: 0, Step: 120, Rank: 4, loss = 1.5633189678192139
nid001241: Epoch: 0, Step: 120, Rank: 0, loss = 1.6856518983840942
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 121, Rank: 3, loss = 1.9049606323242188
nid001241: Epoch: 0, Step: 121, Rank: 2, loss = 1.7179538011550903
nid001244: Epoch: 0, Step: 121, Rank: 7, loss = 1.0554920434951782
nid001244: Epoch: 0, Step: 121, Rank: 6, loss = 1.5106604099273682
nid001244: Epoch: 0, Step: 121, Rank: 5, loss = 1.3117073774337769
nid001241: Epoch: 0, Step: 121, Rank: 1, loss = 1.1753884553909302
nid001244: Epoch: 0, Step: 121, Rank: 4, loss = 1.9588693380355835
nid001241: Epoch: 0, Step: 121, Rank: 0, loss = 1.407672643661499
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 122, Rank: 3, loss = 1.6845542192459106
nid001241: Epoch: 0, Step: 122, Rank: 2, loss = 1.6371707916259766
nid001244: Epoch: 0, Step: 122, Rank: 7, loss = 1.7091513872146606
nid001241: Epoch: 0, Step: 122, Rank: 1, loss = 1.4462357759475708
nid001244: Epoch: 0, Step: 122, Rank: 6, loss = 1.5454962253570557
nid001244: Epoch: 0, Step: 122, Rank: 5, loss = 1.565164566040039
nid001241: Epoch: 0, Step: 122, Rank: 0, loss = 1.8562740087509155
nid001244: Epoch: 0, Step: 122, Rank: 4, loss = 1.8251475095748901
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 123, Rank: 3, loss = 1.6953433752059937
nid001241: Epoch: 0, Step: 123, Rank: 1, loss = 1.6992402076721191
nid001241: Epoch: 0, Step: 123, Rank: 2, loss = 2.101480484008789
nid001244: Epoch: 0, Step: 123, Rank: 7, loss = 1.6368403434753418
nid001241: Epoch: 0, Step: 123, Rank: 0, loss = 1.3750672340393066
nid001244: Epoch: 0, Step: 123, Rank: 6, loss = 1.6177111864089966
nid001244: Epoch: 0, Step: 123, Rank: 5, loss = 1.7408955097198486
nid001244: Epoch: 0, Step: 123, Rank: 4, loss = 1.3685826063156128
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 124, Rank: 3, loss = 1.7262709140777588
nid001241: Epoch: 0, Step: 124, Rank: 2, loss = 1.647567868232727
nid001244: Epoch: 0, Step: 124, Rank: 7, loss = 1.7782182693481445
nid001241: Epoch: 0, Step: 124, Rank: 1, loss = 1.7889775037765503
nid001244: Epoch: 0, Step: 124, Rank: 6, loss = 1.5638459920883179
nid001244: Epoch: 0, Step: 124, Rank: 5, loss = 1.8292458057403564
nid001241: Epoch: 0, Step: 124, Rank: 0, loss = 1.6517213582992554
nid001244: Epoch: 0, Step: 124, Rank: 4, loss = 1.6912555694580078
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 125, Rank: 3, loss = 1.8168613910675049
nid001241: Epoch: 0, Step: 125, Rank: 2, loss = 1.5169156789779663
nid001244: Epoch: 0, Step: 125, Rank: 7, loss = 1.5368146896362305
nid001241: Epoch: 0, Step: 125, Rank: 1, loss = 1.9594380855560303
nid001244: Epoch: 0, Step: 125, Rank: 6, loss = 1.4084441661834717
nid001244: Epoch: 0, Step: 125, Rank: 5, loss = 1.8031632900238037
nid001244: Epoch: 0, Step: 125, Rank: 4, loss = 1.750623106956482
nid001241: Epoch: 0, Step: 125, Rank: 0, loss = 1.5795398950576782
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 126, Rank: 3, loss = 1.6422014236450195
nid001244: Epoch: 0, Step: 126, Rank: 7, loss = 1.689990758895874
nid001244: Epoch: 0, Step: 126, Rank: 6, loss = 1.5095199346542358
nid001241: Epoch: 0, Step: 126, Rank: 2, loss = 1.7104971408843994
nid001244: Epoch: 0, Step: 126, Rank: 5, loss = 1.6418049335479736
nid001244: Epoch: 0, Step: 126, Rank: 4, loss = 1.501868724822998
nid001241: Epoch: 0, Step: 126, Rank: 1, loss = 1.5172992944717407
nid001241: Epoch: 0, Step: 126, Rank: 0, loss = 1.4304202795028687
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 127, Rank: 7, loss = 1.8959059715270996
nid001241: Epoch: 0, Step: 127, Rank: 2, loss = 1.4784241914749146
nid001244: Epoch: 0, Step: 127, Rank: 6, loss = 1.7395981550216675
nid001241: Epoch: 0, Step: 127, Rank: 3, loss = 1.320001244544983
nid001244: Epoch: 0, Step: 127, Rank: 4, loss = 2.0211129188537598
nid001244: Epoch: 0, Step: 127, Rank: 5, loss = 1.640251636505127
nid001241: Epoch: 0, Step: 127, Rank: 0, loss = 1.4449774026870728
nid001241: Epoch: 0, Step: 127, Rank: 1, loss = 1.5039764642715454
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 128, Rank: 3, loss = 1.687465786933899
nid001241: Epoch: 0, Step: 128, Rank: 2, loss = 1.600873589515686
nid001241: Epoch: 0, Step: 128, Rank: 1, loss = 1.7066974639892578
nid001244: Epoch: 0, Step: 128, Rank: 7, loss = 1.7523882389068604
nid001244: Epoch: 0, Step: 128, Rank: 6, loss = 1.6201362609863281
nid001241: Epoch: 0, Step: 128, Rank: 0, loss = 1.765883207321167
nid001244: Epoch: 0, Step: 128, Rank: 5, loss = 1.741864562034607
nid001244: Epoch: 0, Step: 128, Rank: 4, loss = 1.8136625289916992
nid001241: Model Parameters: 8.030 B, Latency: 7.05s, TFLOPs: 4.13, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 129, Rank: 6, loss = 1.6204464435577393
nid001244: Epoch: 0, Step: 129, Rank: 7, loss = 1.5287277698516846
nid001244: Epoch: 0, Step: 129, Rank: 4, loss = 1.9437406063079834
nid001241: Epoch: 0, Step: 129, Rank: 2, loss = 2.0290393829345703
nid001241: Epoch: 0, Step: 129, Rank: 3, loss = 1.3840525150299072
nid001244: Epoch: 0, Step: 129, Rank: 5, loss = 1.9370356798171997
nid001241: Epoch: 0, Step: 129, Rank: 1, loss = 1.5625731945037842Epoch: 0, Step: 129, Rank: 0, loss = 1.6706764698028564
nid001241: 
nid001241: [2024-11-12 12:47:20,063] [INFO] [logging.py:128:log_dist] [Rank 0] step=130, skipped=3, lr=[9.603125354580588e-06, 9.603125354580588e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:47:20,063] [INFO] [timer.py:264:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=4.666802437243472, CurrSamplesPerSec=4.580219216445234, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.99s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 130, Rank: 6, loss = 1.5736404657363892
nid001244: Epoch: 0, Step: 130, Rank: 7, loss = 1.5350704193115234
nid001244: Epoch: 0, Step: 130, Rank: 4, loss = 1.7266238927841187
nid001241: Epoch: 0, Step: 130, Rank: 2, loss = 1.9025487899780273
nid001241: Epoch: 0, Step: 130, Rank: 3, loss = 1.6489896774291992
nid001244: Epoch: 0, Step: 130, Rank: 5, loss = 1.682352066040039
nid001241: Epoch: 0, Step: 130, Rank: 0, loss = 1.6502859592437744
nid001241: Epoch: 0, Step: 130, Rank: 1, loss = 1.389665126800537
nid001241: Model Parameters: 8.030 B, Latency: 7.05s, TFLOPs: 4.13, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 131, Rank: 7, loss = 1.530578851699829
nid001244: Epoch: 0, Step: 131, Rank: 4, loss = 1.498956322669983
nid001244: Epoch: 0, Step: 131, Rank: 5, loss = 1.7522262334823608
nid001241: Epoch: 0, Step: 131, Rank: 0, loss = 1.7736986875534058
nid001244: Epoch: 0, Step: 131, Rank: 6, loss = 1.5197869539260864
nid001241: Epoch: 0, Step: 131, Rank: 3, loss = 1.4290894269943237
nid001241: Epoch: 0, Step: 131, Rank: 1, loss = 1.5871559381484985
nid001241: Epoch: 0, Step: 131, Rank: 2, loss = 1.2982990741729736
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.14, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 132, Rank: 7, loss = 1.2737970352172852
nid001244: Epoch: 0, Step: 132, Rank: 4, loss = 1.8673642873764038
nid001241: Epoch: 0, Step: 132, Rank: 0, loss = 1.5050040483474731
nid001244: Epoch: 0, Step: 132, Rank: 5, loss = 1.6881965398788452
nid001241: Epoch: 0, Step: 132, Rank: 3, loss = 1.77013099193573
nid001241: Epoch: 0, Step: 132, Rank: 1, loss = 1.518290400505066
nid001241: Epoch: 0, Step: 132, Rank: 2, loss = 1.6529239416122437
nid001244: Epoch: 0, Step: 132, Rank: 6, loss = 1.5777426958084106
nid001241: Model Parameters: 8.030 B, Latency: 6.95s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 133, Rank: 7, loss = 1.3061317205429077
nid001244: Epoch: 0, Step: 133, Rank: 4, loss = 1.8505315780639648
nid001241: Epoch: 0, Step: 133, Rank: 0, loss = 1.4840089082717896
nid001244: Epoch: 0, Step: 133, Rank: 5, loss = 1.5631318092346191
nid001241: Epoch: 0, Step: 133, Rank: 3, loss = 1.4985960721969604
nid001244: Epoch: 0, Step: 133, Rank: 6, loss = 1.791343092918396
nid001241: Epoch: 0, Step: 133, Rank: 1, loss = 1.6683452129364014
nid001241: Epoch: 0, Step: 133, Rank: 2, loss = 1.7965998649597168
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 134, Rank: 7, loss = 1.7461178302764893
nid001244: Epoch: 0, Step: 134, Rank: 4, loss = 1.7502371072769165
nid001244: Epoch: 0, Step: 134, Rank: 5, loss = 1.277513861656189
nid001241: Epoch: 0, Step: 134, Rank: 0, loss = 1.5421022176742554
nid001244: Epoch: 0, Step: 134, Rank: 6, loss = 1.7127141952514648
nid001241: Epoch: 0, Step: 134, Rank: 1, loss = 1.6622499227523804
nid001241: Epoch: 0, Step: 134, Rank: 3, loss = 1.5899910926818848
nid001241: Epoch: 0, Step: 134, Rank: 2, loss = 1.7689194679260254
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 135, Rank: 4, loss = 1.6622830629348755
nid001244: Epoch: 0, Step: 135, Rank: 7, loss = 1.8261921405792236
nid001241: Epoch: 0, Step: 135, Rank: 0, loss = 1.4634236097335815
nid001244: Epoch: 0, Step: 135, Rank: 5, loss = 1.7256948947906494
nid001241: Epoch: 0, Step: 135, Rank: 3, loss = 1.9319642782211304
nid001244: Epoch: 0, Step: 135, Rank: 6, loss = 1.7002371549606323
nid001241: Epoch: 0, Step: 135, Rank: 1, loss = 1.877531886100769
nid001241: Epoch: 0, Step: 135, Rank: 2, loss = 1.5866297483444214
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 136, Rank: 7, loss = 1.7897138595581055
nid001244: Epoch: 0, Step: 136, Rank: 4, loss = 1.4859634637832642
nid001244: Epoch: 0, Step: 136, Rank: 5, loss = 1.6241841316223145
nid001244: Epoch: 0, Step: 136, Rank: 6, loss = 1.782065749168396
nid001241: Epoch: 0, Step: 136, Rank: 0, loss = 1.8627679347991943
nid001241: Epoch: 0, Step: 136, Rank: 3, loss = 1.4670275449752808
nid001241: Epoch: 0, Step: 136, Rank: 1, loss = 1.6088896989822388
nid001241: Epoch: 0, Step: 136, Rank: 2, loss = 1.7003724575042725
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 137, Rank: 7, loss = 1.6018866300582886
nid001244: Epoch: 0, Step: 137, Rank: 4, loss = 1.9650105237960815
nid001244: Epoch: 0, Step: 137, Rank: 5, loss = 1.6896048784255981
nid001241: Epoch: 0, Step: 137, Rank: 0, loss = 1.3990130424499512
nid001244: Epoch: 0, Step: 137, Rank: 6, loss = 1.6025656461715698
nid001241: Epoch: 0, Step: 137, Rank: 3, loss = 1.6645320653915405
nid001241: Epoch: 0, Step: 137, Rank: 1, loss = 1.6248034238815308
nid001241: Epoch: 0, Step: 137, Rank: 2, loss = 1.729710340499878
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 138, Rank: 6, loss = 1.5160913467407227
nid001244: Epoch: 0, Step: 138, Rank: 7, loss = 1.5683066844940186
nid001241: Epoch: 0, Step: 138, Rank: 2, loss = 1.5597788095474243
nid001241: Epoch: 0, Step: 138, Rank: 3, loss = 1.8847041130065918
nid001241: Epoch: 0, Step: 138, Rank: 0, loss = 1.587659239768982
nid001241: Epoch: 0, Step: 138, Rank: 1, loss = 1.462068796157837
nid001244: Epoch: 0, Step: 138, Rank: 4, loss = 1.6174315214157104
nid001244: Epoch: 0, Step: 138, Rank: 5, loss = 1.6849106550216675
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 139, Rank: 4, loss = 1.7638427019119263
nid001244: Epoch: 0, Step: 139, Rank: 7, loss = 1.3790785074234009
nid001244: Epoch: 0, Step: 139, Rank: 5, loss = 1.5276086330413818
nid001241: Epoch: 0, Step: 139, Rank: 0, loss = 1.5323930978775024
nid001241: Epoch: 0, Step: 139, Rank: 3, loss = 1.5308393239974976
nid001244: Epoch: 0, Step: 139, Rank: 6, loss = 1.9566179513931274
nid001241: Epoch: 0, Step: 139, Rank: 1, loss = 1.5407456159591675
nid001241: Epoch: 0, Step: 139, Rank: 2, loss = 2.119014024734497
nid001241: [2024-11-12 12:48:29,152] [INFO] [logging.py:128:log_dist] [Rank 0] step=140, skipped=3, lr=[9.59546738021675e-06, 9.59546738021675e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:48:29,152] [INFO] [timer.py:264:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=4.664489742861265, CurrSamplesPerSec=4.632702230001077, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 140, Rank: 7, loss = 1.484590768814087
nid001244: Epoch: 0, Step: 140, Rank: 4, loss = 1.622567057609558
nid001244: Epoch: 0, Step: 140, Rank: 5, loss = 1.863659143447876
nid001241: Epoch: 0, Step: 140, Rank: 0, loss = 1.731400966644287
nid001241: Epoch: 0, Step: 140, Rank: 3, loss = 1.6897495985031128
nid001241: Epoch: 0, Step: 140, Rank: 1, loss = 2.015692710876465
nid001241: Epoch: 0, Step: 140, Rank: 2, loss = 1.5092567205429077
nid001244: Epoch: 0, Step: 140, Rank: 6, loss = 1.8342931270599365
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 141, Rank: 6, loss = 1.5701857805252075
nid001244: Epoch: 0, Step: 141, Rank: 7, loss = 1.3299511671066284
nid001241: Epoch: 0, Step: 141, Rank: 2, loss = 1.6543214321136475
nid001244: Epoch: 0, Step: 141, Rank: 4, loss = 1.4435086250305176
nid001241: Epoch: 0, Step: 141, Rank: 3, loss = 1.835195779800415
nid001241: Epoch: 0, Step: 141, Rank: 0, loss = 1.829282522201538
nid001241: Epoch: 0, Step: 141, Rank: 1, loss = 1.2112276554107666
nid001244: Epoch: 0, Step: 141, Rank: 5, loss = 1.9123378992080688
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 142, Rank: 7, loss = 1.2792713642120361
nid001244: Epoch: 0, Step: 142, Rank: 4, loss = 1.5669299364089966
nid001244: Epoch: 0, Step: 142, Rank: 5, loss = 1.3194459676742554
nid001241: Epoch: 0, Step: 142, Rank: 0, loss = 1.4026315212249756
nid001241: Epoch: 0, Step: 142, Rank: 3, loss = 1.7398349046707153
nid001244: Epoch: 0, Step: 142, Rank: 6, loss = 1.6556165218353271
nid001241: Epoch: 0, Step: 142, Rank: 1, loss = 1.6182609796524048
nid001241: Epoch: 0, Step: 142, Rank: 2, loss = 1.5383378267288208
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 143, Rank: 2, loss = 1.1507086753845215
nid001241: Epoch: 0, Step: 143, Rank: 3, loss = 1.681440830230713
nid001244: Epoch: 0, Step: 143, Rank: 7, loss = 1.362630009651184
nid001244: Epoch: 0, Step: 143, Rank: 4, loss = 1.6858817338943481
nid001244: Epoch: 0, Step: 143, Rank: 5, loss = 1.5616503953933716
nid001241: Epoch: 0, Step: 143, Rank: 0, loss = 1.784314751625061
nid001244: Epoch: 0, Step: 143, Rank: 6, loss = 1.5731784105300903
nid001241: Epoch: 0, Step: 143, Rank: 1, loss = 1.7603284120559692
nid001241: Model Parameters: 8.030 B, Latency: 7.01s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 144, Rank: 5, loss = 1.8626004457473755
nid001244: Epoch: 0, Step: 144, Rank: 4, loss = 1.5535680055618286
nid001244: Epoch: 0, Step: 144, Rank: 6, loss = 1.8412611484527588
nid001244: Epoch: 0, Step: 144, Rank: 7, loss = 1.5722870826721191
nid001241: Epoch: 0, Step: 144, Rank: 1, loss = 1.8322659730911255
nid001241: Epoch: 0, Step: 144, Rank: 0, loss = 1.805813193321228
nid001241: Epoch: 0, Step: 144, Rank: 2, loss = 1.5519801378250122
nid001241: Epoch: 0, Step: 144, Rank: 3, loss = 1.6627848148345947
nid001241: Model Parameters: 8.030 B, Latency: 6.99s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 145, Rank: 3, loss = 1.3193917274475098
nid001241: Epoch: 0, Step: 145, Rank: 0, loss = 1.7953763008117676
nid001241: Epoch: 0, Step: 145, Rank: 1, loss = 1.6016212701797485
nid001244: Epoch: 0, Step: 145, Rank: 4, loss = 1.7125914096832275
nid001241: Epoch: 0, Step: 145, Rank: 2, loss = 1.5368335247039795
nid001244: Epoch: 0, Step: 145, Rank: 7, loss = 1.534197449684143
nid001244: Epoch: 0, Step: 145, Rank: 5, loss = 1.570745825767517
nid001244: Epoch: 0, Step: 145, Rank: 6, loss = 1.7680293321609497
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 146, Rank: 7, loss = 1.7510545253753662
nid001244: Epoch: 0, Step: 146, Rank: 4, loss = 1.322106122970581
nid001241: Epoch: 0, Step: 146, Rank: 0, loss = 1.6438192129135132
nid001241: Epoch: 0, Step: 146, Rank: 3, loss = 1.6739003658294678
nid001244: Epoch: 0, Step: 146, Rank: 5, loss = 1.8666373491287231
nid001241: Epoch: 0, Step: 146, Rank: 1, loss = 1.3778493404388428
nid001241: Epoch: 0, Step: 146, Rank: 2, loss = 1.5845004320144653
nid001244: Epoch: 0, Step: 146, Rank: 6, loss = 1.7148693799972534
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 147, Rank: 0, loss = 1.3434137105941772
nid001244: Epoch: 0, Step: 147, Rank: 4, loss = 1.5835984945297241
nid001241: Epoch: 0, Step: 147, Rank: 3, loss = 1.7839604616165161
nid001244: Epoch: 0, Step: 147, Rank: 7, loss = 1.520508050918579
nid001244: Epoch: 0, Step: 147, Rank: 5, loss = 1.8461825847625732
nid001244: Epoch: 0, Step: 147, Rank: 6, loss = 1.5373600721359253
nid001241: Epoch: 0, Step: 147, Rank: 1, loss = 1.4755538702011108
nid001241: Epoch: 0, Step: 147, Rank: 2, loss = 1.9212281703948975
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 148, Rank: 3, loss = 1.5078277587890625
nid001241: Epoch: 0, Step: 148, Rank: 1, loss = 1.6829311847686768
nid001241: Epoch: 0, Step: 148, Rank: 2, loss = 1.530500888824463
nid001244: Epoch: 0, Step: 148, Rank: 7, loss = 1.377602219581604
nid001244: Epoch: 0, Step: 148, Rank: 6, loss = 1.7068089246749878
nid001241: Epoch: 0, Step: 148, Rank: 0, loss = 1.615126609802246
nid001244: Epoch: 0, Step: 148, Rank: 5, loss = 1.483791470527649
nid001244: Epoch: 0, Step: 148, Rank: 4, loss = 1.8734642267227173
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 149, Rank: 2, loss = 1.4140715599060059
nid001244: Epoch: 0, Step: 149, Rank: 7, loss = 1.5275737047195435
nid001244: Epoch: 0, Step: 149, Rank: 6, loss = 1.6266083717346191
nid001241: Epoch: 0, Step: 149, Rank: 3, loss = 1.2750842571258545
nid001244: Epoch: 0, Step: 149, Rank: 5, loss = 1.6376465559005737
nid001244: Epoch: 0, Step: 149, Rank: 4, loss = 1.5969244241714478
nid001241: Epoch: 0, Step: 149, Rank: 0, loss = 1.7636735439300537
nid001241: Epoch: 0, Step: 149, Rank: 1, loss = 1.954127550125122
nid001241: [2024-11-12 12:49:38,167] [INFO] [logging.py:128:log_dist] [Rank 0] step=150, skipped=3, lr=[9.587233801270383e-06, 9.587233801270383e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:49:38,168] [INFO] [timer.py:264:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=4.662831784549011, CurrSamplesPerSec=4.654753173928614, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 150, Rank: 7, loss = 1.7498891353607178
nid001244: Epoch: 0, Step: 150, Rank: 4, loss = 1.693543553352356
nid001244: Epoch: 0, Step: 150, Rank: 5, loss = 1.064378023147583
nid001241: Epoch: 0, Step: 150, Rank: 0, loss = 1.691044569015503
nid001244: Epoch: 0, Step: 150, Rank: 6, loss = 1.4404159784317017
nid001241: Epoch: 0, Step: 150, Rank: 3, loss = 1.8231219053268433
nid001241: Epoch: 0, Step: 150, Rank: 1, loss = 1.603861689567566
nid001241: Epoch: 0, Step: 150, Rank: 2, loss = 1.374848484992981
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 151, Rank: 0, loss = 1.3342196941375732
nid001244: Epoch: 0, Step: 151, Rank: 7, loss = 1.5481239557266235
nid001244: Epoch: 0, Step: 151, Rank: 6, loss = 1.8730151653289795
nid001244: Epoch: 0, Step: 151, Rank: 4, loss = 1.7948874235153198
nid001244: Epoch: 0, Step: 151, Rank: 5, loss = 1.7994277477264404
nid001241: Epoch: 0, Step: 151, Rank: 1, loss = 1.7152526378631592
nid001241: Epoch: 0, Step: 151, Rank: 3, loss = 1.685347557067871Epoch: 0, Step: 151, Rank: 2, loss = 2.179217576980591
nid001241: 
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 152, Rank: 6, loss = 1.6136231422424316
nid001244: Epoch: 0, Step: 152, Rank: 7, loss = 1.7489738464355469
nid001244: Epoch: 0, Step: 152, Rank: 4, loss = 1.6628998517990112
nid001241: Epoch: 0, Step: 152, Rank: 2, loss = 1.616484522819519
nid001241: Epoch: 0, Step: 152, Rank: 3, loss = 1.711855411529541
nid001244: Epoch: 0, Step: 152, Rank: 5, loss = 1.630357027053833
nid001241: Epoch: 0, Step: 152, Rank: 0, loss = 1.7501869201660156
nid001241: Epoch: 0, Step: 152, Rank: 1, loss = 1.677113652229309
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 153, Rank: 7, loss = 1.6358739137649536
nid001244: Epoch: 0, Step: 153, Rank: 4, loss = 1.6887322664260864
nid001244: Epoch: 0, Step: 153, Rank: 5, loss = 1.8111017942428589
nid001241: Epoch: 0, Step: 153, Rank: 0, loss = 1.735729455947876
nid001244: Epoch: 0, Step: 153, Rank: 6, loss = 1.589481234550476
nid001241: Epoch: 0, Step: 153, Rank: 3, loss = 1.6901147365570068
nid001241: Epoch: 0, Step: 153, Rank: 1, loss = 1.6652553081512451
nid001241: Epoch: 0, Step: 153, Rank: 2, loss = 1.2799880504608154
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 154, Rank: 7, loss = 1.677762746810913
nid001244: Epoch: 0, Step: 154, Rank: 6, loss = 1.5917483568191528
nid001241: Epoch: 0, Step: 154, Rank: 3, loss = 1.8536258935928345
nid001241: Epoch: 0, Step: 154, Rank: 2, loss = 1.875217318534851
nid001244: Epoch: 0, Step: 154, Rank: 5, loss = 1.4083325862884521
nid001244: Epoch: 0, Step: 154, Rank: 4, loss = 1.6047768592834473
nid001241: Epoch: 0, Step: 154, Rank: 1, loss = 1.4910632371902466
nid001241: Epoch: 0, Step: 154, Rank: 0, loss = 1.7217447757720947
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 155, Rank: 7, loss = 1.6558493375778198
nid001244: Epoch: 0, Step: 155, Rank: 4, loss = 1.5991127490997314
nid001244: Epoch: 0, Step: 155, Rank: 5, loss = 1.8836826086044312
nid001241: Epoch: 0, Step: 155, Rank: 0, loss = 1.5492497682571411
nid001241: Epoch: 0, Step: 155, Rank: 3, loss = 1.814359426498413
nid001244: Epoch: 0, Step: 155, Rank: 6, loss = 1.7076144218444824
nid001241: Epoch: 0, Step: 155, Rank: 1, loss = 1.7515305280685425
nid001241: Epoch: 0, Step: 155, Rank: 2, loss = 1.7354421615600586
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 156, Rank: 7, loss = 1.6822137832641602
nid001244: Epoch: 0, Step: 156, Rank: 4, loss = 1.581099033355713
nid001244: Epoch: 0, Step: 156, Rank: 5, loss = 1.7845886945724487
nid001244: Epoch: 0, Step: 156, Rank: 6, loss = 1.1075299978256226
nid001241: Epoch: 0, Step: 156, Rank: 0, loss = 1.6792848110198975
nid001241: Epoch: 0, Step: 156, Rank: 3, loss = 1.5881528854370117
nid001241: Epoch: 0, Step: 156, Rank: 1, loss = 1.6944597959518433
nid001241: Epoch: 0, Step: 156, Rank: 2, loss = 1.5815949440002441
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 157, Rank: 4, loss = 1.5101922750473022
nid001244: Epoch: 0, Step: 157, Rank: 7, loss = 1.360904335975647
nid001244: Epoch: 0, Step: 157, Rank: 5, loss = 1.904104232788086
nid001241: Epoch: 0, Step: 157, Rank: 0, loss = 1.2870382070541382
nid001244: Epoch: 0, Step: 157, Rank: 6, loss = 1.7686549425125122
nid001241: Epoch: 0, Step: 157, Rank: 1, loss = 1.535416603088379
nid001241: Epoch: 0, Step: 157, Rank: 3, loss = 1.5170363187789917
nid001241: Epoch: 0, Step: 157, Rank: 2, loss = 1.4442753791809082
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 158, Rank: 6, loss = 1.5366381406784058
nid001244: Epoch: 0, Step: 158, Rank: 7, loss = 1.6443277597427368
nid001241: Epoch: 0, Step: 158, Rank: 2, loss = 1.5439872741699219
nid001244: Epoch: 0, Step: 158, Rank: 4, loss = 1.633413314819336
nid001241: Epoch: 0, Step: 158, Rank: 3, loss = 1.3598369359970093
nid001241: Epoch: 0, Step: 158, Rank: 0, loss = 1.8904129266738892
nid001244: Epoch: 0, Step: 158, Rank: 5, loss = 1.5663797855377197
nid001241: Epoch: 0, Step: 158, Rank: 1, loss = 1.660746693611145
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 159, Rank: 6, loss = 1.985609531402588
nid001244: Epoch: 0, Step: 159, Rank: 7, loss = 1.657289981842041
nid001241: Epoch: 0, Step: 159, Rank: 2, loss = 1.5342447757720947
nid001241: Epoch: 0, Step: 159, Rank: 3, loss = 1.5463265180587769
nid001244: Epoch: 0, Step: 159, Rank: 4, loss = 1.7163355350494385
nid001241: Epoch: 0, Step: 159, Rank: 0, loss = 1.5085252523422241
nid001241: Epoch: 0, Step: 159, Rank: 1, loss = 1.644612193107605
nid001244: Epoch: 0, Step: 159, Rank: 5, loss = 1.7955923080444336
nid001241: [2024-11-12 12:50:46,538] [INFO] [logging.py:128:log_dist] [Rank 0] step=160, skipped=3, lr=[9.578425611205109e-06, 9.578425611205109e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:50:46,539] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=4.664148341052533, CurrSamplesPerSec=4.7034942657221475, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 160, Rank: 7, loss = 1.7465018033981323
nid001244: Epoch: 0, Step: 160, Rank: 5, loss = 1.7444831132888794
nid001244: Epoch: 0, Step: 160, Rank: 6, loss = 1.6289478540420532
nid001241: Epoch: 0, Step: 160, Rank: 3, loss = 1.7206584215164185
nid001244: Epoch: 0, Step: 160, Rank: 4, loss = 1.7586673498153687
nid001241: Epoch: 0, Step: 160, Rank: 2, loss = 1.4988852739334106
nid001241: Epoch: 0, Step: 160, Rank: 1, loss = 1.5693691968917847
nid001241: Epoch: 0, Step: 160, Rank: 0, loss = 1.7453391551971436
nid001241: Model Parameters: 8.030 B, Latency: 7.24s, TFLOPs: 4.02, Samples/sec: 0.55, Time/seq 1.81s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 161, Rank: 7, loss = 1.4156960248947144
nid001244: Epoch: 0, Step: 161, Rank: 6, loss = 1.5412667989730835
nid001244: Epoch: 0, Step: 161, Rank: 5, loss = 1.0540740489959717
nid001241: Epoch: 0, Step: 161, Rank: 3, loss = 1.752016305923462
nid001241: Epoch: 0, Step: 161, Rank: 2, loss = 1.7599307298660278
nid001241: Epoch: 0, Step: 161, Rank: 1, loss = 1.3584132194519043
nid001244: Epoch: 0, Step: 161, Rank: 4, loss = 1.527051329612732
nid001241: Epoch: 0, Step: 161, Rank: 0, loss = 1.7206988334655762
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 162, Rank: 6, loss = 1.5192053318023682
nid001244: Epoch: 0, Step: 162, Rank: 7, loss = 1.7124814987182617
nid001244: Epoch: 0, Step: 162, Rank: 4, loss = 1.628559947013855
nid001241: Epoch: 0, Step: 162, Rank: 2, loss = 1.5328935384750366
nid001241: Epoch: 0, Step: 162, Rank: 3, loss = 1.6860522031784058
nid001241: Epoch: 0, Step: 162, Rank: 0, loss = 1.5681498050689697
nid001244: Epoch: 0, Step: 162, Rank: 5, loss = 1.7150546312332153
nid001241: Epoch: 0, Step: 162, Rank: 1, loss = 1.887341022491455
nid001241: Model Parameters: 8.030 B, Latency: 6.71s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 163, Rank: 6, loss = 1.8351527452468872
nid001244: Epoch: 0, Step: 163, Rank: 7, loss = 1.8042596578598022
nid001244: Epoch: 0, Step: 163, Rank: 4, loss = 1.7053202390670776
nid001241: Epoch: 0, Step: 163, Rank: 2, loss = 1.869955062866211
nid001241: Epoch: 0, Step: 163, Rank: 3, loss = 1.6920899152755737
nid001241: Epoch: 0, Step: 163, Rank: 0, loss = 1.6523773670196533
nid001244: Epoch: 0, Step: 163, Rank: 5, loss = 1.631207823753357
nid001241: Epoch: 0, Step: 163, Rank: 1, loss = 1.8621128797531128
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 164, Rank: 6, loss = 1.1255115270614624
nid001244: Epoch: 0, Step: 164, Rank: 7, loss = 1.8994642496109009
nid001241: Epoch: 0, Step: 164, Rank: 2, loss = 1.529974341392517
nid001244: Epoch: 0, Step: 164, Rank: 4, loss = 1.4109145402908325
nid001241: Epoch: 0, Step: 164, Rank: 3, loss = 1.64104163646698
nid001244: Epoch: 0, Step: 164, Rank: 5, loss = 1.5412596464157104
nid001241: Epoch: 0, Step: 164, Rank: 0, loss = 1.7776111364364624
nid001241: Epoch: 0, Step: 164, Rank: 1, loss = 1.9756724834442139
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 165, Rank: 6, loss = 1.460162878036499
nid001244: Epoch: 0, Step: 165, Rank: 7, loss = 1.9242544174194336
nid001241: Epoch: 0, Step: 165, Rank: 2, loss = 1.577561616897583
nid001241: Epoch: 0, Step: 165, Rank: 3, loss = 1.6204150915145874
nid001244: Epoch: 0, Step: 165, Rank: 4, loss = 1.3453757762908936
nid001241: Epoch: 0, Step: 165, Rank: 0, loss = 1.8570914268493652
nid001241: Epoch: 0, Step: 165, Rank: 1, loss = 1.7044867277145386
nid001244: Epoch: 0, Step: 165, Rank: 5, loss = 1.7484023571014404
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 166, Rank: 4, loss = 1.597967505455017
nid001244: Epoch: 0, Step: 166, Rank: 7, loss = 1.4707672595977783
nid001241: Epoch: 0, Step: 166, Rank: 0, loss = 1.6307411193847656
nid001244: Epoch: 0, Step: 166, Rank: 5, loss = 1.6903727054595947
nid001241: Epoch: 0, Step: 166, Rank: 1, loss = 1.0787427425384521
nid001241: Epoch: 0, Step: 166, Rank: 3, loss = 1.6124340295791626
nid001241: Epoch: 0, Step: 166, Rank: 2, loss = 1.8621419668197632
nid001244: Epoch: 0, Step: 166, Rank: 6, loss = 1.6583000421524048
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 167, Rank: 7, loss = 1.9281443357467651
nid001244: Epoch: 0, Step: 167, Rank: 4, loss = 1.6507530212402344
nid001244: Epoch: 0, Step: 167, Rank: 5, loss = 1.35159170627594
nid001241: Epoch: 0, Step: 167, Rank: 0, loss = 1.4466465711593628
nid001244: Epoch: 0, Step: 167, Rank: 6, loss = 1.1796923875808716
nid001241: Epoch: 0, Step: 167, Rank: 3, loss = 1.6858950853347778
nid001241: Epoch: 0, Step: 167, Rank: 1, loss = 1.8157198429107666
nid001241: Epoch: 0, Step: 167, Rank: 2, loss = 1.658236026763916
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 168, Rank: 7, loss = 1.6247118711471558
nid001244: Epoch: 0, Step: 168, Rank: 4, loss = 1.6644959449768066
nid001244: Epoch: 0, Step: 168, Rank: 5, loss = 1.6917058229446411
nid001241: Epoch: 0, Step: 168, Rank: 0, loss = 1.6774159669876099
nid001241: Epoch: 0, Step: 168, Rank: 3, loss = 1.9276329278945923
nid001244: Epoch: 0, Step: 168, Rank: 6, loss = 1.6158894300460815
nid001241: Epoch: 0, Step: 168, Rank: 1, loss = 1.8263275623321533
nid001241: Epoch: 0, Step: 168, Rank: 2, loss = 1.605208158493042
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 169, Rank: 7, loss = 1.7720168828964233
nid001244: Epoch: 0, Step: 169, Rank: 4, loss = 1.710777997970581
nid001244: Epoch: 0, Step: 169, Rank: 5, loss = 1.7932170629501343
nid001241: Epoch: 0, Step: 169, Rank: 0, loss = 1.4735338687896729
nid001244: Epoch: 0, Step: 169, Rank: 6, loss = 1.4901295900344849
nid001241: Epoch: 0, Step: 169, Rank: 1, loss = 1.5348471403121948
nid001241: Epoch: 0, Step: 169, Rank: 3, loss = 1.2090100049972534
nid001241: Epoch: 0, Step: 169, Rank: 2, loss = 1.4420515298843384
nid001241: [2024-11-12 12:51:55,502] [INFO] [logging.py:128:log_dist] [Rank 0] step=170, skipped=3, lr=[9.569043872817112e-06, 9.569043872817112e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:51:55,503] [INFO] [timer.py:264:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=4.662921559049801, CurrSamplesPerSec=4.601496272917263, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 170, Rank: 7, loss = 1.85444974899292
nid001244: Epoch: 0, Step: 170, Rank: 4, loss = 1.6394665241241455
nid001244: Epoch: 0, Step: 170, Rank: 5, loss = 1.64408278465271
nid001241: Epoch: 0, Step: 170, Rank: 0, loss = 1.613993763923645
nid001241: Epoch: 0, Step: 170, Rank: 3, loss = 1.6748615503311157
nid001241: Epoch: 0, Step: 170, Rank: 1, loss = 1.5725294351577759
nid001244: Epoch: 0, Step: 170, Rank: 6, loss = 1.7861429452896118
nid001241: Epoch: 0, Step: 170, Rank: 2, loss = 1.8571720123291016
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 171, Rank: 4, loss = 1.6923608779907227
nid001244: Epoch: 0, Step: 171, Rank: 7, loss = 1.669568419456482
nid001241: Epoch: 0, Step: 171, Rank: 0, loss = 1.6214078664779663
nid001244: Epoch: 0, Step: 171, Rank: 5, loss = 1.6177027225494385
nid001241: Epoch: 0, Step: 171, Rank: 3, loss = 1.543297529220581
nid001241: Epoch: 0, Step: 171, Rank: 1, loss = 1.8152166604995728
nid001244: Epoch: 0, Step: 171, Rank: 6, loss = 1.7628395557403564
nid001241: Epoch: 0, Step: 171, Rank: 2, loss = 1.879876971244812
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 172, Rank: 0, loss = 1.4970548152923584
nid001241: Epoch: 0, Step: 172, Rank: 3, loss = 1.5379056930541992
nid001241: Epoch: 0, Step: 172, Rank: 1, loss = 1.627170443534851
nid001241: Epoch: 0, Step: 172, Rank: 2, loss = 1.7115626335144043
nid001244: Epoch: 0, Step: 172, Rank: 4, loss = 2.043429374694824
nid001244: Epoch: 0, Step: 172, Rank: 7, loss = 1.5846790075302124
nid001244: Epoch: 0, Step: 172, Rank: 5, loss = 1.8892128467559814
nid001244: Epoch: 0, Step: 172, Rank: 6, loss = 1.662130355834961
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 173, Rank: 0, loss = 1.7236435413360596
nid001241: Epoch: 0, Step: 173, Rank: 3, loss = 1.398988127708435
nid001244: Epoch: 0, Step: 173, Rank: 7, loss = 1.489140510559082
nid001244: Epoch: 0, Step: 173, Rank: 4, loss = 1.603516936302185
nid001244: Epoch: 0, Step: 173, Rank: 5, loss = 1.3201814889907837
nid001244: Epoch: 0, Step: 173, Rank: 6, loss = 1.532202124595642
nid001241: Epoch: 0, Step: 173, Rank: 1, loss = 1.4312292337417603
nid001241: Epoch: 0, Step: 173, Rank: 2, loss = 1.7077608108520508
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 174, Rank: 2, loss = 1.159932017326355
nid001244: Epoch: 0, Step: 174, Rank: 7, loss = 1.8314785957336426
nid001244: Epoch: 0, Step: 174, Rank: 4, loss = 1.552052617073059
nid001244: Epoch: 0, Step: 174, Rank: 5, loss = 1.6349934339523315
nid001244: Epoch: 0, Step: 174, Rank: 6, loss = 1.9220085144042969
nid001241: Epoch: 0, Step: 174, Rank: 3, loss = 1.1940741539001465
nid001241: Epoch: 0, Step: 174, Rank: 1, loss = 1.7841380834579468
nid001241: Epoch: 0, Step: 174, Rank: 0, loss = 1.2431893348693848
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 175, Rank: 6, loss = 1.7131158113479614
nid001244: Epoch: 0, Step: 175, Rank: 7, loss = 1.6208261251449585
nid001241: Epoch: 0, Step: 175, Rank: 2, loss = 1.7786818742752075
nid001241: Epoch: 0, Step: 175, Rank: 3, loss = 1.5543795824050903
nid001241: Epoch: 0, Step: 175, Rank: 1, loss = 1.6592092514038086
nid001241: Epoch: 0, Step: 175, Rank: 0, loss = 1.5554261207580566
nid001244: Epoch: 0, Step: 175, Rank: 5, loss = 1.4084879159927368
nid001244: Epoch: 0, Step: 175, Rank: 4, loss = 1.5519297122955322
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 176, Rank: 2, loss = 1.893226981163025
nid001241: Epoch: 0, Step: 176, Rank: 3, loss = 1.8387991189956665
nid001244: Epoch: 0, Step: 176, Rank: 6, loss = 1.7052663564682007
nid001241: Epoch: 0, Step: 176, Rank: 0, loss = 1.6402606964111328
nid001244: Epoch: 0, Step: 176, Rank: 7, loss = 1.6407982110977173
nid001241: Epoch: 0, Step: 176, Rank: 1, loss = 1.628259301185608
nid001244: Epoch: 0, Step: 176, Rank: 4, loss = 1.9954923391342163
nid001244: Epoch: 0, Step: 176, Rank: 5, loss = 1.5986416339874268
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 177, Rank: 6, loss = 1.2603929042816162
nid001241: Epoch: 0, Step: 177, Rank: 2, loss = 1.6427940130233765
nid001244: Epoch: 0, Step: 177, Rank: 7, loss = 1.4585177898406982
nid001241: Epoch: 0, Step: 177, Rank: 3, loss = 1.7257237434387207
nid001244: Epoch: 0, Step: 177, Rank: 4, loss = 1.8513269424438477
nid001244: Epoch: 0, Step: 177, Rank: 5, loss = 1.4370155334472656
nid001241: Epoch: 0, Step: 177, Rank: 0, loss = 1.7005022764205933
nid001241: Epoch: 0, Step: 177, Rank: 1, loss = 1.3623236417770386
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 178, Rank: 6, loss = 1.3748698234558105
nid001244: Epoch: 0, Step: 178, Rank: 7, loss = 1.2662403583526611
nid001241: Epoch: 0, Step: 178, Rank: 2, loss = 1.747183084487915
nid001241: Epoch: 0, Step: 178, Rank: 3, loss = 1.807720422744751
nid001241: Epoch: 0, Step: 178, Rank: 0, loss = 1.5602151155471802
nid001241: Epoch: 0, Step: 178, Rank: 1, loss = 1.7394070625305176
nid001244: Epoch: 0, Step: 178, Rank: 4, loss = 1.6653170585632324
nid001244: Epoch: 0, Step: 178, Rank: 5, loss = 1.6713377237319946
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 179, Rank: 3, loss = 1.661176323890686
nid001241: Epoch: 0, Step: 179, Rank: 0, loss = 1.8034024238586426
nid001241: Epoch: 0, Step: 179, Rank: 1, loss = 1.632869005203247
nid001241: Epoch: 0, Step: 179, Rank: 2, loss = 1.706032156944275
nid001244: Epoch: 0, Step: 179, Rank: 7, loss = 1.6812539100646973
nid001244: Epoch: 0, Step: 179, Rank: 4, loss = 2.0007588863372803
nid001244: Epoch: 0, Step: 179, Rank: 5, loss = 1.4985729455947876
nid001244: Epoch: 0, Step: 179, Rank: 6, loss = 1.1979477405548096
nid001241: [2024-11-12 12:53:04,140] [INFO] [logging.py:128:log_dist] [Rank 0] step=180, skipped=3, lr=[9.559089718106928e-06, 9.559089718106928e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:53:04,141] [INFO] [timer.py:264:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=4.66307032206876, CurrSamplesPerSec=4.687702535235748, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 180, Rank: 3, loss = 1.4012258052825928
nid001241: Epoch: 0, Step: 180, Rank: 2, loss = 1.5385991334915161
nid001241: Epoch: 0, Step: 180, Rank: 0, loss = 1.8420270681381226
nid001241: Epoch: 0, Step: 180, Rank: 1, loss = 1.8826295137405396
nid001244: Epoch: 0, Step: 180, Rank: 7, loss = 1.839769959449768
nid001244: Epoch: 0, Step: 180, Rank: 6, loss = 1.656504511833191
nid001244: Epoch: 0, Step: 180, Rank: 5, loss = 1.765864610671997
nid001244: Epoch: 0, Step: 180, Rank: 4, loss = 1.5101150274276733
nid001241: Model Parameters: 8.030 B, Latency: 6.70s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 181, Rank: 3, loss = 1.5423580408096313
nid001241: Epoch: 0, Step: 181, Rank: 1, loss = 1.9326351881027222
nid001241: Epoch: 0, Step: 181, Rank: 2, loss = 1.7582874298095703
nid001241: Epoch: 0, Step: 181, Rank: 0, loss = 1.767584204673767
nid001244: Epoch: 0, Step: 181, Rank: 7, loss = 1.5381678342819214
nid001244: Epoch: 0, Step: 181, Rank: 6, loss = 2.007566213607788
nid001244: Epoch: 0, Step: 181, Rank: 5, loss = 1.6358765363693237
nid001244: Epoch: 0, Step: 181, Rank: 4, loss = 1.778975009918213
nid001241: Model Parameters: 8.030 B, Latency: 6.71s, TFLOPs: 4.33, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 182, Rank: 3, loss = 1.852318286895752
nid001241: Epoch: 0, Step: 182, Rank: 2, loss = 1.799890398979187
nid001244: Epoch: 0, Step: 182, Rank: 7, loss = 1.4643573760986328
nid001244: Epoch: 0, Step: 182, Rank: 6, loss = 1.7189018726348877
nid001241: Epoch: 0, Step: 182, Rank: 1, loss = 1.9848209619522095
nid001244: Epoch: 0, Step: 182, Rank: 5, loss = 1.7286665439605713
nid001244: Epoch: 0, Step: 182, Rank: 4, loss = 1.5710248947143555
nid001241: Epoch: 0, Step: 182, Rank: 0, loss = 1.9722949266433716
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 183, Rank: 6, loss = 1.4938552379608154
nid001244: Epoch: 0, Step: 183, Rank: 7, loss = 1.5696481466293335
nid001241: Epoch: 0, Step: 183, Rank: 2, loss = 1.5006157159805298
nid001244: Epoch: 0, Step: 183, Rank: 4, loss = 1.22324538230896
nid001241: Epoch: 0, Step: 183, Rank: 0, loss = 1.8881326913833618Epoch: 0, Step: 183, Rank: 3, loss = 1.5125582218170166
nid001241: 
nid001241: Epoch: 0, Step: 183, Rank: 1, loss = 1.603348731994629
nid001244: Epoch: 0, Step: 183, Rank: 5, loss = 1.6335933208465576
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 184, Rank: 7, loss = 1.317518711090088
nid001244: Epoch: 0, Step: 184, Rank: 4, loss = 1.0723069906234741
nid001244: Epoch: 0, Step: 184, Rank: 6, loss = 1.9149445295333862
nid001244: Epoch: 0, Step: 184, Rank: 5, loss = 1.7839523553848267
nid001241: Epoch: 0, Step: 184, Rank: 3, loss = 1.679771900177002
nid001241: Epoch: 0, Step: 184, Rank: 2, loss = 1.5385005474090576
nid001241: Epoch: 0, Step: 184, Rank: 1, loss = 1.7408150434494019
nid001241: Epoch: 0, Step: 184, Rank: 0, loss = 1.8592441082000732
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 185, Rank: 0, loss = 1.614668369293213
nid001241: Epoch: 0, Step: 185, Rank: 3, loss = 1.159433364868164
nid001241: Epoch: 0, Step: 185, Rank: 1, loss = 1.2013511657714844
nid001244: Epoch: 0, Step: 185, Rank: 4, loss = 1.3990726470947266
nid001244: Epoch: 0, Step: 185, Rank: 7, loss = 1.4047739505767822
nid001244: Epoch: 0, Step: 185, Rank: 5, loss = 1.5956083536148071
nid001241: Epoch: 0, Step: 185, Rank: 2, loss = 1.6740968227386475
nid001244: Epoch: 0, Step: 185, Rank: 6, loss = 1.640778660774231
nid001241: Model Parameters: 8.030 B, Latency: 6.70s, TFLOPs: 4.35, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 186, Rank: 6, loss = 1.8111109733581543
nid001244: Epoch: 0, Step: 186, Rank: 7, loss = 1.659245252609253
nid001241: Epoch: 0, Step: 186, Rank: 0, loss = 1.5542930364608765
nid001241: Epoch: 0, Step: 186, Rank: 3, loss = 1.5589252710342407
nid001244: Epoch: 0, Step: 186, Rank: 4, loss = 1.1939829587936401
nid001244: Epoch: 0, Step: 186, Rank: 5, loss = 1.5539625883102417
nid001241: Epoch: 0, Step: 186, Rank: 1, loss = 1.4322055578231812
nid001241: Epoch: 0, Step: 186, Rank: 2, loss = 1.808974266052246
nid001241: Model Parameters: 8.030 B, Latency: 6.66s, TFLOPs: 4.37, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 187, Rank: 3, loss = 1.3843352794647217
nid001244: Epoch: 0, Step: 187, Rank: 7, loss = 1.4525771141052246
nid001244: Epoch: 0, Step: 187, Rank: 6, loss = 1.6934555768966675
nid001241: Epoch: 0, Step: 187, Rank: 1, loss = 1.608263611793518
nid001241: Epoch: 0, Step: 187, Rank: 2, loss = 1.3048253059387207
nid001244: Epoch: 0, Step: 187, Rank: 5, loss = 1.595664620399475
nid001244: Epoch: 0, Step: 187, Rank: 4, loss = 1.885094165802002
nid001241: Epoch: 0, Step: 187, Rank: 0, loss = 1.3835976123809814
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 188, Rank: 2, loss = 1.4992856979370117Epoch: 0, Step: 188, Rank: 3, loss = 1.6753226518630981
nid001241: 
nid001244: Epoch: 0, Step: 188, Rank: 7, loss = 1.4350377321243286
nid001241: Epoch: 0, Step: 188, Rank: 1, loss = 1.5078750848770142
nid001244: Epoch: 0, Step: 188, Rank: 6, loss = 1.5719319581985474
nid001241: Epoch: 0, Step: 188, Rank: 0, loss = 1.433447241783142
nid001244: Epoch: 0, Step: 188, Rank: 4, loss = 2.078198194503784
nid001244: Epoch: 0, Step: 188, Rank: 5, loss = 1.5778541564941406
nid001241: Model Parameters: 8.030 B, Latency: 6.67s, TFLOPs: 4.37, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 189, Rank: 3, loss = 1.980905532836914
nid001241: Epoch: 0, Step: 189, Rank: 2, loss = 1.7407130002975464
nid001241: Epoch: 0, Step: 189, Rank: 1, loss = 1.5566083192825317
nid001244: Epoch: 0, Step: 189, Rank: 7, loss = 1.5269593000411987
nid001244: Epoch: 0, Step: 189, Rank: 6, loss = 1.63904869556427
nid001241: Epoch: 0, Step: 189, Rank: 0, loss = 1.495527982711792
nid001244: Epoch: 0, Step: 189, Rank: 5, loss = 1.6493620872497559
nid001244: Epoch: 0, Step: 189, Rank: 4, loss = 1.661808967590332
nid001241: [2024-11-12 12:54:11,475] [INFO] [logging.py:128:log_dist] [Rank 0] step=190, skipped=3, lr=[9.548564348142837e-06, 9.548564348142837e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:54:11,475] [INFO] [timer.py:264:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=4.667911106626886, CurrSamplesPerSec=4.794517820351641, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.68s, TFLOPs: 4.36, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 190, Rank: 3, loss = 1.8148891925811768
nid001244: Epoch: 0, Step: 190, Rank: 7, loss = 1.7516857385635376
nid001244: Epoch: 0, Step: 190, Rank: 6, loss = 1.7045491933822632
nid001241: Epoch: 0, Step: 190, Rank: 0, loss = 1.6785860061645508
nid001241: Epoch: 0, Step: 190, Rank: 1, loss = 1.5213590860366821
nid001244: Epoch: 0, Step: 190, Rank: 4, loss = 1.6715495586395264
nid001241: Epoch: 0, Step: 190, Rank: 2, loss = 1.4080716371536255
nid001244: Epoch: 0, Step: 190, Rank: 5, loss = 1.4626981019973755
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 191, Rank: 6, loss = 1.4906611442565918
nid001244: Epoch: 0, Step: 191, Rank: 7, loss = 1.8593114614486694
nid001241: Epoch: 0, Step: 191, Rank: 2, loss = 1.6179406642913818
nid001244: Epoch: 0, Step: 191, Rank: 4, loss = 1.5011160373687744
nid001241: Epoch: 0, Step: 191, Rank: 3, loss = 1.6275619268417358
nid001241: Epoch: 0, Step: 191, Rank: 0, loss = 1.6139017343521118
nid001244: Epoch: 0, Step: 191, Rank: 5, loss = 1.8747382164001465
nid001241: Epoch: 0, Step: 191, Rank: 1, loss = 1.6395834684371948
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 192, Rank: 2, loss = 1.6995418071746826
nid001241: Epoch: 0, Step: 192, Rank: 3, loss = 1.4692676067352295
nid001244: Epoch: 0, Step: 192, Rank: 6, loss = 1.491234540939331
nid001244: Epoch: 0, Step: 192, Rank: 7, loss = 1.6561309099197388
nid001241: Epoch: 0, Step: 192, Rank: 0, loss = 1.476010799407959
nid001244: Epoch: 0, Step: 192, Rank: 4, loss = 1.5894153118133545
nid001244: Epoch: 0, Step: 192, Rank: 5, loss = 1.9595732688903809
nid001241: Epoch: 0, Step: 192, Rank: 1, loss = 1.4545575380325317
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 193, Rank: 7, loss = 1.5186831951141357
nid001244: Epoch: 0, Step: 193, Rank: 6, loss = 1.7779465913772583
nid001241: Epoch: 0, Step: 193, Rank: 3, loss = 1.708069920539856
nid001241: Epoch: 0, Step: 193, Rank: 2, loss = 1.869136929512024
nid001241: Epoch: 0, Step: 193, Rank: 1, loss = 1.5888488292694092
nid001241: Epoch: 0, Step: 193, Rank: 0, loss = 1.6713018417358398
nid001244: Epoch: 0, Step: 193, Rank: 5, loss = 1.5490689277648926
nid001244: Epoch: 0, Step: 193, Rank: 4, loss = 1.7530111074447632
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 194, Rank: 2, loss = 1.4112160205841064
nid001241: Epoch: 0, Step: 194, Rank: 0, loss = 1.769212007522583
nid001241: Epoch: 0, Step: 194, Rank: 3, loss = 1.841737985610962
nid001244: Epoch: 0, Step: 194, Rank: 6, loss = 1.8186852931976318
nid001244: Epoch: 0, Step: 194, Rank: 7, loss = 1.8309177160263062
nid001241: Epoch: 0, Step: 194, Rank: 1, loss = 1.5393741130828857
nid001244: Epoch: 0, Step: 194, Rank: 4, loss = 1.5742881298065186
nid001244: Epoch: 0, Step: 194, Rank: 5, loss = 1.659408450126648
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 195, Rank: 2, loss = 1.7780466079711914
nid001241: Epoch: 0, Step: 195, Rank: 0, loss = 1.7041875123977661
nid001241: Epoch: 0, Step: 195, Rank: 3, loss = 1.3755062818527222
nid001241: Epoch: 0, Step: 195, Rank: 1, loss = 1.6567788124084473
nid001244: Epoch: 0, Step: 195, Rank: 6, loss = 1.5665671825408936
nid001244: Epoch: 0, Step: 195, Rank: 7, loss = 1.8760614395141602
nid001244: Epoch: 0, Step: 195, Rank: 4, loss = 1.4410600662231445
nid001244: Epoch: 0, Step: 195, Rank: 5, loss = 1.6558939218521118
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 196, Rank: 6, loss = 1.2915184497833252
nid001241: Epoch: 0, Step: 196, Rank: 2, loss = 1.4116787910461426
nid001241: Epoch: 0, Step: 196, Rank: 3, loss = 1.5103853940963745
nid001244: Epoch: 0, Step: 196, Rank: 7, loss = 1.8666636943817139
nid001241: Epoch: 0, Step: 196, Rank: 0, loss = 1.950578212738037
nid001244: Epoch: 0, Step: 196, Rank: 5, loss = 1.6358773708343506
nid001244: Epoch: 0, Step: 196, Rank: 4, loss = 1.6891629695892334
nid001241: Epoch: 0, Step: 196, Rank: 1, loss = 1.6080427169799805
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 197, Rank: 2, loss = 1.6375211477279663
nid001241: Epoch: 0, Step: 197, Rank: 3, loss = 1.8866163492202759
nid001244: Epoch: 0, Step: 197, Rank: 6, loss = 1.2976101636886597
nid001241: Epoch: 0, Step: 197, Rank: 0, loss = 1.7966125011444092
nid001244: Epoch: 0, Step: 197, Rank: 7, loss = 1.6791744232177734
nid001244: Epoch: 0, Step: 197, Rank: 4, loss = 1.5450139045715332
nid001244: Epoch: 0, Step: 197, Rank: 5, loss = 1.548427700996399
nid001241: Epoch: 0, Step: 197, Rank: 1, loss = 1.493917465209961
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 198, Rank: 2, loss = 1.7993203401565552
nid001241: Epoch: 0, Step: 198, Rank: 3, loss = 1.701431155204773
nid001244: Epoch: 0, Step: 198, Rank: 6, loss = 1.6340519189834595
nid001244: Epoch: 0, Step: 198, Rank: 7, loss = 1.3786718845367432
nid001241: Epoch: 0, Step: 198, Rank: 0, loss = 1.8634332418441772
nid001244: Epoch: 0, Step: 198, Rank: 4, loss = 1.497183084487915
nid001244: Epoch: 0, Step: 198, Rank: 5, loss = 1.226271390914917
nid001241: Epoch: 0, Step: 198, Rank: 1, loss = 1.6965268850326538
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 199, Rank: 6, loss = 1.8787977695465088
nid001241: Epoch: 0, Step: 199, Rank: 2, loss = 1.9341163635253906
nid001241: Epoch: 0, Step: 199, Rank: 3, loss = 1.8506431579589844
nid001244: Epoch: 0, Step: 199, Rank: 7, loss = 1.8804126977920532
nid001241: Epoch: 0, Step: 199, Rank: 0, loss = 1.7895022630691528
nid001244: Epoch: 0, Step: 199, Rank: 5, loss = 1.7227421998977661
nid001244: Epoch: 0, Step: 199, Rank: 4, loss = 1.703176736831665
nid001241: Epoch: 0, Step: 199, Rank: 1, loss = 1.8806402683258057
nid001241: [2024-11-12 12:55:19,769] [INFO] [logging.py:128:log_dist] [Rank 0] step=200, skipped=3, lr=[9.537469032915956e-06, 9.537469032915956e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:55:19,769] [INFO] [timer.py:264:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=4.668970623730046, CurrSamplesPerSec=4.678441364945664, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 200, Rank: 6, loss = 1.5140204429626465
nid001244: Epoch: 0, Step: 200, Rank: 7, loss = 1.5246827602386475
nid001241: Epoch: 0, Step: 200, Rank: 0, loss = 1.6599225997924805
nid001241: Epoch: 0, Step: 200, Rank: 3, loss = 1.6706446409225464
nid001244: Epoch: 0, Step: 200, Rank: 4, loss = 1.38045334815979
nid001244: Epoch: 0, Step: 200, Rank: 5, loss = 1.872518539428711
nid001241: Epoch: 0, Step: 200, Rank: 1, loss = 1.4668577909469604
nid001241: Epoch: 0, Step: 200, Rank: 2, loss = 1.6186610460281372
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Step 200: GPU Memory Usage
nid001241: GPU 0 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36858 MB
nid001241:   Free:  4101 MB
nid001241:   Usage: 36858/40960 MB (89.99%)
nid001241: 
nid001241: GPU 1 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36860 MB
nid001241:   Free:  4099 MB
nid001241:   Usage: 36860/40960 MB (89.99%)
nid001241: 
nid001241: GPU 2 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36884 MB
nid001241:   Free:  4075 MB
nid001241:   Usage: 36884/40960 MB (90.05%)
nid001241: 
nid001241: GPU 3 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36836 MB
nid001241:   Free:  4123 MB
nid001241:   Usage: 36836/40960 MB (89.93%)
nid001241: 
nid001244: Epoch: 0, Step: 201, Rank: 6, loss = 1.2804957628250122
nid001244: Epoch: 0, Step: 201, Rank: 7, loss = 1.7767198085784912
nid001241: Epoch: 0, Step: 201, Rank: 2, loss = 1.6139808893203735
nid001244: Epoch: 0, Step: 201, Rank: 4, loss = 1.7872898578643799
nid001241: Epoch: 0, Step: 201, Rank: 3, loss = 1.611478328704834
nid001244: Epoch: 0, Step: 201, Rank: 5, loss = 1.874313235282898
nid001241: Epoch: 0, Step: 201, Rank: 0, loss = 1.9462041854858398Epoch: 0, Step: 201, Rank: 1, loss = 1.6883620023727417
nid001241: 
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 202, Rank: 6, loss = 1.5669666528701782
nid001244: Epoch: 0, Step: 202, Rank: 7, loss = 1.619909405708313
nid001241: Epoch: 0, Step: 202, Rank: 2, loss = 1.3404390811920166
nid001241: Epoch: 0, Step: 202, Rank: 3, loss = 1.837782859802246
nid001244: Epoch: 0, Step: 202, Rank: 4, loss = 1.8215675354003906
nid001241: Epoch: 0, Step: 202, Rank: 0, loss = 1.5407634973526
nid001241: Epoch: 0, Step: 202, Rank: 1, loss = 1.7648802995681763
nid001244: Epoch: 0, Step: 202, Rank: 5, loss = 1.8109140396118164
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 203, Rank: 7, loss = 1.7632414102554321
nid001244: Epoch: 0, Step: 203, Rank: 4, loss = 1.758520245552063
nid001244: Epoch: 0, Step: 203, Rank: 5, loss = 1.32356858253479
nid001241: Epoch: 0, Step: 203, Rank: 0, loss = 1.7009518146514893
nid001244: Epoch: 0, Step: 203, Rank: 6, loss = 1.846013069152832
nid001241: Epoch: 0, Step: 203, Rank: 1, loss = 2.0830254554748535
nid001241: Epoch: 0, Step: 203, Rank: 3, loss = 1.65162193775177
nid001241: Epoch: 0, Step: 203, Rank: 2, loss = 1.7461854219436646
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 204, Rank: 3, loss = 1.933164119720459
nid001244: Epoch: 0, Step: 204, Rank: 6, loss = 1.9569729566574097
nid001241: Epoch: 0, Step: 204, Rank: 0, loss = 1.7610266208648682
nid001241: Epoch: 0, Step: 204, Rank: 2, loss = 1.6910337209701538
nid001244: Epoch: 0, Step: 204, Rank: 7, loss = 1.613855242729187
nid001241: Epoch: 0, Step: 204, Rank: 1, loss = 1.7918776273727417
nid001244: Epoch: 0, Step: 204, Rank: 5, loss = 1.6248571872711182
nid001244: Epoch: 0, Step: 204, Rank: 4, loss = 2.194490432739258
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 205, Rank: 6, loss = 1.5836504697799683
nid001244: Epoch: 0, Step: 205, Rank: 7, loss = 1.7334436178207397
nid001241: Epoch: 0, Step: 205, Rank: 3, loss = 1.6105947494506836
nid001241: Epoch: 0, Step: 205, Rank: 0, loss = 1.7046064138412476
nid001241: Epoch: 0, Step: 205, Rank: 2, loss = 1.8073770999908447
nid001241: Epoch: 0, Step: 205, Rank: 1, loss = 1.4302314519882202
nid001244: Epoch: 0, Step: 205, Rank: 4, loss = 1.5692989826202393
nid001244: Epoch: 0, Step: 205, Rank: 5, loss = 1.5939030647277832
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 206, Rank: 6, loss = 1.6633998155593872
nid001244: Epoch: 0, Step: 206, Rank: 4, loss = 1.8399720191955566
nid001244: Epoch: 0, Step: 206, Rank: 7, loss = 1.6138105392456055
nid001244: Epoch: 0, Step: 206, Rank: 5, loss = 1.5155400037765503
nid001241: Epoch: 0, Step: 206, Rank: 3, loss = 1.605291485786438
nid001241: Epoch: 0, Step: 206, Rank: 0, loss = 1.5344233512878418
nid001241: Epoch: 0, Step: 206, Rank: 1, loss = 1.5489157438278198
nid001241: Epoch: 0, Step: 206, Rank: 2, loss = 1.471232533454895
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 207, Rank: 7, loss = 1.7162301540374756
nid001244: Epoch: 0, Step: 207, Rank: 4, loss = 1.301160454750061
nid001244: Epoch: 0, Step: 207, Rank: 5, loss = 1.6084431409835815
nid001241: Epoch: 0, Step: 207, Rank: 0, loss = 1.443604826927185
nid001244: Epoch: 0, Step: 207, Rank: 6, loss = 1.4879587888717651
nid001241: Epoch: 0, Step: 207, Rank: 1, loss = 1.7326879501342773
nid001241: Epoch: 0, Step: 207, Rank: 3, loss = 1.405625343322754
nid001241: Epoch: 0, Step: 207, Rank: 2, loss = 1.8139599561691284
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 208, Rank: 4, loss = 1.6206425428390503
nid001244: Epoch: 0, Step: 208, Rank: 7, loss = 1.602533221244812
nid001241: Epoch: 0, Step: 208, Rank: 0, loss = 1.4244804382324219
nid001241: Epoch: 0, Step: 208, Rank: 3, loss = 1.4686447381973267
nid001241: Epoch: 0, Step: 208, Rank: 1, loss = 1.7949087619781494
nid001244: Epoch: 0, Step: 208, Rank: 5, loss = 1.4649779796600342
nid001241: Epoch: 0, Step: 208, Rank: 2, loss = 1.6669291257858276
nid001244: Epoch: 0, Step: 208, Rank: 6, loss = 1.6842693090438843
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 209, Rank: 7, loss = 1.573443055152893
nid001244: Epoch: 0, Step: 209, Rank: 6, loss = 1.842556118965149
nid001244: Epoch: 0, Step: 209, Rank: 5, loss = 1.6115949153900146
nid001241: Epoch: 0, Step: 209, Rank: 3, loss = 1.6690658330917358
nid001241: Epoch: 0, Step: 209, Rank: 2, loss = 1.6721723079681396
nid001244: Epoch: 0, Step: 209, Rank: 4, loss = 1.6973817348480225
nid001241: Epoch: 0, Step: 209, Rank: 1, loss = 1.4986575841903687
nid001241: Epoch: 0, Step: 209, Rank: 0, loss = 1.611063838005066
nid001241: [2024-11-12 12:56:28,402] [INFO] [logging.py:128:log_dist] [Rank 0] step=210, skipped=3, lr=[9.525805111186993e-06, 9.525805111186993e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:56:28,402] [INFO] [timer.py:264:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=4.668832848679875, CurrSamplesPerSec=4.6957415175338335, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 210, Rank: 6, loss = 1.6360554695129395
nid001244: Epoch: 0, Step: 210, Rank: 7, loss = 1.8486239910125732
nid001241: Epoch: 0, Step: 210, Rank: 2, loss = 2.0626914501190186
nid001244: Epoch: 0, Step: 210, Rank: 4, loss = 1.4207353591918945
nid001241: Epoch: 0, Step: 210, Rank: 3, loss = 1.6457935571670532
nid001241: Epoch: 0, Step: 210, Rank: 1, loss = 1.8323383331298828
nid001241: Epoch: 0, Step: 210, Rank: 0, loss = 1.5703182220458984
nid001244: Epoch: 0, Step: 210, Rank: 5, loss = 1.710965871810913
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 211, Rank: 4, loss = 1.822391390800476
nid001244: Epoch: 0, Step: 211, Rank: 7, loss = 1.6857601404190063
nid001244: Epoch: 0, Step: 211, Rank: 5, loss = 1.7500373125076294
nid001241: Epoch: 0, Step: 211, Rank: 0, loss = 1.5449434518814087
nid001244: Epoch: 0, Step: 211, Rank: 6, loss = 1.5108312368392944
nid001241: Epoch: 0, Step: 211, Rank: 3, loss = 1.421120047569275
nid001241: Epoch: 0, Step: 211, Rank: 1, loss = 1.6458861827850342
nid001241: Epoch: 0, Step: 211, Rank: 2, loss = 1.7591036558151245
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 212, Rank: 4, loss = 1.860991358757019
nid001244: Epoch: 0, Step: 212, Rank: 7, loss = 1.6190614700317383
nid001244: Epoch: 0, Step: 212, Rank: 5, loss = 1.8402962684631348
nid001241: Epoch: 0, Step: 212, Rank: 0, loss = 1.5888996124267578
nid001244: Epoch: 0, Step: 212, Rank: 6, loss = 1.8908497095108032
nid001241: Epoch: 0, Step: 212, Rank: 3, loss = 1.5013737678527832
nid001241: Epoch: 0, Step: 212, Rank: 1, loss = 1.5896532535552979
nid001241: Epoch: 0, Step: 212, Rank: 2, loss = 1.492306113243103
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 213, Rank: 4, loss = 1.1594945192337036
nid001244: Epoch: 0, Step: 213, Rank: 7, loss = 1.3452216386795044
nid001241: Epoch: 0, Step: 213, Rank: 0, loss = 1.7277710437774658
nid001241: Epoch: 0, Step: 213, Rank: 3, loss = 1.761594533920288
nid001244: Epoch: 0, Step: 213, Rank: 5, loss = 1.3446637392044067
nid001244: Epoch: 0, Step: 213, Rank: 6, loss = 1.7799715995788574
nid001241: Epoch: 0, Step: 213, Rank: 1, loss = 1.6695740222930908
nid001241: Epoch: 0, Step: 213, Rank: 2, loss = 1.6519298553466797
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 214, Rank: 7, loss = 1.7671564817428589
nid001244: Epoch: 0, Step: 214, Rank: 4, loss = 1.7027549743652344
nid001244: Epoch: 0, Step: 214, Rank: 5, loss = 1.389249324798584
nid001241: Epoch: 0, Step: 214, Rank: 0, loss = 1.1958485841751099
nid001244: Epoch: 0, Step: 214, Rank: 6, loss = 1.3701143264770508
nid001241: Epoch: 0, Step: 214, Rank: 3, loss = 1.7505724430084229
nid001241: Epoch: 0, Step: 214, Rank: 1, loss = 1.6156435012817383
nid001241: Epoch: 0, Step: 214, Rank: 2, loss = 1.7352734804153442
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 215, Rank: 7, loss = 1.8414582014083862
nid001244: Epoch: 0, Step: 215, Rank: 6, loss = 1.7346535921096802
nid001241: Epoch: 0, Step: 215, Rank: 3, loss = 1.6550209522247314
nid001244: Epoch: 0, Step: 215, Rank: 5, loss = 1.532382845878601
nid001241: Epoch: 0, Step: 215, Rank: 2, loss = 1.6994117498397827
nid001241: Epoch: 0, Step: 215, Rank: 1, loss = 1.7294138669967651
nid001241: Epoch: 0, Step: 215, Rank: 0, loss = 1.6253142356872559
nid001244: Epoch: 0, Step: 215, Rank: 4, loss = 1.5452284812927246
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 216, Rank: 4, loss = 1.66740083694458
nid001244: Epoch: 0, Step: 216, Rank: 7, loss = 1.7822747230529785
nid001241: Epoch: 0, Step: 216, Rank: 0, loss = 1.4217908382415771
nid001241: Epoch: 0, Step: 216, Rank: 3, loss = 1.80312979221344
nid001244: Epoch: 0, Step: 216, Rank: 5, loss = 1.6118721961975098
nid001241: Epoch: 0, Step: 216, Rank: 1, loss = 1.579998254776001
nid001241: Epoch: 0, Step: 216, Rank: 2, loss = 1.7037084102630615
nid001244: Epoch: 0, Step: 216, Rank: 6, loss = 1.5545754432678223
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 217, Rank: 7, loss = 1.5434361696243286
nid001244: Epoch: 0, Step: 217, Rank: 4, loss = 1.636008381843567
nid001241: Epoch: 0, Step: 217, Rank: 0, loss = 1.7544269561767578
nid001244: Epoch: 0, Step: 217, Rank: 5, loss = 1.7081252336502075
nid001241: Epoch: 0, Step: 217, Rank: 3, loss = 1.8508723974227905
nid001241: Epoch: 0, Step: 217, Rank: 1, loss = 1.8267656564712524
nid001244: Epoch: 0, Step: 217, Rank: 6, loss = 1.486860752105713
nid001241: Epoch: 0, Step: 217, Rank: 2, loss = 1.552599310874939
nid001241: Model Parameters: 8.030 B, Latency: 6.95s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 218, Rank: 7, loss = 1.4421833753585815
nid001244: Epoch: 0, Step: 218, Rank: 4, loss = 1.6425706148147583
nid001244: Epoch: 0, Step: 218, Rank: 5, loss = 1.867626667022705
nid001241: Epoch: 0, Step: 218, Rank: 0, loss = 1.6023520231246948
nid001241: Epoch: 0, Step: 218, Rank: 3, loss = 1.5860624313354492
nid001244: Epoch: 0, Step: 218, Rank: 6, loss = 1.7092138528823853
nid001241: Epoch: 0, Step: 218, Rank: 1, loss = 1.5832792520523071
nid001241: Epoch: 0, Step: 218, Rank: 2, loss = 1.5710798501968384
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 219, Rank: 4, loss = 1.415068507194519
nid001244: Epoch: 0, Step: 219, Rank: 5, loss = 1.4244341850280762
nid001244: Epoch: 0, Step: 219, Rank: 6, loss = 1.5479528903961182
nid001241: Epoch: 0, Step: 219, Rank: 0, loss = 1.4710357189178467
nid001241: Epoch: 0, Step: 219, Rank: 1, loss = 1.8323734998703003
nid001244: Epoch: 0, Step: 219, Rank: 7, loss = 1.51753830909729
nid001241: Epoch: 0, Step: 219, Rank: 2, loss = 1.4298583269119263
nid001241: Epoch: 0, Step: 219, Rank: 3, loss = 1.9847149848937988
nid001241: [2024-11-12 12:57:36,832] [INFO] [logging.py:128:log_dist] [Rank 0] step=220, skipped=3, lr=[9.513573990324714e-06, 9.513573990324714e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:57:36,832] [INFO] [timer.py:264:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=4.669329538482664, CurrSamplesPerSec=4.688128909209427, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 220, Rank: 4, loss = 1.7780994176864624
nid001244: Epoch: 0, Step: 220, Rank: 7, loss = 2.04628324508667
nid001241: Epoch: 0, Step: 220, Rank: 0, loss = 1.773434042930603
nid001241: Epoch: 0, Step: 220, Rank: 3, loss = 1.4853683710098267
nid001244: Epoch: 0, Step: 220, Rank: 5, loss = 1.686294674873352
nid001241: Epoch: 0, Step: 220, Rank: 1, loss = 1.6882926225662231
nid001241: Epoch: 0, Step: 220, Rank: 2, loss = 1.6561695337295532
nid001244: Epoch: 0, Step: 220, Rank: 6, loss = 1.6690380573272705
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 221, Rank: 4, loss = 1.3958426713943481
nid001241: Epoch: 0, Step: 221, Rank: 3, loss = 1.6804893016815186
nid001241: Epoch: 0, Step: 221, Rank: 0, loss = 1.5407774448394775
nid001244: Epoch: 0, Step: 221, Rank: 7, loss = 1.6153864860534668
nid001241: Epoch: 0, Step: 221, Rank: 1, loss = 1.7423053979873657
nid001241: Epoch: 0, Step: 221, Rank: 2, loss = 1.7400604486465454
nid001244: Epoch: 0, Step: 221, Rank: 5, loss = 1.686299204826355
nid001244: Epoch: 0, Step: 221, Rank: 6, loss = 1.7247304916381836
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 222, Rank: 6, loss = 1.4820424318313599
nid001244: Epoch: 0, Step: 222, Rank: 4, loss = 1.6584821939468384
nid001244: Epoch: 0, Step: 222, Rank: 7, loss = 1.6128003597259521
nid001241: Epoch: 0, Step: 222, Rank: 2, loss = 1.6037262678146362
nid001244: Epoch: 0, Step: 222, Rank: 5, loss = 1.3556265830993652
nid001241: Epoch: 0, Step: 222, Rank: 3, loss = 1.4668891429901123
nid001241: Epoch: 0, Step: 222, Rank: 0, loss = 1.6655994653701782
nid001241: Epoch: 0, Step: 222, Rank: 1, loss = 1.686700701713562
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 223, Rank: 5, loss = 1.7414767742156982
nid001241: Epoch: 0, Step: 223, Rank: 1, loss = 1.4024569988250732
nid001241: Epoch: 0, Step: 223, Rank: 0, loss = 1.7873733043670654
nid001244: Epoch: 0, Step: 223, Rank: 4, loss = 1.2737994194030762
nid001244: Epoch: 0, Step: 223, Rank: 6, loss = 1.574271559715271
nid001241: Epoch: 0, Step: 223, Rank: 2, loss = 1.9312885999679565
nid001241: Epoch: 0, Step: 223, Rank: 3, loss = 1.4961495399475098
nid001244: Epoch: 0, Step: 223, Rank: 7, loss = 1.6622403860092163
nid001241: Model Parameters: 8.030 B, Latency: 6.69s, TFLOPs: 4.35, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 224, Rank: 4, loss = 1.8998397588729858
nid001244: Epoch: 0, Step: 224, Rank: 5, loss = 1.5909092426300049
nid001244: Epoch: 0, Step: 224, Rank: 6, loss = 1.5444315671920776
nid001241: Epoch: 0, Step: 224, Rank: 0, loss = 1.801625370979309
nid001241: Epoch: 0, Step: 224, Rank: 3, loss = 1.674142837524414
nid001241: Epoch: 0, Step: 224, Rank: 1, loss = 1.527081847190857
nid001241: Epoch: 0, Step: 224, Rank: 2, loss = 1.4617843627929688
nid001244: Epoch: 0, Step: 224, Rank: 7, loss = 1.206315040588379
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 225, Rank: 7, loss = 1.6754462718963623
nid001244: Epoch: 0, Step: 225, Rank: 6, loss = 1.6857986450195312
nid001244: Epoch: 0, Step: 225, Rank: 5, loss = 1.3176571130752563
nid001241: Epoch: 0, Step: 225, Rank: 3, loss = 1.659142017364502
nid001241: Epoch: 0, Step: 225, Rank: 2, loss = 1.6396139860153198
nid001241: Epoch: 0, Step: 225, Rank: 1, loss = 1.7448073625564575
nid001241: Epoch: 0, Step: 225, Rank: 0, loss = 1.8538520336151123
nid001244: Epoch: 0, Step: 225, Rank: 4, loss = 1.7003002166748047
nid001241: Model Parameters: 8.030 B, Latency: 7.07s, TFLOPs: 4.11, Samples/sec: 0.57, Time/seq 1.77s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 226, Rank: 7, loss = 1.5576744079589844
nid001244: Epoch: 0, Step: 226, Rank: 4, loss = 1.9539767503738403
nid001244: Epoch: 0, Step: 226, Rank: 5, loss = 1.4722884893417358
nid001241: Epoch: 0, Step: 226, Rank: 0, loss = 2.0299735069274902
nid001241: Epoch: 0, Step: 226, Rank: 3, loss = 1.691280722618103
nid001244: Epoch: 0, Step: 226, Rank: 6, loss = 1.66347074508667
nid001241: Epoch: 0, Step: 226, Rank: 1, loss = 1.5203256607055664
nid001241: Epoch: 0, Step: 226, Rank: 2, loss = 1.7393018007278442
nid001241: Model Parameters: 8.030 B, Latency: 7.24s, TFLOPs: 4.02, Samples/sec: 0.55, Time/seq 1.81s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 227, Rank: 6, loss = 1.3212918043136597
nid001244: Epoch: 0, Step: 227, Rank: 7, loss = 1.9099183082580566
nid001241: Epoch: 0, Step: 227, Rank: 3, loss = 1.4708553552627563
nid001244: Epoch: 0, Step: 227, Rank: 4, loss = 1.8160760402679443
nid001241: Epoch: 0, Step: 227, Rank: 0, loss = 1.8391036987304688
nid001244: Epoch: 0, Step: 227, Rank: 5, loss = 1.4816242456436157
nid001241: Epoch: 0, Step: 227, Rank: 1, loss = 1.733690619468689
nid001241: Epoch: 0, Step: 227, Rank: 2, loss = 1.4575181007385254
nid001241: Model Parameters: 8.030 B, Latency: 7.03s, TFLOPs: 4.14, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 228, Rank: 6, loss = 1.892230749130249
nid001244: Epoch: 0, Step: 228, Rank: 7, loss = 1.5849395990371704
nid001241: Epoch: 0, Step: 228, Rank: 2, loss = 1.8613883256912231
nid001241: Epoch: 0, Step: 228, Rank: 3, loss = 1.534326195716858
nid001244: Epoch: 0, Step: 228, Rank: 5, loss = 1.6121286153793335
nid001244: Epoch: 0, Step: 228, Rank: 4, loss = 1.6757773160934448
nid001241: Epoch: 0, Step: 228, Rank: 1, loss = 1.5176783800125122Epoch: 0, Step: 228, Rank: 0, loss = 1.7453620433807373
nid001241: 
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 229, Rank: 6, loss = 1.7460960149765015
nid001244: Epoch: 0, Step: 229, Rank: 7, loss = 1.7084438800811768
nid001241: Epoch: 0, Step: 229, Rank: 2, loss = 1.877371907234192
nid001244: Epoch: 0, Step: 229, Rank: 4, loss = 1.1914159059524536
nid001244: Epoch: 0, Step: 229, Rank: 5, loss = 1.6408288478851318
nid001241: Epoch: 0, Step: 229, Rank: 3, loss = 1.3379186391830444
nid001241: Epoch: 0, Step: 229, Rank: 0, loss = 1.7858954668045044
nid001241: Epoch: 0, Step: 229, Rank: 1, loss = 1.7074310779571533
nid001241: [2024-11-12 12:58:46,038] [INFO] [logging.py:128:log_dist] [Rank 0] step=230, skipped=3, lr=[9.500777146136134e-06, 9.500777146136134e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:58:46,039] [INFO] [timer.py:264:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=4.667458955816759, CurrSamplesPerSec=4.6030942736873754, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 230, Rank: 7, loss = 1.5858781337738037
nid001244: Epoch: 0, Step: 230, Rank: 4, loss = 1.8399074077606201
nid001244: Epoch: 0, Step: 230, Rank: 5, loss = 1.453813076019287
nid001241: Epoch: 0, Step: 230, Rank: 0, loss = 1.5110024213790894
nid001244: Epoch: 0, Step: 230, Rank: 6, loss = 1.2955340147018433
nid001241: Epoch: 0, Step: 230, Rank: 3, loss = 1.60630464553833
nid001241: Epoch: 0, Step: 230, Rank: 1, loss = 1.3479787111282349
nid001241: Epoch: 0, Step: 230, Rank: 2, loss = 1.5006248950958252
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 231, Rank: 7, loss = 1.8203213214874268
nid001244: Epoch: 0, Step: 231, Rank: 4, loss = 1.7189255952835083
nid001244: Epoch: 0, Step: 231, Rank: 6, loss = 1.9129754304885864
nid001244: Epoch: 0, Step: 231, Rank: 5, loss = 1.5747894048690796
nid001241: Epoch: 0, Step: 231, Rank: 0, loss = 1.962125301361084
nid001241: Epoch: 0, Step: 231, Rank: 3, loss = 1.497805118560791
nid001241: Epoch: 0, Step: 231, Rank: 1, loss = 1.7844231128692627
nid001241: Epoch: 0, Step: 231, Rank: 2, loss = 1.8181889057159424
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 232, Rank: 7, loss = 1.9235100746154785
nid001244: Epoch: 0, Step: 232, Rank: 6, loss = 1.646718978881836
nid001244: Epoch: 0, Step: 232, Rank: 5, loss = 1.4305717945098877
nid001241: Epoch: 0, Step: 232, Rank: 3, loss = 1.3225377798080444
nid001241: Epoch: 0, Step: 232, Rank: 2, loss = 1.6776484251022339
nid001241: Epoch: 0, Step: 232, Rank: 1, loss = 1.1914080381393433
nid001241: Epoch: 0, Step: 232, Rank: 0, loss = 1.5184688568115234
nid001244: Epoch: 0, Step: 232, Rank: 4, loss = 1.7019706964492798
nid001241: Model Parameters: 8.030 B, Latency: 7.07s, TFLOPs: 4.12, Samples/sec: 0.57, Time/seq 1.77s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 233, Rank: 5, loss = 1.5488507747650146
nid001244: Epoch: 0, Step: 233, Rank: 4, loss = 1.8280344009399414
nid001244: Epoch: 0, Step: 233, Rank: 7, loss = 1.7182848453521729
nid001244: Epoch: 0, Step: 233, Rank: 6, loss = 1.5630276203155518
nid001241: Epoch: 0, Step: 233, Rank: 1, loss = 1.5055195093154907
nid001241: Epoch: 0, Step: 233, Rank: 0, loss = 1.8250819444656372
nid001241: Epoch: 0, Step: 233, Rank: 2, loss = 1.7164270877838135
nid001241: Epoch: 0, Step: 233, Rank: 3, loss = 2.048692226409912
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 234, Rank: 7, loss = 1.334011435508728
nid001244: Epoch: 0, Step: 234, Rank: 4, loss = 1.7493635416030884
nid001244: Epoch: 0, Step: 234, Rank: 5, loss = 1.5767555236816406
nid001241: Epoch: 0, Step: 234, Rank: 0, loss = 1.712963342666626
nid001241: Epoch: 0, Step: 234, Rank: 3, loss = 1.3648110628128052
nid001244: Epoch: 0, Step: 234, Rank: 6, loss = 1.658551812171936
nid001241: Epoch: 0, Step: 234, Rank: 1, loss = 1.7057595252990723
nid001241: Epoch: 0, Step: 234, Rank: 2, loss = 1.8608468770980835
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 235, Rank: 7, loss = 1.6741880178451538
nid001244: Epoch: 0, Step: 235, Rank: 4, loss = 1.0844005346298218
nid001244: Epoch: 0, Step: 235, Rank: 6, loss = 1.7404060363769531
nid001244: Epoch: 0, Step: 235, Rank: 5, loss = 1.668219804763794
nid001241: Epoch: 0, Step: 235, Rank: 0, loss = 1.8048912286758423
nid001241: Epoch: 0, Step: 235, Rank: 3, loss = 1.8086227178573608
nid001241: Epoch: 0, Step: 235, Rank: 1, loss = 1.411889910697937
nid001241: Epoch: 0, Step: 235, Rank: 2, loss = 1.668244481086731
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 236, Rank: 4, loss = 1.5189003944396973
nid001244: Epoch: 0, Step: 236, Rank: 5, loss = 1.7846803665161133
nid001244: Epoch: 0, Step: 236, Rank: 6, loss = 1.6022439002990723
nid001241: Epoch: 0, Step: 236, Rank: 0, loss = 1.7406814098358154
nid001241: Epoch: 0, Step: 236, Rank: 3, loss = 1.321814775466919
nid001244: Epoch: 0, Step: 236, Rank: 7, loss = 1.5174576044082642
nid001241: Epoch: 0, Step: 236, Rank: 1, loss = 1.3388551473617554
nid001241: Epoch: 0, Step: 236, Rank: 2, loss = 1.3441563844680786
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 237, Rank: 4, loss = 1.3038228750228882
nid001244: Epoch: 0, Step: 237, Rank: 7, loss = 1.6364814043045044
nid001241: Epoch: 0, Step: 237, Rank: 0, loss = 1.4917041063308716
nid001241: Epoch: 0, Step: 237, Rank: 3, loss = 1.73811936378479
nid001244: Epoch: 0, Step: 237, Rank: 5, loss = 1.5970969200134277
nid001241: Epoch: 0, Step: 237, Rank: 1, loss = 1.8076003789901733
nid001241: Epoch: 0, Step: 237, Rank: 2, loss = 1.8867796659469604
nid001244: Epoch: 0, Step: 237, Rank: 6, loss = 1.7665129899978638
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 238, Rank: 4, loss = 1.1352578401565552
nid001244: Epoch: 0, Step: 238, Rank: 5, loss = 1.3918722867965698
nid001244: Epoch: 0, Step: 238, Rank: 7, loss = 1.5123928785324097
nid001241: Epoch: 0, Step: 238, Rank: 0, loss = 1.8923076391220093
nid001241: Epoch: 0, Step: 238, Rank: 3, loss = 1.2278105020523071
nid001244: Epoch: 0, Step: 238, Rank: 6, loss = 1.547460913658142
nid001241: Epoch: 0, Step: 238, Rank: 1, loss = 1.6397451162338257
nid001241: Epoch: 0, Step: 238, Rank: 2, loss = 1.8774893283843994
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 239, Rank: 7, loss = 1.743291974067688
nid001244: Epoch: 0, Step: 239, Rank: 5, loss = 1.5861529111862183
nid001244: Epoch: 0, Step: 239, Rank: 6, loss = 1.323965072631836
nid001241: Epoch: 0, Step: 239, Rank: 3, loss = 1.6045575141906738
nid001244: Epoch: 0, Step: 239, Rank: 4, loss = 1.7916282415390015
nid001241: Epoch: 0, Step: 239, Rank: 2, loss = 1.6582186222076416
nid001241: Epoch: 0, Step: 239, Rank: 1, loss = 1.4520281553268433
nid001241: Epoch: 0, Step: 239, Rank: 0, loss = 2.1499292850494385
nid001241: [2024-11-12 12:59:54,552] [INFO] [logging.py:128:log_dist] [Rank 0] step=240, skipped=3, lr=[9.487416122688442e-06, 9.487416122688442e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 12:59:54,553] [INFO] [timer.py:264:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=4.667728149181335, CurrSamplesPerSec=4.576783010971772, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 7.00s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 240, Rank: 7, loss = 1.5544840097427368
nid001244: Epoch: 0, Step: 240, Rank: 6, loss = 1.1094406843185425
nid001241: Epoch: 0, Step: 240, Rank: 3, loss = 1.6624261140823364
nid001241: Epoch: 0, Step: 240, Rank: 2, loss = 1.3046568632125854
nid001244: Epoch: 0, Step: 240, Rank: 5, loss = 1.852046251296997
nid001241: Epoch: 0, Step: 240, Rank: 1, loss = 1.599915623664856
nid001241: Epoch: 0, Step: 240, Rank: 0, loss = 1.8340729475021362
nid001244: Epoch: 0, Step: 240, Rank: 4, loss = 1.8996587991714478
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 241, Rank: 7, loss = 1.4340702295303345
nid001244: Epoch: 0, Step: 241, Rank: 5, loss = 1.5628166198730469
nid001244: Epoch: 0, Step: 241, Rank: 6, loss = 1.5610395669937134
nid001241: Epoch: 0, Step: 241, Rank: 3, loss = 1.5811045169830322
nid001241: Epoch: 0, Step: 241, Rank: 2, loss = 1.6210663318634033
nid001244: Epoch: 0, Step: 241, Rank: 4, loss = 2.0144660472869873
nid001241: Epoch: 0, Step: 241, Rank: 1, loss = 1.4511895179748535
nid001241: Epoch: 0, Step: 241, Rank: 0, loss = 2.050612688064575
nid001241: Model Parameters: 8.030 B, Latency: 7.12s, TFLOPs: 4.09, Samples/sec: 0.56, Time/seq 1.78s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 242, Rank: 7, loss = 1.8529489040374756
nid001244: Epoch: 0, Step: 242, Rank: 6, loss = 1.6122173070907593
nid001244: Epoch: 0, Step: 242, Rank: 5, loss = 1.7903945446014404
nid001241: Epoch: 0, Step: 242, Rank: 3, loss = 1.7038249969482422
nid001241: Epoch: 0, Step: 242, Rank: 2, loss = 1.8219432830810547
nid001241: Epoch: 0, Step: 242, Rank: 1, loss = 1.6491708755493164
nid001241: Epoch: 0, Step: 242, Rank: 0, loss = 1.5617091655731201
nid001244: Epoch: 0, Step: 242, Rank: 4, loss = 1.5360838174819946
nid001241: Model Parameters: 8.030 B, Latency: 7.05s, TFLOPs: 4.13, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 243, Rank: 2, loss = 1.6822659969329834
nid001241: Epoch: 0, Step: 243, Rank: 3, loss = 1.7385376691818237
nid001244: Epoch: 0, Step: 243, Rank: 6, loss = 1.4770450592041016
nid001244: Epoch: 0, Step: 243, Rank: 7, loss = 1.8829983472824097
nid001244: Epoch: 0, Step: 243, Rank: 5, loss = 1.7121847867965698
nid001244: Epoch: 0, Step: 243, Rank: 4, loss = 1.5075438022613525
nid001241: Epoch: 0, Step: 243, Rank: 0, loss = 1.5395196676254272
nid001241: Epoch: 0, Step: 243, Rank: 1, loss = 1.6002570390701294
nid001241: Model Parameters: 8.030 B, Latency: 7.01s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 244, Rank: 7, loss = 1.663683533668518
nid001244: Epoch: 0, Step: 244, Rank: 6, loss = 1.7952221632003784
nid001241: Epoch: 0, Step: 244, Rank: 3, loss = 1.7129838466644287
nid001244: Epoch: 0, Step: 244, Rank: 5, loss = 1.8587977886199951
nid001241: Epoch: 0, Step: 244, Rank: 2, loss = 1.5317168235778809
nid001241: Epoch: 0, Step: 244, Rank: 1, loss = 1.9292129278182983
nid001241: Epoch: 0, Step: 244, Rank: 0, loss = 1.6069214344024658
nid001244: Epoch: 0, Step: 244, Rank: 4, loss = 1.5465447902679443
nid001241: Model Parameters: 8.030 B, Latency: 7.10s, TFLOPs: 4.10, Samples/sec: 0.56, Time/seq 1.78s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 245, Rank: 7, loss = 1.8252038955688477
nid001244: Epoch: 0, Step: 245, Rank: 5, loss = 1.8758774995803833
nid001244: Epoch: 0, Step: 245, Rank: 6, loss = 1.4073593616485596
nid001241: Epoch: 0, Step: 245, Rank: 3, loss = 1.3050205707550049
nid001244: Epoch: 0, Step: 245, Rank: 4, loss = 1.7352226972579956
nid001241: Epoch: 0, Step: 245, Rank: 2, loss = 1.5611724853515625
nid001241: Epoch: 0, Step: 245, Rank: 1, loss = 1.7337251901626587
nid001241: Epoch: 0, Step: 245, Rank: 0, loss = 1.5647419691085815
nid001241: Model Parameters: 8.030 B, Latency: 7.14s, TFLOPs: 4.07, Samples/sec: 0.56, Time/seq 1.79s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 246, Rank: 2, loss = 1.6435177326202393
nid001244: Epoch: 0, Step: 246, Rank: 7, loss = 1.4530442953109741
nid001244: Epoch: 0, Step: 246, Rank: 6, loss = 1.6530789136886597
nid001244: Epoch: 0, Step: 246, Rank: 5, loss = 1.5394971370697021
nid001241: Epoch: 0, Step: 246, Rank: 3, loss = 1.6236473321914673
nid001244: Epoch: 0, Step: 246, Rank: 4, loss = 1.234826922416687
nid001241: Epoch: 0, Step: 246, Rank: 0, loss = 1.3499380350112915
nid001241: Epoch: 0, Step: 246, Rank: 1, loss = 1.9449313879013062
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 247, Rank: 7, loss = 1.705613613128662
nid001244: Epoch: 0, Step: 247, Rank: 6, loss = 1.417925238609314
nid001244: Epoch: 0, Step: 247, Rank: 5, loss = 1.9696364402770996
nid001241: Epoch: 0, Step: 247, Rank: 3, loss = 1.557901382446289
nid001241: Epoch: 0, Step: 247, Rank: 2, loss = 1.7347646951675415
nid001241: Epoch: 0, Step: 247, Rank: 1, loss = 1.5184246301651
nid001244: Epoch: 0, Step: 247, Rank: 4, loss = 1.7820501327514648
nid001241: Epoch: 0, Step: 247, Rank: 0, loss = 1.291081428527832
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 248, Rank: 7, loss = 1.540939450263977
nid001244: Epoch: 0, Step: 248, Rank: 6, loss = 1.625355839729309
nid001244: Epoch: 0, Step: 248, Rank: 5, loss = 1.5924978256225586
nid001241: Epoch: 0, Step: 248, Rank: 3, loss = 1.5646203756332397
nid001241: Epoch: 0, Step: 248, Rank: 2, loss = 1.9266566038131714
nid001244: Epoch: 0, Step: 248, Rank: 4, loss = 1.9732004404067993
nid001241: Epoch: 0, Step: 248, Rank: 1, loss = 1.6209708452224731
nid001241: Epoch: 0, Step: 248, Rank: 0, loss = 1.6676225662231445
nid001241: Model Parameters: 8.030 B, Latency: 6.96s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 249, Rank: 7, loss = 1.7820684909820557
nid001244: Epoch: 0, Step: 249, Rank: 4, loss = 1.6633402109146118
nid001244: Epoch: 0, Step: 249, Rank: 5, loss = 1.8379687070846558
nid001244: Epoch: 0, Step: 249, Rank: 6, loss = 1.4932221174240112
nid001241: Epoch: 0, Step: 249, Rank: 0, loss = 1.566251516342163
nid001241: Epoch: 0, Step: 249, Rank: 3, loss = 1.3020098209381104
nid001241: Epoch: 0, Step: 249, Rank: 1, loss = 1.4219328165054321Epoch: 0, Step: 249, Rank: 2, loss = 1.3579380512237549
nid001241: 
nid001241: [2024-11-12 13:01:04,460] [INFO] [logging.py:128:log_dist] [Rank 0] step=250, skipped=3, lr=[9.473492532122694e-06, 9.473492532122694e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:01:04,460] [INFO] [timer.py:264:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=4.664152360114166, CurrSamplesPerSec=4.6187694629457265, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 250, Rank: 7, loss = 1.7423532009124756
nid001244: Epoch: 0, Step: 250, Rank: 4, loss = 1.4715040922164917
nid001244: Epoch: 0, Step: 250, Rank: 6, loss = 1.699009656906128
nid001244: Epoch: 0, Step: 250, Rank: 5, loss = 1.7045832872390747
nid001241: Epoch: 0, Step: 250, Rank: 0, loss = 1.4710065126419067
nid001241: Epoch: 0, Step: 250, Rank: 3, loss = 1.4262118339538574
nid001241: Epoch: 0, Step: 250, Rank: 1, loss = 2.0045714378356934
nid001241: Epoch: 0, Step: 250, Rank: 2, loss = 1.3465287685394287
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 251, Rank: 7, loss = 1.6567485332489014
nid001244: Epoch: 0, Step: 251, Rank: 4, loss = 1.5358439683914185
nid001244: Epoch: 0, Step: 251, Rank: 5, loss = 1.5685521364212036
nid001241: Epoch: 0, Step: 251, Rank: 0, loss = 1.7894283533096313
nid001241: Epoch: 0, Step: 251, Rank: 3, loss = 1.7189910411834717
nid001244: Epoch: 0, Step: 251, Rank: 6, loss = 1.8965691328048706
nid001241: Epoch: 0, Step: 251, Rank: 1, loss = 1.7771598100662231
nid001241: Epoch: 0, Step: 251, Rank: 2, loss = 1.9895884990692139
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 252, Rank: 7, loss = 1.6979414224624634
nid001244: Epoch: 0, Step: 252, Rank: 6, loss = 1.5047484636306763
nid001241: Epoch: 0, Step: 252, Rank: 3, loss = 1.3771058320999146
nid001244: Epoch: 0, Step: 252, Rank: 5, loss = 1.7189791202545166
nid001244: Epoch: 0, Step: 252, Rank: 4, loss = 1.7411988973617554
nid001241: Epoch: 0, Step: 252, Rank: 1, loss = 1.6243919134140015
nid001241: Epoch: 0, Step: 252, Rank: 2, loss = 1.5113134384155273
nid001241: Epoch: 0, Step: 252, Rank: 0, loss = 1.244985580444336
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 253, Rank: 6, loss = 1.6809688806533813
nid001244: Epoch: 0, Step: 253, Rank: 7, loss = 1.9535386562347412
nid001241: Epoch: 0, Step: 253, Rank: 2, loss = 1.2249473333358765
nid001244: Epoch: 0, Step: 253, Rank: 4, loss = 1.6365886926651
nid001241: Epoch: 0, Step: 253, Rank: 3, loss = 1.0653644800186157
nid001241: Epoch: 0, Step: 253, Rank: 1, loss = 2.007737398147583Epoch: 0, Step: 253, Rank: 0, loss = 1.3837854862213135
nid001241: 
nid001244: Epoch: 0, Step: 253, Rank: 5, loss = 1.4921897649765015
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 254, Rank: 6, loss = 1.7228903770446777
nid001244: Epoch: 0, Step: 254, Rank: 7, loss = 1.9647318124771118
nid001241: Epoch: 0, Step: 254, Rank: 2, loss = 1.626523494720459
nid001241: Epoch: 0, Step: 254, Rank: 3, loss = 1.6618510484695435
nid001244: Epoch: 0, Step: 254, Rank: 4, loss = 1.796601414680481
nid001241: Epoch: 0, Step: 254, Rank: 0, loss = 1.747973918914795
nid001241: Epoch: 0, Step: 254, Rank: 1, loss = 1.8134641647338867
nid001244: Epoch: 0, Step: 254, Rank: 5, loss = 1.378239631652832
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 255, Rank: 6, loss = 1.6515860557556152
nid001244: Epoch: 0, Step: 255, Rank: 7, loss = 1.809423804283142
nid001241: Epoch: 0, Step: 255, Rank: 2, loss = 1.788771629333496
nid001244: Epoch: 0, Step: 255, Rank: 4, loss = 1.8431934118270874
nid001241: Epoch: 0, Step: 255, Rank: 3, loss = 1.6995627880096436
nid001241: Epoch: 0, Step: 255, Rank: 0, loss = 1.8271052837371826
nid001241: Epoch: 0, Step: 255, Rank: 1, loss = 1.4085382223129272
nid001244: Epoch: 0, Step: 255, Rank: 5, loss = 1.487728476524353
nid001241: Model Parameters: 8.030 B, Latency: 7.01s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 256, Rank: 5, loss = 1.932703971862793
nid001244: Epoch: 0, Step: 256, Rank: 4, loss = 1.749655842781067
nid001244: Epoch: 0, Step: 256, Rank: 6, loss = 1.4372416734695435
nid001241: Epoch: 0, Step: 256, Rank: 1, loss = 1.3394203186035156
nid001244: Epoch: 0, Step: 256, Rank: 7, loss = 1.6764823198318481
nid001241: Epoch: 0, Step: 256, Rank: 0, loss = 1.3608958721160889
nid001241: Epoch: 0, Step: 256, Rank: 2, loss = 1.5018556118011475
nid001241: Epoch: 0, Step: 256, Rank: 3, loss = 1.5510648488998413
nid001241: Model Parameters: 8.030 B, Latency: 6.97s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 257, Rank: 5, loss = 1.6424388885498047
nid001244: Epoch: 0, Step: 257, Rank: 4, loss = 1.4792578220367432
nid001241: Epoch: 0, Step: 257, Rank: 1, loss = 1.5325285196304321
nid001244: Epoch: 0, Step: 257, Rank: 6, loss = 1.6935688257217407
nid001241: Epoch: 0, Step: 257, Rank: 0, loss = 1.6433368921279907
nid001244: Epoch: 0, Step: 257, Rank: 7, loss = 1.5296838283538818
nid001241: Epoch: 0, Step: 257, Rank: 2, loss = 1.4814783334732056
nid001241: Epoch: 0, Step: 257, Rank: 3, loss = 1.097343921661377
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 258, Rank: 5, loss = 1.7737325429916382
nid001244: Epoch: 0, Step: 258, Rank: 4, loss = 1.4868282079696655
nid001241: Epoch: 0, Step: 258, Rank: 1, loss = 1.682823657989502
nid001241: Epoch: 0, Step: 258, Rank: 0, loss = 1.8683723211288452
nid001241: Epoch: 0, Step: 258, Rank: 2, loss = 1.4682198762893677
nid001244: Epoch: 0, Step: 258, Rank: 6, loss = 1.7415348291397095
nid001241: Epoch: 0, Step: 258, Rank: 3, loss = 1.5962309837341309
nid001244: Epoch: 0, Step: 258, Rank: 7, loss = 1.2532522678375244
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 259, Rank: 6, loss = 1.8321914672851562
nid001244: Epoch: 0, Step: 259, Rank: 7, loss = 1.6435426473617554
nid001241: Epoch: 0, Step: 259, Rank: 2, loss = 1.4102989435195923
nid001244: Epoch: 0, Step: 259, Rank: 4, loss = 1.3702456951141357
nid001241: Epoch: 0, Step: 259, Rank: 0, loss = 1.6333938837051392
nid001241: Epoch: 0, Step: 259, Rank: 3, loss = 1.544646978378296
nid001244: Epoch: 0, Step: 259, Rank: 5, loss = 1.5874195098876953
nid001241: Epoch: 0, Step: 259, Rank: 1, loss = 1.4094122648239136
nid001241: [2024-11-12 13:02:13,089] [INFO] [logging.py:128:log_dist] [Rank 0] step=260, skipped=3, lr=[9.459008054459293e-06, 9.459008054459293e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:02:13,089] [INFO] [timer.py:264:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=4.664229487979585, CurrSamplesPerSec=4.698764338729838, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 260, Rank: 7, loss = 1.505829095840454
nid001241: Epoch: 0, Step: 260, Rank: 3, loss = 1.382538914680481
nid001244: Epoch: 0, Step: 260, Rank: 6, loss = 1.4918931722640991
nid001241: Epoch: 0, Step: 260, Rank: 2, loss = 1.6654658317565918
nid001244: Epoch: 0, Step: 260, Rank: 5, loss = 1.6157118082046509
nid001241: Epoch: 0, Step: 260, Rank: 1, loss = 1.658900260925293
nid001244: Epoch: 0, Step: 260, Rank: 4, loss = 1.6689900159835815
nid001241: Epoch: 0, Step: 260, Rank: 0, loss = 1.9096639156341553
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 261, Rank: 3, loss = 1.6635973453521729
nid001241: Epoch: 0, Step: 261, Rank: 2, loss = 1.8487409353256226
nid001244: Epoch: 0, Step: 261, Rank: 7, loss = 1.4320241212844849
nid001244: Epoch: 0, Step: 261, Rank: 6, loss = 1.6744952201843262
nid001241: Epoch: 0, Step: 261, Rank: 1, loss = 1.5095125436782837
nid001244: Epoch: 0, Step: 261, Rank: 5, loss = 1.3230705261230469
nid001244: Epoch: 0, Step: 261, Rank: 4, loss = 1.6088870763778687
nid001241: Epoch: 0, Step: 261, Rank: 0, loss = 1.3212475776672363
nid001241: Model Parameters: 8.030 B, Latency: 6.69s, TFLOPs: 4.35, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 262, Rank: 6, loss = 2.2970879077911377
nid001244: Epoch: 0, Step: 262, Rank: 7, loss = 1.426226019859314
nid001241: Epoch: 0, Step: 262, Rank: 3, loss = 1.5361905097961426
nid001241: Epoch: 0, Step: 262, Rank: 0, loss = 1.857790231704712
nid001244: Epoch: 0, Step: 262, Rank: 4, loss = 1.3726389408111572
nid001244: Epoch: 0, Step: 262, Rank: 5, loss = 1.8030339479446411
nid001241: Epoch: 0, Step: 262, Rank: 1, loss = 1.842515230178833
nid001241: Epoch: 0, Step: 262, Rank: 2, loss = 1.8020799160003662
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 263, Rank: 7, loss = 1.6396632194519043
nid001244: Epoch: 0, Step: 263, Rank: 6, loss = 1.6925636529922485
nid001241: Epoch: 0, Step: 263, Rank: 3, loss = 1.7347381114959717
nid001241: Epoch: 0, Step: 263, Rank: 2, loss = 1.6501997709274292
nid001244: Epoch: 0, Step: 263, Rank: 5, loss = 1.5448731184005737
nid001244: Epoch: 0, Step: 263, Rank: 4, loss = 1.434900164604187
nid001241: Epoch: 0, Step: 263, Rank: 1, loss = 1.4254038333892822
nid001241: Epoch: 0, Step: 263, Rank: 0, loss = 1.5981192588806152
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 264, Rank: 6, loss = 1.6582670211791992
nid001244: Epoch: 0, Step: 264, Rank: 7, loss = 1.57796049118042
nid001241: Epoch: 0, Step: 264, Rank: 2, loss = 1.3606173992156982
nid001241: Epoch: 0, Step: 264, Rank: 3, loss = 1.7249808311462402
nid001241: Epoch: 0, Step: 264, Rank: 0, loss = 1.541879415512085
nid001244: Epoch: 0, Step: 264, Rank: 4, loss = 1.8234851360321045
nid001241: Epoch: 0, Step: 264, Rank: 1, loss = 1.7357456684112549
nid001244: Epoch: 0, Step: 264, Rank: 5, loss = 1.522201418876648
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 265, Rank: 6, loss = 1.4381215572357178
nid001244: Epoch: 0, Step: 265, Rank: 4, loss = 1.5622721910476685
nid001244: Epoch: 0, Step: 265, Rank: 7, loss = 1.49461030960083
nid001241: Epoch: 0, Step: 265, Rank: 2, loss = 1.5492485761642456
nid001241: Epoch: 0, Step: 265, Rank: 3, loss = 1.6842308044433594
nid001244: Epoch: 0, Step: 265, Rank: 5, loss = 1.6678153276443481
nid001241: Epoch: 0, Step: 265, Rank: 0, loss = 1.595670461654663
nid001241: Epoch: 0, Step: 265, Rank: 1, loss = 1.707116961479187
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 266, Rank: 6, loss = 1.623774766921997
nid001244: Epoch: 0, Step: 266, Rank: 7, loss = 1.67938232421875
nid001244: Epoch: 0, Step: 266, Rank: 4, loss = 1.87941312789917
nid001241: Epoch: 0, Step: 266, Rank: 2, loss = 1.6019827127456665
nid001244: Epoch: 0, Step: 266, Rank: 5, loss = 1.8289687633514404
nid001241: Epoch: 0, Step: 266, Rank: 3, loss = 1.836951494216919
nid001241: Epoch: 0, Step: 266, Rank: 0, loss = 1.4318294525146484
nid001241: Epoch: 0, Step: 266, Rank: 1, loss = 1.7025423049926758
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 267, Rank: 1, loss = 1.5709102153778076
nid001241: Epoch: 0, Step: 267, Rank: 2, loss = 1.534116506576538
nid001241: Epoch: 0, Step: 267, Rank: 0, loss = 1.6097317934036255
nid001244: Epoch: 0, Step: 267, Rank: 5, loss = 1.4617863893508911
nid001241: Epoch: 0, Step: 267, Rank: 3, loss = 1.4480371475219727
nid001244: Epoch: 0, Step: 267, Rank: 6, loss = 1.4980597496032715
nid001244: Epoch: 0, Step: 267, Rank: 7, loss = 1.6996283531188965
nid001244: Epoch: 0, Step: 267, Rank: 4, loss = 1.7160459756851196
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 268, Rank: 4, loss = 1.2823097705841064
nid001244: Epoch: 0, Step: 268, Rank: 7, loss = 1.7260220050811768
nid001244: Epoch: 0, Step: 268, Rank: 5, loss = 1.8806322813034058
nid001241: Epoch: 0, Step: 268, Rank: 0, loss = 1.4826892614364624
nid001244: Epoch: 0, Step: 268, Rank: 6, loss = 1.4612317085266113
nid001241: Epoch: 0, Step: 268, Rank: 3, loss = 1.3124110698699951
nid001241: Epoch: 0, Step: 268, Rank: 1, loss = 1.8048144578933716
nid001241: Epoch: 0, Step: 268, Rank: 2, loss = 1.255621075630188
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 269, Rank: 7, loss = 1.375244140625
nid001244: Epoch: 0, Step: 269, Rank: 4, loss = 2.089857578277588
nid001244: Epoch: 0, Step: 269, Rank: 5, loss = 1.321455955505371
nid001244: Epoch: 0, Step: 269, Rank: 6, loss = 1.174077033996582
nid001241: Epoch: 0, Step: 269, Rank: 0, loss = 1.661906361579895
nid001241: Epoch: 0, Step: 269, Rank: 3, loss = 1.8406651020050049
nid001241: Epoch: 0, Step: 269, Rank: 1, loss = 1.256598711013794
nid001241: Epoch: 0, Step: 269, Rank: 2, loss = 1.7234426736831665
nid001241: [2024-11-12 13:03:21,127] [INFO] [logging.py:128:log_dist] [Rank 0] step=270, skipped=3, lr=[9.44396443739528e-06, 9.44396443739528e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:03:21,127] [INFO] [timer.py:264:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=4.665799701920467, CurrSamplesPerSec=4.7061854702414925, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 270, Rank: 2, loss = 1.4768383502960205
nid001241: Epoch: 0, Step: 270, Rank: 3, loss = 1.530485987663269
nid001244: Epoch: 0, Step: 270, Rank: 6, loss = 1.3104687929153442
nid001244: Epoch: 0, Step: 270, Rank: 7, loss = 1.6379746198654175
nid001241: Epoch: 0, Step: 270, Rank: 1, loss = 1.4474961757659912
nid001241: Epoch: 0, Step: 270, Rank: 0, loss = 1.3423031568527222
nid001244: Epoch: 0, Step: 270, Rank: 4, loss = 1.628761649131775
nid001244: Epoch: 0, Step: 270, Rank: 5, loss = 1.8831661939620972
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 271, Rank: 6, loss = 1.7715299129486084
nid001244: Epoch: 0, Step: 271, Rank: 7, loss = 1.6333142518997192
nid001241: Epoch: 0, Step: 271, Rank: 2, loss = 1.6892000436782837
nid001244: Epoch: 0, Step: 271, Rank: 4, loss = 1.6288868188858032
nid001241: Epoch: 0, Step: 271, Rank: 3, loss = 1.5324957370758057
nid001241: Epoch: 0, Step: 271, Rank: 0, loss = 1.728041410446167
nid001244: Epoch: 0, Step: 271, Rank: 5, loss = 1.5138026475906372
nid001241: Epoch: 0, Step: 271, Rank: 1, loss = 1.7237170934677124
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 272, Rank: 6, loss = 1.7424331903457642
nid001244: Epoch: 0, Step: 272, Rank: 7, loss = 1.55950129032135
nid001241: Epoch: 0, Step: 272, Rank: 2, loss = 1.5229002237319946
nid001241: Epoch: 0, Step: 272, Rank: 3, loss = 1.7854797840118408
nid001244: Epoch: 0, Step: 272, Rank: 4, loss = 1.7523913383483887
nid001241: Epoch: 0, Step: 272, Rank: 0, loss = 1.4854223728179932
nid001241: Epoch: 0, Step: 272, Rank: 1, loss = 1.8808374404907227
nid001244: Epoch: 0, Step: 272, Rank: 5, loss = 1.4910520315170288
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 273, Rank: 4, loss = 1.799652099609375
nid001244: Epoch: 0, Step: 273, Rank: 5, loss = 1.5196688175201416
nid001241: Epoch: 0, Step: 273, Rank: 0, loss = 1.3351161479949951
nid001244: Epoch: 0, Step: 273, Rank: 6, loss = 1.7565772533416748
nid001241: Epoch: 0, Step: 273, Rank: 3, loss = 1.390515685081482
nid001241: Epoch: 0, Step: 273, Rank: 1, loss = 1.3121867179870605
nid001241: Epoch: 0, Step: 273, Rank: 2, loss = 1.3256090879440308
nid001244: Epoch: 0, Step: 273, Rank: 7, loss = 1.630539059638977
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 274, Rank: 4, loss = 1.651498556137085
nid001244: Epoch: 0, Step: 274, Rank: 7, loss = 1.8310105800628662
nid001241: Epoch: 0, Step: 274, Rank: 0, loss = 1.699327826499939
nid001241: Epoch: 0, Step: 274, Rank: 3, loss = 1.5945435762405396
nid001244: Epoch: 0, Step: 274, Rank: 5, loss = 1.8225953578948975
nid001241: Epoch: 0, Step: 274, Rank: 1, loss = 1.8073116540908813
nid001241: Epoch: 0, Step: 274, Rank: 2, loss = 1.289342999458313
nid001244: Epoch: 0, Step: 274, Rank: 6, loss = 1.4979453086853027
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 275, Rank: 7, loss = 1.8921806812286377
nid001244: Epoch: 0, Step: 275, Rank: 6, loss = 1.732823133468628
nid001244: Epoch: 0, Step: 275, Rank: 4, loss = 1.4089101552963257
nid001241: Epoch: 0, Step: 275, Rank: 0, loss = 1.633226752281189
nid001244: Epoch: 0, Step: 275, Rank: 5, loss = 1.4992282390594482
nid001241: Epoch: 0, Step: 275, Rank: 3, loss = 1.4992146492004395
nid001241: Epoch: 0, Step: 275, Rank: 1, loss = 1.6398147344589233
nid001241: Epoch: 0, Step: 275, Rank: 2, loss = 1.7857130765914917
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 276, Rank: 4, loss = 1.6205735206604004
nid001244: Epoch: 0, Step: 276, Rank: 7, loss = 1.5270105600357056
nid001241: Epoch: 0, Step: 276, Rank: 0, loss = 1.8119248151779175Epoch: 0, Step: 276, Rank: 3, loss = 1.7389425039291382
nid001241: 
nid001244: Epoch: 0, Step: 276, Rank: 5, loss = 1.357526183128357
nid001244: Epoch: 0, Step: 276, Rank: 6, loss = 1.8202136754989624
nid001241: Epoch: 0, Step: 276, Rank: 1, loss = 1.1124930381774902
nid001241: Epoch: 0, Step: 276, Rank: 2, loss = 1.3381611108779907
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 277, Rank: 6, loss = 1.8696247339248657
nid001244: Epoch: 0, Step: 277, Rank: 7, loss = 1.7529882192611694
nid001241: Epoch: 0, Step: 277, Rank: 2, loss = 1.7132880687713623
nid001244: Epoch: 0, Step: 277, Rank: 4, loss = 1.556740403175354
nid001241: Epoch: 0, Step: 277, Rank: 3, loss = 1.6437230110168457
nid001241: Epoch: 0, Step: 277, Rank: 0, loss = 1.669694185256958
nid001241: Epoch: 0, Step: 277, Rank: 1, loss = 1.586754322052002
nid001244: Epoch: 0, Step: 277, Rank: 5, loss = 1.716345191001892
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 278, Rank: 6, loss = 1.6488642692565918
nid001241: Epoch: 0, Step: 278, Rank: 2, loss = 1.4993327856063843
nid001244: Epoch: 0, Step: 278, Rank: 7, loss = 1.5406278371810913
nid001241: Epoch: 0, Step: 278, Rank: 3, loss = 1.2571791410446167Epoch: 0, Step: 278, Rank: 0, loss = 1.4187328815460205
nid001241: 
nid001241: Epoch: 0, Step: 278, Rank: 1, loss = 1.6613718271255493
nid001244: Epoch: 0, Step: 278, Rank: 4, loss = 1.5018686056137085
nid001244: Epoch: 0, Step: 278, Rank: 5, loss = 1.2209646701812744
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 279, Rank: 6, loss = 1.0002028942108154
nid001244: Epoch: 0, Step: 279, Rank: 7, loss = 1.739429235458374
nid001244: Epoch: 0, Step: 279, Rank: 4, loss = 1.5854047536849976
nid001241: Epoch: 0, Step: 279, Rank: 2, loss = 1.477096438407898
nid001241: Epoch: 0, Step: 279, Rank: 3, loss = 1.5809898376464844
nid001244: Epoch: 0, Step: 279, Rank: 5, loss = 1.5152270793914795
nid001241: Epoch: 0, Step: 279, Rank: 0, loss = 1.4204336404800415
nid001241: Epoch: 0, Step: 279, Rank: 1, loss = 1.4457957744598389
nid001241: [2024-11-12 13:04:29,128] [INFO] [logging.py:128:log_dist] [Rank 0] step=280, skipped=3, lr=[9.428363496093455e-06, 9.428363496093455e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:04:29,129] [INFO] [timer.py:264:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=4.667340076797965, CurrSamplesPerSec=4.692099676690129, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 280, Rank: 7, loss = 1.7176108360290527
nid001244: Epoch: 0, Step: 280, Rank: 4, loss = 1.7667334079742432
nid001244: Epoch: 0, Step: 280, Rank: 6, loss = 1.8895877599716187
nid001244: Epoch: 0, Step: 280, Rank: 5, loss = 1.5982047319412231
nid001241: Epoch: 0, Step: 280, Rank: 2, loss = 1.6672354936599731
nid001241: Epoch: 0, Step: 280, Rank: 3, loss = 1.2755557298660278
nid001241: Epoch: 0, Step: 280, Rank: 1, loss = 1.778956651687622
nid001241: Epoch: 0, Step: 280, Rank: 0, loss = 1.8121216297149658
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 281, Rank: 7, loss = 1.6926562786102295
nid001244: Epoch: 0, Step: 281, Rank: 4, loss = 1.6933740377426147
nid001244: Epoch: 0, Step: 281, Rank: 5, loss = 1.8003363609313965
nid001244: Epoch: 0, Step: 281, Rank: 6, loss = 1.681767463684082
nid001241: Epoch: 0, Step: 281, Rank: 0, loss = 1.535129427909851
nid001241: Epoch: 0, Step: 281, Rank: 3, loss = 1.3922067880630493
nid001241: Epoch: 0, Step: 281, Rank: 1, loss = 1.66915762424469
nid001241: Epoch: 0, Step: 281, Rank: 2, loss = 1.5524001121520996
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 282, Rank: 6, loss = 2.0860908031463623
nid001241: Epoch: 0, Step: 282, Rank: 3, loss = 1.8179959058761597
nid001241: Epoch: 0, Step: 282, Rank: 0, loss = 1.5431698560714722
nid001241: Epoch: 0, Step: 282, Rank: 2, loss = 1.9193087816238403
nid001244: Epoch: 0, Step: 282, Rank: 7, loss = 1.5185972452163696
nid001241: Epoch: 0, Step: 282, Rank: 1, loss = 1.5901761054992676
nid001244: Epoch: 0, Step: 282, Rank: 5, loss = 1.4017810821533203
nid001244: Epoch: 0, Step: 282, Rank: 4, loss = 1.768759846687317
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 283, Rank: 7, loss = 1.5954129695892334
nid001244: Epoch: 0, Step: 283, Rank: 5, loss = 1.4658654928207397
nid001241: Epoch: 0, Step: 283, Rank: 3, loss = 2.1439261436462402
nid001241: Epoch: 0, Step: 283, Rank: 2, loss = 1.3787219524383545
nid001244: Epoch: 0, Step: 283, Rank: 6, loss = 1.41329824924469
nid001244: Epoch: 0, Step: 283, Rank: 4, loss = 1.6910301446914673
nid001241: Epoch: 0, Step: 283, Rank: 1, loss = 1.6740190982818604
nid001241: Epoch: 0, Step: 283, Rank: 0, loss = 1.387625813484192
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 284, Rank: 6, loss = 1.6700934171676636
nid001244: Epoch: 0, Step: 284, Rank: 7, loss = 1.5913761854171753
nid001241: Epoch: 0, Step: 284, Rank: 2, loss = 1.7645632028579712
nid001244: Epoch: 0, Step: 284, Rank: 4, loss = 1.6991865634918213
nid001241: Epoch: 0, Step: 284, Rank: 3, loss = 1.7136095762252808
nid001244: Epoch: 0, Step: 284, Rank: 5, loss = 1.7278313636779785
nid001241: Epoch: 0, Step: 284, Rank: 0, loss = 1.520476222038269
nid001241: Epoch: 0, Step: 284, Rank: 1, loss = 1.652652621269226
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 285, Rank: 2, loss = 1.5514620542526245
nid001244: Epoch: 0, Step: 285, Rank: 6, loss = 1.3846498727798462
nid001241: Epoch: 0, Step: 285, Rank: 3, loss = 1.1886781454086304
nid001244: Epoch: 0, Step: 285, Rank: 7, loss = 1.4958815574645996
nid001244: Epoch: 0, Step: 285, Rank: 4, loss = 1.9545609951019287
nid001244: Epoch: 0, Step: 285, Rank: 5, loss = 1.9582446813583374
nid001241: Epoch: 0, Step: 285, Rank: 0, loss = 1.7895821332931519
nid001241: Epoch: 0, Step: 285, Rank: 1, loss = 1.3370109796524048
nid001241: [2024-11-12 13:05:09,753] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
nid001241: Model Parameters: 8.030 B, Latency: 6.70s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 286, Rank: 6, loss = 1.6980211734771729
nid001244: Epoch: 0, Step: 286, Rank: 7, loss = 1.7928826808929443
nid001241: Epoch: 0, Step: 286, Rank: 0, loss = 1.5404452085494995
nid001241: Epoch: 0, Step: 286, Rank: 3, loss = 1.2759677171707153
nid001241: Epoch: 0, Step: 286, Rank: 1, loss = 1.7118831872940063
nid001244: Epoch: 0, Step: 286, Rank: 4, loss = 1.511413335800171
nid001244: Epoch: 0, Step: 286, Rank: 5, loss = 1.632987141609192
nid001241: Epoch: 0, Step: 286, Rank: 2, loss = 1.5328068733215332
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 287, Rank: 6, loss = 1.7055001258850098
nid001244: Epoch: 0, Step: 287, Rank: 4, loss = 1.3623650074005127
nid001244: Epoch: 0, Step: 287, Rank: 7, loss = 1.681640863418579
nid001244: Epoch: 0, Step: 287, Rank: 5, loss = 1.5889034271240234
nid001241: Epoch: 0, Step: 287, Rank: 2, loss = 1.6094121932983398
nid001241: Epoch: 0, Step: 287, Rank: 3, loss = 1.5294616222381592
nid001241: Epoch: 0, Step: 287, Rank: 0, loss = 1.4713770151138306
nid001241: Epoch: 0, Step: 287, Rank: 1, loss = 2.1135449409484863
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 288, Rank: 3, loss = 1.2554336786270142
nid001241: Epoch: 0, Step: 288, Rank: 2, loss = 1.486183524131775
nid001244: Epoch: 0, Step: 288, Rank: 7, loss = 1.5318644046783447
nid001244: Epoch: 0, Step: 288, Rank: 6, loss = 1.8583159446716309
nid001241: Epoch: 0, Step: 288, Rank: 1, loss = 1.8233330249786377
nid001244: Epoch: 0, Step: 288, Rank: 5, loss = 1.753373384475708
nid001241: Epoch: 0, Step: 288, Rank: 0, loss = 1.8513402938842773
nid001244: Epoch: 0, Step: 288, Rank: 4, loss = 1.4783754348754883
nid001241: Model Parameters: 8.030 B, Latency: 6.99s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 289, Rank: 4, loss = 1.7936502695083618
nid001244: Epoch: 0, Step: 289, Rank: 7, loss = 1.686578631401062
nid001244: Epoch: 0, Step: 289, Rank: 5, loss = 1.7712081670761108
nid001241: Epoch: 0, Step: 289, Rank: 0, loss = 2.029107093811035
nid001244: Epoch: 0, Step: 289, Rank: 6, loss = 1.980454683303833
nid001241: Epoch: 0, Step: 289, Rank: 3, loss = 1.8439931869506836
nid001241: Epoch: 0, Step: 289, Rank: 1, loss = 1.3871315717697144
nid001241: Epoch: 0, Step: 289, Rank: 2, loss = 1.3802790641784668
nid001241: [2024-11-12 13:05:37,555] [INFO] [logging.py:128:log_dist] [Rank 0] step=290, skipped=4, lr=[9.413847691124117e-06, 9.413847691124117e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:05:37,555] [INFO] [timer.py:264:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=4.667775709316776, CurrSamplesPerSec=4.58816715201266, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 290, Rank: 6, loss = 1.5497716665267944
nid001244: Epoch: 0, Step: 290, Rank: 7, loss = 1.5425059795379639
nid001241: Epoch: 0, Step: 290, Rank: 2, loss = 1.518362045288086
nid001241: Epoch: 0, Step: 290, Rank: 3, loss = 1.3325543403625488
nid001244: Epoch: 0, Step: 290, Rank: 4, loss = 1.745879888534546
nid001241: Epoch: 0, Step: 290, Rank: 0, loss = 1.4097530841827393
nid001241: Epoch: 0, Step: 290, Rank: 1, loss = 1.6703824996948242
nid001244: Epoch: 0, Step: 290, Rank: 5, loss = 1.2657629251480103
nid001241: Model Parameters: 8.030 B, Latency: 6.86s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 291, Rank: 7, loss = 1.606533408164978
nid001244: Epoch: 0, Step: 291, Rank: 6, loss = 1.7190130949020386
nid001244: Epoch: 0, Step: 291, Rank: 5, loss = 1.6930828094482422
nid001244: Epoch: 0, Step: 291, Rank: 4, loss = 1.4739972352981567
nid001241: Epoch: 0, Step: 291, Rank: 3, loss = 1.376805305480957
nid001241: Epoch: 0, Step: 291, Rank: 2, loss = 1.8349281549453735
nid001241: Epoch: 0, Step: 291, Rank: 1, loss = 1.244568943977356
nid001241: Epoch: 0, Step: 291, Rank: 0, loss = 1.9152926206588745
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 292, Rank: 7, loss = 1.7895050048828125
nid001244: Epoch: 0, Step: 292, Rank: 4, loss = 1.5782325267791748
nid001244: Epoch: 0, Step: 292, Rank: 5, loss = 1.7154759168624878
nid001241: Epoch: 0, Step: 292, Rank: 0, loss = 1.204269289970398
nid001241: Epoch: 0, Step: 292, Rank: 3, loss = 1.6048842668533325
nid001244: Epoch: 0, Step: 292, Rank: 6, loss = 1.5165760517120361
nid001241: Epoch: 0, Step: 292, Rank: 1, loss = 1.6614739894866943
nid001241: Epoch: 0, Step: 292, Rank: 2, loss = 1.6017931699752808
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 293, Rank: 7, loss = 1.7651176452636719
nid001244: Epoch: 0, Step: 293, Rank: 6, loss = 1.2040584087371826
nid001241: Epoch: 0, Step: 293, Rank: 3, loss = 1.503027081489563
nid001241: Epoch: 0, Step: 293, Rank: 0, loss = 1.3911068439483643
nid001241: Epoch: 0, Step: 293, Rank: 1, loss = 1.2263168096542358
nid001244: Epoch: 0, Step: 293, Rank: 4, loss = 1.3978075981140137
nid001244: Epoch: 0, Step: 293, Rank: 5, loss = 1.354996681213379
nid001241: Epoch: 0, Step: 293, Rank: 2, loss = 1.4319555759429932
nid001241: Model Parameters: 8.030 B, Latency: 7.00s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 294, Rank: 7, loss = 1.68990957736969
nid001241: Epoch: 0, Step: 294, Rank: 3, loss = 1.4057906866073608
nid001241: Epoch: 0, Step: 294, Rank: 2, loss = 1.5336649417877197
nid001244: Epoch: 0, Step: 294, Rank: 5, loss = 1.632866382598877
nid001244: Epoch: 0, Step: 294, Rank: 6, loss = 1.7098451852798462
nid001244: Epoch: 0, Step: 294, Rank: 4, loss = 1.6991811990737915
nid001241: Epoch: 0, Step: 294, Rank: 0, loss = 1.4672778844833374
nid001241: Epoch: 0, Step: 294, Rank: 1, loss = 1.6759368181228638
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 295, Rank: 7, loss = 1.3406636714935303
nid001244: Epoch: 0, Step: 295, Rank: 4, loss = 1.5309162139892578
nid001244: Epoch: 0, Step: 295, Rank: 5, loss = 1.6326740980148315
nid001241: Epoch: 0, Step: 295, Rank: 0, loss = 1.7553529739379883
nid001241: Epoch: 0, Step: 295, Rank: 3, loss = 1.8312935829162598
nid001244: Epoch: 0, Step: 295, Rank: 6, loss = 1.7228291034698486
nid001241: Epoch: 0, Step: 295, Rank: 1, loss = 1.5825273990631104
nid001241: Epoch: 0, Step: 295, Rank: 2, loss = 1.2812542915344238
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 296, Rank: 6, loss = 1.775480031967163
nid001244: Epoch: 0, Step: 296, Rank: 7, loss = 1.745113492012024
nid001241: Epoch: 0, Step: 296, Rank: 2, loss = 1.5526100397109985
nid001244: Epoch: 0, Step: 296, Rank: 4, loss = 1.1632773876190186
nid001241: Epoch: 0, Step: 296, Rank: 3, loss = 1.7918082475662231
nid001244: Epoch: 0, Step: 296, Rank: 5, loss = 1.718004584312439
nid001241: Epoch: 0, Step: 296, Rank: 0, loss = 1.630787968635559
nid001241: Epoch: 0, Step: 296, Rank: 1, loss = 1.5277695655822754
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 297, Rank: 7, loss = 1.6222120523452759
nid001244: Epoch: 0, Step: 297, Rank: 6, loss = 1.6901240348815918
nid001241: Epoch: 0, Step: 297, Rank: 3, loss = 1.690758466720581
nid001244: Epoch: 0, Step: 297, Rank: 4, loss = 1.5486059188842773
nid001244: Epoch: 0, Step: 297, Rank: 5, loss = 1.9468085765838623
nid001241: Epoch: 0, Step: 297, Rank: 2, loss = 1.3866446018218994
nid001241: Epoch: 0, Step: 297, Rank: 1, loss = 1.464542269706726
nid001241: Epoch: 0, Step: 297, Rank: 0, loss = 1.8715403079986572
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 298, Rank: 4, loss = 1.8374794721603394
nid001244: Epoch: 0, Step: 298, Rank: 7, loss = 1.7473305463790894
nid001241: Epoch: 0, Step: 298, Rank: 0, loss = 1.435347557067871
nid001241: Epoch: 0, Step: 298, Rank: 3, loss = 1.4496015310287476
nid001244: Epoch: 0, Step: 298, Rank: 5, loss = 1.6895174980163574
nid001241: Epoch: 0, Step: 298, Rank: 1, loss = 1.7176927328109741
nid001241: Epoch: 0, Step: 298, Rank: 2, loss = 1.746184229850769
nid001244: Epoch: 0, Step: 298, Rank: 6, loss = 1.452510952949524
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 299, Rank: 4, loss = 1.4376941919326782
nid001241: Epoch: 0, Step: 299, Rank: 2, loss = 1.4149190187454224
nid001244: Epoch: 0, Step: 299, Rank: 7, loss = 1.3051713705062866
nid001241: Epoch: 0, Step: 299, Rank: 3, loss = 1.738775372505188
nid001241: Epoch: 0, Step: 299, Rank: 0, loss = 1.6067782640457153
nid001244: Epoch: 0, Step: 299, Rank: 5, loss = 1.6600645780563354
nid001241: Epoch: 0, Step: 299, Rank: 1, loss = 1.4865976572036743
nid001244: Epoch: 0, Step: 299, Rank: 6, loss = 1.5181573629379272
nid001241: [2024-11-12 13:06:46,198] [INFO] [logging.py:128:log_dist] [Rank 0] step=300, skipped=4, lr=[9.397193075205258e-06, 9.397193075205258e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:06:46,198] [INFO] [timer.py:264:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=4.667685260093443, CurrSamplesPerSec=4.674138301875681, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 300, Rank: 7, loss = 1.757194995880127
nid001244: Epoch: 0, Step: 300, Rank: 6, loss = 1.684830665588379
nid001241: Epoch: 0, Step: 300, Rank: 3, loss = 1.5572365522384644
nid001244: Epoch: 0, Step: 300, Rank: 5, loss = 1.59255051612854
nid001241: Epoch: 0, Step: 300, Rank: 2, loss = 1.371379017829895
nid001241: Epoch: 0, Step: 300, Rank: 1, loss = 1.57057785987854
nid001241: Epoch: 0, Step: 300, Rank: 0, loss = 1.9495010375976562
nid001244: Epoch: 0, Step: 300, Rank: 4, loss = 1.4391475915908813
nid001241: Model Parameters: 8.030 B, Latency: 7.00s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Step 300: GPU Memory Usage
nid001241: GPU 0 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36862 MB
nid001241:   Free:  4097 MB
nid001241:   Usage: 36862/40960 MB (90.00%)
nid001241: 
nid001241: GPU 1 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36862 MB
nid001241:   Free:  4097 MB
nid001241:   Usage: 36862/40960 MB (90.00%)
nid001241: 
nid001241: GPU 2 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36886 MB
nid001241:   Free:  4073 MB
nid001241:   Usage: 36886/40960 MB (90.05%)
nid001241: 
nid001241: GPU 3 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36838 MB
nid001241:   Free:  4121 MB
nid001241:   Usage: 36838/40960 MB (89.94%)
nid001241: 
nid001244: Epoch: 0, Step: 301, Rank: 7, loss = 2.0020699501037598
nid001244: Epoch: 0, Step: 301, Rank: 6, loss = 1.394046664237976
nid001244: Epoch: 0, Step: 301, Rank: 5, loss = 1.4856196641921997
nid001241: Epoch: 0, Step: 301, Rank: 3, loss = 1.6917667388916016
nid001241: Epoch: 0, Step: 301, Rank: 2, loss = 1.5716184377670288
nid001241: Epoch: 0, Step: 301, Rank: 1, loss = 1.5248714685440063
nid001244: Epoch: 0, Step: 301, Rank: 4, loss = 1.6429766416549683
nid001241: Epoch: 0, Step: 301, Rank: 0, loss = 1.4339739084243774
nid001241: Model Parameters: 8.030 B, Latency: 7.11s, TFLOPs: 4.09, Samples/sec: 0.56, Time/seq 1.78s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 302, Rank: 7, loss = 1.3679847717285156
nid001244: Epoch: 0, Step: 302, Rank: 6, loss = 1.9883735179901123
nid001241: Epoch: 0, Step: 302, Rank: 2, loss = 1.2761867046356201
nid001241: Epoch: 0, Step: 302, Rank: 3, loss = 1.6256730556488037
nid001244: Epoch: 0, Step: 302, Rank: 5, loss = 1.4600099325180054
nid001241: Epoch: 0, Step: 302, Rank: 1, loss = 1.6641395092010498
nid001241: Epoch: 0, Step: 302, Rank: 0, loss = 1.5721778869628906
nid001244: Epoch: 0, Step: 302, Rank: 4, loss = 1.298359751701355
nid001241: Model Parameters: 8.030 B, Latency: 7.01s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 303, Rank: 7, loss = 1.7723469734191895
nid001244: Epoch: 0, Step: 303, Rank: 6, loss = 1.7430850267410278
nid001241: Epoch: 0, Step: 303, Rank: 3, loss = 1.5000039339065552
nid001241: Epoch: 0, Step: 303, Rank: 2, loss = 1.4705619812011719
nid001241: Epoch: 0, Step: 303, Rank: 1, loss = 1.7409385442733765
nid001241: Epoch: 0, Step: 303, Rank: 0, loss = 1.6512713432312012
nid001244: Epoch: 0, Step: 303, Rank: 5, loss = 1.4122689962387085
nid001244: Epoch: 0, Step: 303, Rank: 4, loss = 1.5487722158432007
nid001241: Model Parameters: 8.030 B, Latency: 7.00s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 304, Rank: 2, loss = 1.657213568687439
nid001241: Epoch: 0, Step: 304, Rank: 3, loss = 1.711312174797058
nid001244: Epoch: 0, Step: 304, Rank: 6, loss = 1.5182491540908813
nid001244: Epoch: 0, Step: 304, Rank: 7, loss = 1.3566246032714844
nid001241: Epoch: 0, Step: 304, Rank: 1, loss = 1.825925350189209
nid001241: Epoch: 0, Step: 304, Rank: 0, loss = 1.4092987775802612
nid001244: Epoch: 0, Step: 304, Rank: 5, loss = 1.5725778341293335
nid001244: Epoch: 0, Step: 304, Rank: 4, loss = 1.8227866888046265
nid001241: Model Parameters: 8.030 B, Latency: 6.99s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 305, Rank: 0, loss = 1.4036767482757568
nid001241: Epoch: 0, Step: 305, Rank: 3, loss = 1.5937716960906982
nid001241: Epoch: 0, Step: 305, Rank: 1, loss = 1.6689079999923706
nid001241: Epoch: 0, Step: 305, Rank: 2, loss = 1.59015953540802
nid001244: Epoch: 0, Step: 305, Rank: 7, loss = 1.844760775566101
nid001244: Epoch: 0, Step: 305, Rank: 4, loss = 1.2710219621658325
nid001244: Epoch: 0, Step: 305, Rank: 5, loss = 1.3847898244857788
nid001244: Epoch: 0, Step: 305, Rank: 6, loss = 1.512718915939331
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 306, Rank: 7, loss = 1.5071675777435303
nid001244: Epoch: 0, Step: 306, Rank: 4, loss = 1.7810914516448975
nid001244: Epoch: 0, Step: 306, Rank: 5, loss = 1.863980770111084
nid001241: Epoch: 0, Step: 306, Rank: 0, loss = 1.541660189628601
nid001244: Epoch: 0, Step: 306, Rank: 6, loss = 1.639424443244934
nid001241: Epoch: 0, Step: 306, Rank: 3, loss = 1.804405927658081
nid001241: Epoch: 0, Step: 306, Rank: 1, loss = 1.4495681524276733
nid001241: Epoch: 0, Step: 306, Rank: 2, loss = 1.7001144886016846
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 307, Rank: 6, loss = 1.7301884889602661
nid001241: Epoch: 0, Step: 307, Rank: 2, loss = 1.5301469564437866
nid001244: Epoch: 0, Step: 307, Rank: 4, loss = 1.6819218397140503
nid001244: Epoch: 0, Step: 307, Rank: 7, loss = 1.5579198598861694
nid001241: Epoch: 0, Step: 307, Rank: 3, loss = 1.4922659397125244
nid001241: Epoch: 0, Step: 307, Rank: 0, loss = 1.511855959892273
nid001244: Epoch: 0, Step: 307, Rank: 5, loss = 1.6397417783737183
nid001241: Epoch: 0, Step: 307, Rank: 1, loss = 1.1556936502456665
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 308, Rank: 7, loss = 1.6525473594665527
nid001244: Epoch: 0, Step: 308, Rank: 6, loss = 1.725520133972168
nid001244: Epoch: 0, Step: 308, Rank: 4, loss = 1.6077156066894531
nid001244: Epoch: 0, Step: 308, Rank: 5, loss = 1.5910296440124512
nid001241: Epoch: 0, Step: 308, Rank: 0, loss = 1.8329201936721802
nid001241: Epoch: 0, Step: 308, Rank: 3, loss = 1.7986280918121338
nid001241: Epoch: 0, Step: 308, Rank: 1, loss = 1.8301078081130981
nid001241: Epoch: 0, Step: 308, Rank: 2, loss = 1.3946322202682495
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 309, Rank: 7, loss = 1.5534932613372803
nid001244: Epoch: 0, Step: 309, Rank: 6, loss = 1.4659409523010254
nid001244: Epoch: 0, Step: 309, Rank: 5, loss = 1.5910544395446777
nid001241: Epoch: 0, Step: 309, Rank: 3, loss = 1.7937519550323486
nid001241: Epoch: 0, Step: 309, Rank: 2, loss = 1.457279920578003
nid001241: Epoch: 0, Step: 309, Rank: 1, loss = 1.534134030342102
nid001244: Epoch: 0, Step: 309, Rank: 4, loss = 1.5716221332550049
nid001241: Epoch: 0, Step: 309, Rank: 0, loss = 1.6454085111618042
nid001241: [2024-11-12 13:07:55,596] [INFO] [logging.py:128:log_dist] [Rank 0] step=310, skipped=4, lr=[9.379986778481033e-06, 9.379986778481033e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:07:55,596] [INFO] [timer.py:264:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=4.665936937872041, CurrSamplesPerSec=4.655430147089086, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 310, Rank: 6, loss = 1.4883207082748413
nid001244: Epoch: 0, Step: 310, Rank: 4, loss = 1.6619415283203125
nid001244: Epoch: 0, Step: 310, Rank: 7, loss = 1.5907601118087769
nid001241: Epoch: 0, Step: 310, Rank: 2, loss = 1.9581623077392578
nid001241: Epoch: 0, Step: 310, Rank: 3, loss = 1.9039638042449951
nid001244: Epoch: 0, Step: 310, Rank: 5, loss = 1.6513776779174805
nid001241: Epoch: 0, Step: 310, Rank: 0, loss = 1.8488545417785645
nid001241: Epoch: 0, Step: 310, Rank: 1, loss = 1.721051573753357
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 311, Rank: 7, loss = 1.6597797870635986
nid001244: Epoch: 0, Step: 311, Rank: 4, loss = 1.7987780570983887
nid001244: Epoch: 0, Step: 311, Rank: 6, loss = 1.229248285293579
nid001244: Epoch: 0, Step: 311, Rank: 5, loss = 1.9148272275924683
nid001241: Epoch: 0, Step: 311, Rank: 0, loss = 1.6759274005889893
nid001241: Epoch: 0, Step: 311, Rank: 3, loss = 1.8737884759902954
nid001241: Epoch: 0, Step: 311, Rank: 1, loss = 1.4967187643051147
nid001241: Epoch: 0, Step: 311, Rank: 2, loss = 1.6115366220474243
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 312, Rank: 7, loss = 1.7283378839492798
nid001244: Epoch: 0, Step: 312, Rank: 6, loss = 1.8416472673416138
nid001241: Epoch: 0, Step: 312, Rank: 0, loss = 1.9788625240325928
nid001244: Epoch: 0, Step: 312, Rank: 4, loss = 1.552942156791687
nid001244: Epoch: 0, Step: 312, Rank: 5, loss = 1.4069713354110718
nid001241: Epoch: 0, Step: 312, Rank: 1, loss = 1.4736820459365845
nid001241: Epoch: 0, Step: 312, Rank: 3, loss = 1.5380593538284302
nid001241: Epoch: 0, Step: 312, Rank: 2, loss = 1.7920836210250854
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 313, Rank: 7, loss = 1.68836510181427
nid001244: Epoch: 0, Step: 313, Rank: 4, loss = 1.2213408946990967
nid001244: Epoch: 0, Step: 313, Rank: 5, loss = 1.2804524898529053
nid001244: Epoch: 0, Step: 313, Rank: 6, loss = 1.6910216808319092
nid001241: Epoch: 0, Step: 313, Rank: 0, loss = 1.5733249187469482
nid001241: Epoch: 0, Step: 313, Rank: 3, loss = 1.7060858011245728
nid001241: Epoch: 0, Step: 313, Rank: 1, loss = 1.7053931951522827
nid001241: Epoch: 0, Step: 313, Rank: 2, loss = 1.836272120475769
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 314, Rank: 6, loss = 1.6255584955215454
nid001244: Epoch: 0, Step: 314, Rank: 7, loss = 1.4000521898269653
nid001244: Epoch: 0, Step: 314, Rank: 4, loss = 1.3594157695770264
nid001241: Epoch: 0, Step: 314, Rank: 2, loss = 1.791245698928833
nid001244: Epoch: 0, Step: 314, Rank: 5, loss = 1.3175112009048462
nid001241: Epoch: 0, Step: 314, Rank: 3, loss = 1.6823278665542603
nid001241: Epoch: 0, Step: 314, Rank: 0, loss = 1.4981393814086914
nid001241: Epoch: 0, Step: 314, Rank: 1, loss = 1.546496033668518
nid001241: Model Parameters: 8.030 B, Latency: 6.73s, TFLOPs: 4.33, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 315, Rank: 4, loss = 1.7710012197494507
nid001244: Epoch: 0, Step: 315, Rank: 7, loss = 1.1608991622924805
nid001244: Epoch: 0, Step: 315, Rank: 5, loss = 1.2876659631729126
nid001244: Epoch: 0, Step: 315, Rank: 6, loss = 1.2265324592590332
nid001241: Epoch: 0, Step: 315, Rank: 0, loss = 1.706504464149475
nid001241: Epoch: 0, Step: 315, Rank: 3, loss = 1.559394121170044
nid001241: Epoch: 0, Step: 315, Rank: 1, loss = 1.843932032585144
nid001241: Epoch: 0, Step: 315, Rank: 2, loss = 1.597693920135498
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 316, Rank: 0, loss = 1.8936628103256226
nid001241: Epoch: 0, Step: 316, Rank: 2, loss = 1.4358874559402466
nid001241: Epoch: 0, Step: 316, Rank: 3, loss = 1.7664411067962646
nid001244: Epoch: 0, Step: 316, Rank: 6, loss = 1.6951329708099365
nid001244: Epoch: 0, Step: 316, Rank: 7, loss = 1.3644002676010132
nid001241: Epoch: 0, Step: 316, Rank: 1, loss = 1.7118148803710938
nid001244: Epoch: 0, Step: 316, Rank: 5, loss = 1.059011697769165
nid001244: Epoch: 0, Step: 316, Rank: 4, loss = 1.9505375623703003
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 317, Rank: 4, loss = 1.4621354341506958
nid001244: Epoch: 0, Step: 317, Rank: 7, loss = 1.763657569885254
nid001244: Epoch: 0, Step: 317, Rank: 5, loss = 1.5310063362121582
nid001241: Epoch: 0, Step: 317, Rank: 0, loss = 1.5247026681900024
nid001241: Epoch: 0, Step: 317, Rank: 3, loss = 1.4882773160934448
nid001244: Epoch: 0, Step: 317, Rank: 6, loss = 1.813178300857544
nid001241: Epoch: 0, Step: 317, Rank: 1, loss = 1.598751187324524
nid001241: Epoch: 0, Step: 317, Rank: 2, loss = 1.8225595951080322
nid001241: [2024-11-12 13:08:49,962] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 318, Rank: 7, loss = 1.7677631378173828
nid001244: Epoch: 0, Step: 318, Rank: 4, loss = 1.7492555379867554
nid001241: Epoch: 0, Step: 318, Rank: 0, loss = 1.7449827194213867
nid001244: Epoch: 0, Step: 318, Rank: 5, loss = 1.718499779701233
nid001241: Epoch: 0, Step: 318, Rank: 3, loss = 1.6883891820907593
nid001241: Epoch: 0, Step: 318, Rank: 1, loss = 2.0901739597320557
nid001241: Epoch: 0, Step: 318, Rank: 2, loss = 1.2676459550857544
nid001244: Epoch: 0, Step: 318, Rank: 6, loss = 1.4961965084075928
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 319, Rank: 4, loss = 1.476459264755249
nid001244: Epoch: 0, Step: 319, Rank: 7, loss = 1.9154843091964722
nid001241: Epoch: 0, Step: 319, Rank: 0, loss = 1.614274024963379
nid001244: Epoch: 0, Step: 319, Rank: 5, loss = 1.3786240816116333
nid001244: Epoch: 0, Step: 319, Rank: 6, loss = 1.9551749229431152
nid001241: Epoch: 0, Step: 319, Rank: 3, loss = 1.5544407367706299
nid001241: Epoch: 0, Step: 319, Rank: 1, loss = 1.6350672245025635
nid001241: Epoch: 0, Step: 319, Rank: 2, loss = 1.6154333353042603
nid001241: [2024-11-12 13:09:03,529] [INFO] [logging.py:128:log_dist] [Rank 0] step=320, skipped=5, lr=[9.364031138875579e-06, 9.364031138875579e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:09:03,529] [INFO] [timer.py:264:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=4.667433976032179, CurrSamplesPerSec=4.681357529141473, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 320, Rank: 7, loss = 1.6710424423217773
nid001244: Epoch: 0, Step: 320, Rank: 4, loss = 1.83363938331604
nid001241: Epoch: 0, Step: 320, Rank: 0, loss = 1.8292485475540161
nid001244: Epoch: 0, Step: 320, Rank: 5, loss = 1.445902943611145
nid001241: Epoch: 0, Step: 320, Rank: 3, loss = 1.4912346601486206
nid001241: Epoch: 0, Step: 320, Rank: 1, loss = 1.8244965076446533
nid001241: Epoch: 0, Step: 320, Rank: 2, loss = 1.4575601816177368
nid001244: Epoch: 0, Step: 320, Rank: 6, loss = 1.7571208477020264
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 321, Rank: 4, loss = 1.4990806579589844
nid001244: Epoch: 0, Step: 321, Rank: 7, loss = 0.9377939701080322
nid001241: Epoch: 0, Step: 321, Rank: 0, loss = 1.7050185203552246
nid001241: Epoch: 0, Step: 321, Rank: 3, loss = 1.747652530670166
nid001244: Epoch: 0, Step: 321, Rank: 5, loss = 1.664262294769287
nid001244: Epoch: 0, Step: 321, Rank: 6, loss = 1.1365797519683838
nid001241: Epoch: 0, Step: 321, Rank: 1, loss = 1.6622698307037354
nid001241: Epoch: 0, Step: 321, Rank: 2, loss = 1.568848729133606
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 322, Rank: 3, loss = 1.7858610153198242
nid001241: Epoch: 0, Step: 322, Rank: 1, loss = 1.6955510377883911
nid001241: Epoch: 0, Step: 322, Rank: 2, loss = 1.352906346321106
nid001244: Epoch: 0, Step: 322, Rank: 7, loss = 1.6951076984405518
nid001244: Epoch: 0, Step: 322, Rank: 6, loss = 1.490346908569336
nid001241: Epoch: 0, Step: 322, Rank: 0, loss = 1.1882127523422241
nid001244: Epoch: 0, Step: 322, Rank: 5, loss = 1.6991348266601562
nid001244: Epoch: 0, Step: 322, Rank: 4, loss = 1.675132393836975
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 323, Rank: 3, loss = 1.7604666948318481
nid001241: Epoch: 0, Step: 323, Rank: 2, loss = 1.778269648551941
nid001244: Epoch: 0, Step: 323, Rank: 7, loss = 1.6023157835006714
nid001244: Epoch: 0, Step: 323, Rank: 6, loss = 1.5804383754730225
nid001244: Epoch: 0, Step: 323, Rank: 5, loss = 1.8488227128982544
nid001241: Epoch: 0, Step: 323, Rank: 1, loss = 1.7958966493606567
nid001244: Epoch: 0, Step: 323, Rank: 4, loss = 1.6163510084152222
nid001241: Epoch: 0, Step: 323, Rank: 0, loss = 1.518004298210144
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 324, Rank: 2, loss = 1.5126876831054688
nid001244: Epoch: 0, Step: 324, Rank: 6, loss = 1.561070442199707
nid001244: Epoch: 0, Step: 324, Rank: 7, loss = 1.4160195589065552
nid001241: Epoch: 0, Step: 324, Rank: 0, loss = 1.8144779205322266
nid001241: Epoch: 0, Step: 324, Rank: 3, loss = 2.1424100399017334
nid001241: Epoch: 0, Step: 324, Rank: 1, loss = 1.7486778497695923
nid001244: Epoch: 0, Step: 324, Rank: 5, loss = 1.5762323141098022
nid001244: Epoch: 0, Step: 324, Rank: 4, loss = 1.5220564603805542
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 325, Rank: 7, loss = 1.67070734500885
nid001244: Epoch: 0, Step: 325, Rank: 4, loss = 1.6139676570892334
nid001244: Epoch: 0, Step: 325, Rank: 5, loss = 0.8002318739891052
nid001241: Epoch: 0, Step: 325, Rank: 0, loss = 1.8407056331634521
nid001241: Epoch: 0, Step: 325, Rank: 1, loss = 1.639858365058899Epoch: 0, Step: 325, Rank: 3, loss = 1.7388100624084473
nid001241: 
nid001244: Epoch: 0, Step: 325, Rank: 6, loss = 1.4394629001617432
nid001241: Epoch: 0, Step: 325, Rank: 2, loss = 1.4459232091903687
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 326, Rank: 6, loss = 1.781053066253662
nid001244: Epoch: 0, Step: 326, Rank: 7, loss = 1.5572093725204468
nid001244: Epoch: 0, Step: 326, Rank: 4, loss = 1.5565577745437622
nid001241: Epoch: 0, Step: 326, Rank: 2, loss = 1.6366878747940063
nid001241: Epoch: 0, Step: 326, Rank: 3, loss = 1.381186842918396
nid001241: Epoch: 0, Step: 326, Rank: 0, loss = 1.2910100221633911
nid001244: Epoch: 0, Step: 326, Rank: 5, loss = 1.484407663345337
nid001241: Epoch: 0, Step: 326, Rank: 1, loss = 1.70552396774292
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 327, Rank: 4, loss = 1.817400574684143
nid001241: Epoch: 0, Step: 327, Rank: 0, loss = 1.770603060722351
nid001244: Epoch: 0, Step: 327, Rank: 7, loss = 1.4833215475082397
nid001241: Epoch: 0, Step: 327, Rank: 3, loss = 1.4493952989578247
nid001241: Epoch: 0, Step: 327, Rank: 1, loss = 1.6917678117752075
nid001241: Epoch: 0, Step: 327, Rank: 2, loss = 1.7448539733886719
nid001244: Epoch: 0, Step: 327, Rank: 5, loss = 1.5779048204421997
nid001244: Epoch: 0, Step: 327, Rank: 6, loss = 1.724173903465271
nid001241: Model Parameters: 8.030 B, Latency: 6.92s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 328, Rank: 4, loss = 1.8006439208984375
nid001244: Epoch: 0, Step: 328, Rank: 7, loss = 1.3828099966049194
nid001241: Epoch: 0, Step: 328, Rank: 0, loss = 1.943044662475586
nid001241: Epoch: 0, Step: 328, Rank: 3, loss = 2.1014788150787354
nid001241: Epoch: 0, Step: 328, Rank: 1, loss = 1.596269965171814
nid001244: Epoch: 0, Step: 328, Rank: 5, loss = 1.1723524332046509
nid001241: Epoch: 0, Step: 328, Rank: 2, loss = 1.6682852506637573
nid001244: Epoch: 0, Step: 328, Rank: 6, loss = 1.9056828022003174
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 329, Rank: 4, loss = 1.4881237745285034
nid001244: Epoch: 0, Step: 329, Rank: 7, loss = 0.9917138814926147
nid001241: Epoch: 0, Step: 329, Rank: 0, loss = 1.3145239353179932
nid001244: Epoch: 0, Step: 329, Rank: 5, loss = 1.761067509651184
nid001241: Epoch: 0, Step: 329, Rank: 3, loss = 1.5470887422561646
nid001241: Epoch: 0, Step: 329, Rank: 1, loss = 1.7271761894226074
nid001244: Epoch: 0, Step: 329, Rank: 6, loss = 1.4039039611816406
nid001241: Epoch: 0, Step: 329, Rank: 2, loss = 1.834019422531128
nid001241: [2024-11-12 13:10:12,029] [INFO] [logging.py:128:log_dist] [Rank 0] step=330, skipped=5, lr=[9.345782423123852e-06, 9.345782423123852e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:10:12,030] [INFO] [timer.py:264:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=4.6676620190993905, CurrSamplesPerSec=4.714152911489041, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 330, Rank: 7, loss = 1.6520603895187378
nid001244: Epoch: 0, Step: 330, Rank: 4, loss = 1.2896102666854858
nid001244: Epoch: 0, Step: 330, Rank: 5, loss = 1.5415968894958496
nid001241: Epoch: 0, Step: 330, Rank: 0, loss = 1.731547474861145
nid001244: Epoch: 0, Step: 330, Rank: 6, loss = 1.9739210605621338
nid001241: Epoch: 0, Step: 330, Rank: 3, loss = 1.7104547023773193Epoch: 0, Step: 330, Rank: 1, loss = 1.6686689853668213
nid001241: 
nid001241: Epoch: 0, Step: 330, Rank: 2, loss = 1.634535789489746
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 331, Rank: 7, loss = 1.2810527086257935
nid001244: Epoch: 0, Step: 331, Rank: 4, loss = 1.9109013080596924
nid001241: Epoch: 0, Step: 331, Rank: 0, loss = 1.390404462814331
nid001244: Epoch: 0, Step: 331, Rank: 5, loss = 2.163938283920288
nid001241: Epoch: 0, Step: 331, Rank: 3, loss = 1.312804102897644
nid001241: Epoch: 0, Step: 331, Rank: 1, loss = 1.629833459854126
nid001241: Epoch: 0, Step: 331, Rank: 2, loss = 1.469455599784851
nid001244: Epoch: 0, Step: 331, Rank: 6, loss = 1.4811419248580933
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 332, Rank: 4, loss = 1.719445824623108
nid001244: Epoch: 0, Step: 332, Rank: 7, loss = 1.9665899276733398
nid001244: Epoch: 0, Step: 332, Rank: 5, loss = 1.8419344425201416
nid001241: Epoch: 0, Step: 332, Rank: 0, loss = 1.8188918828964233
nid001241: Epoch: 0, Step: 332, Rank: 3, loss = 1.5262670516967773
nid001241: Epoch: 0, Step: 332, Rank: 1, loss = 1.4716044664382935
nid001244: Epoch: 0, Step: 332, Rank: 6, loss = 1.757891058921814
nid001241: Epoch: 0, Step: 332, Rank: 2, loss = 1.9962400197982788
nid001241: Model Parameters: 8.030 B, Latency: 6.90s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 333, Rank: 6, loss = 1.6837599277496338
nid001244: Epoch: 0, Step: 333, Rank: 7, loss = 1.8281192779541016
nid001244: Epoch: 0, Step: 333, Rank: 4, loss = 1.8467190265655518
nid001241: Epoch: 0, Step: 333, Rank: 0, loss = 1.5748052597045898
nid001241: Epoch: 0, Step: 333, Rank: 3, loss = 1.470276117324829
nid001244: Epoch: 0, Step: 333, Rank: 5, loss = 1.912632703781128
nid001241: Epoch: 0, Step: 333, Rank: 1, loss = 1.588797926902771
nid001241: Epoch: 0, Step: 333, Rank: 2, loss = 1.7726393938064575
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 334, Rank: 7, loss = 1.7441508769989014
nid001244: Epoch: 0, Step: 334, Rank: 4, loss = 1.6517622470855713
nid001244: Epoch: 0, Step: 334, Rank: 5, loss = 1.2678486108779907
nid001241: Epoch: 0, Step: 334, Rank: 0, loss = 1.6306493282318115
nid001241: Epoch: 0, Step: 334, Rank: 3, loss = 1.4305498600006104
nid001244: Epoch: 0, Step: 334, Rank: 6, loss = 1.6317362785339355
nid001241: Epoch: 0, Step: 334, Rank: 1, loss = 1.7157313823699951
nid001241: Epoch: 0, Step: 334, Rank: 2, loss = 1.6147902011871338
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 335, Rank: 4, loss = 1.7081549167633057
nid001244: Epoch: 0, Step: 335, Rank: 7, loss = 1.341011643409729
nid001241: Epoch: 0, Step: 335, Rank: 0, loss = 1.382961630821228
nid001241: Epoch: 0, Step: 335, Rank: 3, loss = 1.1917200088500977
nid001244: Epoch: 0, Step: 335, Rank: 5, loss = 1.9077280759811401
nid001241: Epoch: 0, Step: 335, Rank: 1, loss = 1.986748456954956
nid001244: Epoch: 0, Step: 335, Rank: 6, loss = 1.2939237356185913
nid001241: Epoch: 0, Step: 335, Rank: 2, loss = 1.4382773637771606
nid001241: Model Parameters: 8.030 B, Latency: 7.10s, TFLOPs: 4.10, Samples/sec: 0.56, Time/seq 1.77s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 336, Rank: 4, loss = 1.5951954126358032
nid001244: Epoch: 0, Step: 336, Rank: 7, loss = 1.7585442066192627
nid001244: Epoch: 0, Step: 336, Rank: 5, loss = 1.7055964469909668
nid001241: Epoch: 0, Step: 336, Rank: 0, loss = 1.3524507284164429
nid001244: Epoch: 0, Step: 336, Rank: 6, loss = 1.9088934659957886
nid001241: Epoch: 0, Step: 336, Rank: 3, loss = 1.5398952960968018
nid001241: Epoch: 0, Step: 336, Rank: 1, loss = 1.739822506904602
nid001241: Epoch: 0, Step: 336, Rank: 2, loss = 1.5832231044769287
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.22, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 337, Rank: 6, loss = 1.4193674325942993
nid001244: Epoch: 0, Step: 337, Rank: 7, loss = 1.6781699657440186
nid001241: Epoch: 0, Step: 337, Rank: 2, loss = 1.577186107635498
nid001244: Epoch: 0, Step: 337, Rank: 4, loss = 1.2992433309555054
nid001241: Epoch: 0, Step: 337, Rank: 3, loss = 1.4889652729034424
nid001241: Epoch: 0, Step: 337, Rank: 0, loss = 1.7983953952789307
nid001244: Epoch: 0, Step: 337, Rank: 5, loss = 1.5264477729797363
nid001241: Epoch: 0, Step: 337, Rank: 1, loss = 1.8084683418273926
nid001241: Model Parameters: 8.030 B, Latency: 7.00s, TFLOPs: 4.16, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 338, Rank: 7, loss = 1.4035769701004028
nid001244: Epoch: 0, Step: 338, Rank: 4, loss = 1.7116268873214722
nid001244: Epoch: 0, Step: 338, Rank: 5, loss = 1.500412106513977
nid001241: Epoch: 0, Step: 338, Rank: 0, loss = 1.5334075689315796
nid001244: Epoch: 0, Step: 338, Rank: 6, loss = 1.697935700416565
nid001241: Epoch: 0, Step: 338, Rank: 3, loss = 1.5122085809707642
nid001241: Epoch: 0, Step: 338, Rank: 1, loss = 1.7048869132995605
nid001241: Epoch: 0, Step: 338, Rank: 2, loss = 1.7235435247421265
nid001241: Model Parameters: 8.030 B, Latency: 7.08s, TFLOPs: 4.11, Samples/sec: 0.56, Time/seq 1.77s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 339, Rank: 4, loss = 1.4666630029678345
nid001244: Epoch: 0, Step: 339, Rank: 7, loss = 1.4978734254837036
nid001241: Epoch: 0, Step: 339, Rank: 0, loss = 1.4954646825790405
nid001241: Epoch: 0, Step: 339, Rank: 3, loss = 1.6930009126663208
nid001241: Epoch: 0, Step: 339, Rank: 1, loss = 1.4699339866638184
nid001244: Epoch: 0, Step: 339, Rank: 5, loss = 1.306705117225647
nid001244: Epoch: 0, Step: 339, Rank: 6, loss = 1.510999083518982
nid001241: Epoch: 0, Step: 339, Rank: 2, loss = 1.5671806335449219
nid001241: [2024-11-12 13:11:21,159] [INFO] [logging.py:128:log_dist] [Rank 0] step=340, skipped=5, lr=[9.326988229775916e-06, 9.326988229775916e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:11:21,159] [INFO] [timer.py:264:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=4.666604402265511, CurrSamplesPerSec=4.559468529744107, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.14, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 340, Rank: 7, loss = 1.681835651397705
nid001244: Epoch: 0, Step: 340, Rank: 4, loss = 1.4259908199310303
nid001241: Epoch: 0, Step: 340, Rank: 0, loss = 1.5275412797927856
nid001244: Epoch: 0, Step: 340, Rank: 5, loss = 2.007321357727051
nid001241: Epoch: 0, Step: 340, Rank: 1, loss = 1.6869075298309326
nid001241: Epoch: 0, Step: 340, Rank: 3, loss = 1.6965810060501099
nid001241: Epoch: 0, Step: 340, Rank: 2, loss = 1.6325335502624512
nid001244: Epoch: 0, Step: 340, Rank: 6, loss = 1.7128193378448486
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 341, Rank: 7, loss = 1.4706913232803345
nid001244: Epoch: 0, Step: 341, Rank: 6, loss = 1.5020140409469604
nid001241: Epoch: 0, Step: 341, Rank: 0, loss = 1.2773977518081665
nid001244: Epoch: 0, Step: 341, Rank: 4, loss = 1.6993776559829712
nid001244: Epoch: 0, Step: 341, Rank: 5, loss = 1.4600964784622192
nid001241: Epoch: 0, Step: 341, Rank: 3, loss = 1.747079610824585
nid001241: Epoch: 0, Step: 341, Rank: 1, loss = 1.2528011798858643
nid001241: Epoch: 0, Step: 341, Rank: 2, loss = 1.6745156049728394
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 342, Rank: 4, loss = 1.6006652116775513
nid001244: Epoch: 0, Step: 342, Rank: 7, loss = 1.664778709411621
nid001244: Epoch: 0, Step: 342, Rank: 5, loss = 1.8577563762664795
nid001241: Epoch: 0, Step: 342, Rank: 0, loss = 1.7284828424453735
nid001241: Epoch: 0, Step: 342, Rank: 3, loss = 1.4396932125091553
nid001244: Epoch: 0, Step: 342, Rank: 6, loss = 1.487506628036499
nid001241: Epoch: 0, Step: 342, Rank: 1, loss = 1.6343939304351807
nid001241: Epoch: 0, Step: 342, Rank: 2, loss = 1.2718188762664795
nid001241: Model Parameters: 8.030 B, Latency: 6.97s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 343, Rank: 6, loss = 1.6740195751190186
nid001244: Epoch: 0, Step: 343, Rank: 7, loss = 1.516684889793396
nid001241: Epoch: 0, Step: 343, Rank: 2, loss = 1.4520753622055054
nid001241: Epoch: 0, Step: 343, Rank: 3, loss = 1.441699504852295
nid001241: Epoch: 0, Step: 343, Rank: 0, loss = 1.8152369260787964
nid001241: Epoch: 0, Step: 343, Rank: 1, loss = 1.6205172538757324
nid001244: Epoch: 0, Step: 343, Rank: 4, loss = 1.592044472694397
nid001244: Epoch: 0, Step: 343, Rank: 5, loss = 1.6419788599014282
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 344, Rank: 0, loss = 1.6398212909698486
nid001244: Epoch: 0, Step: 344, Rank: 4, loss = 1.3088737726211548
nid001244: Epoch: 0, Step: 344, Rank: 5, loss = 1.2884128093719482
nid001244: Epoch: 0, Step: 344, Rank: 7, loss = 1.5513674020767212
nid001241: Epoch: 0, Step: 344, Rank: 1, loss = 1.3153231143951416
nid001244: Epoch: 0, Step: 344, Rank: 6, loss = 1.3143130540847778
nid001241: Epoch: 0, Step: 344, Rank: 2, loss = 1.5120649337768555
nid001241: Epoch: 0, Step: 344, Rank: 3, loss = 1.5118745565414429
nid001241: Model Parameters: 8.030 B, Latency: 6.68s, TFLOPs: 4.36, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 345, Rank: 0, loss = 1.4719634056091309
nid001241: Epoch: 0, Step: 345, Rank: 1, loss = 1.6822720766067505
nid001241: Epoch: 0, Step: 345, Rank: 3, loss = 1.8060163259506226
nid001244: Epoch: 0, Step: 345, Rank: 4, loss = 1.43206787109375
nid001241: Epoch: 0, Step: 345, Rank: 2, loss = 1.506714105606079
nid001244: Epoch: 0, Step: 345, Rank: 7, loss = 1.7246754169464111
nid001244: Epoch: 0, Step: 345, Rank: 5, loss = 1.6605204343795776
nid001244: Epoch: 0, Step: 345, Rank: 6, loss = 1.637387990951538
nid001241: Model Parameters: 8.030 B, Latency: 6.70s, TFLOPs: 4.35, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 346, Rank: 5, loss = 1.651086449623108
nid001244: Epoch: 0, Step: 346, Rank: 6, loss = 1.6460667848587036
nid001244: Epoch: 0, Step: 346, Rank: 4, loss = 1.5613189935684204
nid001241: Epoch: 0, Step: 346, Rank: 1, loss = 1.8559011220932007
nid001241: Epoch: 0, Step: 346, Rank: 0, loss = 1.274883508682251
nid001241: Epoch: 0, Step: 346, Rank: 2, loss = 1.4746567010879517
nid001244: Epoch: 0, Step: 346, Rank: 7, loss = 1.60432767868042
nid001241: Epoch: 0, Step: 346, Rank: 3, loss = 1.9404292106628418
nid001241: Model Parameters: 8.030 B, Latency: 6.73s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 347, Rank: 7, loss = 1.708621621131897
nid001244: Epoch: 0, Step: 347, Rank: 6, loss = 1.5221246480941772
nid001241: Epoch: 0, Step: 347, Rank: 3, loss = 1.0670125484466553
nid001241: Epoch: 0, Step: 347, Rank: 2, loss = 1.5139930248260498
nid001241: Epoch: 0, Step: 347, Rank: 1, loss = 1.5677688121795654
nid001244: Epoch: 0, Step: 347, Rank: 5, loss = 1.6648523807525635
nid001241: Epoch: 0, Step: 347, Rank: 0, loss = 1.6754939556121826
nid001244: Epoch: 0, Step: 347, Rank: 4, loss = 1.5677827596664429
nid001241: Model Parameters: 8.030 B, Latency: 7.27s, TFLOPs: 4.00, Samples/sec: 0.55, Time/seq 1.82s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 348, Rank: 7, loss = 1.6680940389633179
nid001244: Epoch: 0, Step: 348, Rank: 6, loss = 1.485345482826233
nid001241: Epoch: 0, Step: 348, Rank: 3, loss = 1.7255127429962158
nid001241: Epoch: 0, Step: 348, Rank: 2, loss = 1.7586010694503784
nid001244: Epoch: 0, Step: 348, Rank: 5, loss = 1.8295806646347046
nid001241: Epoch: 0, Step: 348, Rank: 1, loss = 1.3466240167617798
nid001241: Epoch: 0, Step: 348, Rank: 0, loss = 1.6754697561264038
nid001244: Epoch: 0, Step: 348, Rank: 4, loss = 1.918419361114502
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 349, Rank: 3, loss = 1.4822381734848022
nid001241: Epoch: 0, Step: 349, Rank: 0, loss = 1.3073515892028809
nid001241: Epoch: 0, Step: 349, Rank: 1, loss = 1.569801926612854
nid001241: Epoch: 0, Step: 349, Rank: 2, loss = 1.6838818788528442
nid001244: Epoch: 0, Step: 349, Rank: 4, loss = 1.554546594619751
nid001244: Epoch: 0, Step: 349, Rank: 7, loss = 1.5826655626296997
nid001244: Epoch: 0, Step: 349, Rank: 5, loss = 1.696051836013794
nid001244: Epoch: 0, Step: 349, Rank: 6, loss = 1.289440393447876
nid001241: [2024-11-12 13:12:29,715] [INFO] [logging.py:128:log_dist] [Rank 0] step=350, skipped=5, lr=[9.307650826539092e-06, 9.307650826539092e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:12:29,716] [INFO] [timer.py:264:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=4.666730563530433, CurrSamplesPerSec=4.708634768713375, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 350, Rank: 4, loss = 1.630916714668274
nid001244: Epoch: 0, Step: 350, Rank: 7, loss = 1.5767159461975098
nid001244: Epoch: 0, Step: 350, Rank: 5, loss = 1.5807980298995972
nid001241: Epoch: 0, Step: 350, Rank: 0, loss = 1.6694285869598389
nid001241: Epoch: 0, Step: 350, Rank: 3, loss = 2.020787477493286
nid001241: Epoch: 0, Step: 350, Rank: 1, loss = 1.3846060037612915
nid001241: Epoch: 0, Step: 350, Rank: 2, loss = 1.7214932441711426
nid001244: Epoch: 0, Step: 350, Rank: 6, loss = 1.7088661193847656
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 351, Rank: 3, loss = 1.3913613557815552
nid001244: Epoch: 0, Step: 351, Rank: 7, loss = 1.461656093597412
nid001241: Epoch: 0, Step: 351, Rank: 2, loss = 1.5533630847930908
nid001244: Epoch: 0, Step: 351, Rank: 6, loss = 1.9078900814056396
nid001244: Epoch: 0, Step: 351, Rank: 5, loss = 1.6815320253372192
nid001244: Epoch: 0, Step: 351, Rank: 4, loss = 2.113626718521118
nid001241: Epoch: 0, Step: 351, Rank: 1, loss = 1.8362113237380981
nid001241: Epoch: 0, Step: 351, Rank: 0, loss = 1.5730961561203003
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 352, Rank: 6, loss = 1.8466111421585083
nid001241: Epoch: 0, Step: 352, Rank: 3, loss = 0.8280662298202515
nid001244: Epoch: 0, Step: 352, Rank: 7, loss = 1.220718502998352
nid001241: Epoch: 0, Step: 352, Rank: 1, loss = 1.3364267349243164
nid001241: Epoch: 0, Step: 352, Rank: 0, loss = 1.4982218742370605
nid001241: Epoch: 0, Step: 352, Rank: 2, loss = 1.5425817966461182
nid001244: Epoch: 0, Step: 352, Rank: 4, loss = 1.584115982055664
nid001244: Epoch: 0, Step: 352, Rank: 5, loss = 1.6230428218841553
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 353, Rank: 7, loss = 1.7683995962142944
nid001241: Epoch: 0, Step: 353, Rank: 0, loss = 1.3445287942886353
nid001244: Epoch: 0, Step: 353, Rank: 4, loss = 1.4857666492462158
nid001241: Epoch: 0, Step: 353, Rank: 3, loss = 1.9494370222091675
nid001241: Epoch: 0, Step: 353, Rank: 1, loss = 1.8214610815048218
nid001241: Epoch: 0, Step: 353, Rank: 2, loss = 1.4478671550750732
nid001244: Epoch: 0, Step: 353, Rank: 5, loss = 1.7046921253204346
nid001244: Epoch: 0, Step: 353, Rank: 6, loss = 1.5866960287094116
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 354, Rank: 4, loss = 1.383156418800354
nid001244: Epoch: 0, Step: 354, Rank: 7, loss = 1.4093098640441895
nid001241: Epoch: 0, Step: 354, Rank: 0, loss = 1.4577115774154663
nid001244: Epoch: 0, Step: 354, Rank: 5, loss = 1.8417388200759888
nid001241: Epoch: 0, Step: 354, Rank: 3, loss = 1.4653246402740479
nid001241: Epoch: 0, Step: 354, Rank: 1, loss = 1.830660343170166
nid001244: Epoch: 0, Step: 354, Rank: 6, loss = 0.9834078550338745
nid001241: Epoch: 0, Step: 354, Rank: 2, loss = 1.373946189880371
nid001241: Model Parameters: 8.030 B, Latency: 6.70s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 355, Rank: 6, loss = 1.3510946035385132
nid001244: Epoch: 0, Step: 355, Rank: 7, loss = 1.692205548286438
nid001241: Epoch: 0, Step: 355, Rank: 2, loss = 1.655975580215454
nid001241: Epoch: 0, Step: 355, Rank: 3, loss = 1.9077366590499878
nid001241: Epoch: 0, Step: 355, Rank: 0, loss = 1.5627449750900269
nid001244: Epoch: 0, Step: 355, Rank: 4, loss = 1.499617576599121
nid001241: Epoch: 0, Step: 355, Rank: 1, loss = 1.9663236141204834
nid001244: Epoch: 0, Step: 355, Rank: 5, loss = 1.6723040342330933
nid001241: Model Parameters: 8.030 B, Latency: 6.72s, TFLOPs: 4.33, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 356, Rank: 6, loss = 1.6493475437164307
nid001244: Epoch: 0, Step: 356, Rank: 7, loss = 1.4256494045257568
nid001241: Epoch: 0, Step: 356, Rank: 2, loss = 1.7981250286102295
nid001241: Epoch: 0, Step: 356, Rank: 3, loss = 1.6883314847946167
nid001241: Epoch: 0, Step: 356, Rank: 0, loss = 1.6012499332427979
nid001244: Epoch: 0, Step: 356, Rank: 5, loss = 1.2888617515563965
nid001241: Epoch: 0, Step: 356, Rank: 1, loss = 1.3130263090133667
nid001244: Epoch: 0, Step: 356, Rank: 4, loss = 1.9767974615097046
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 357, Rank: 6, loss = 1.73221755027771
nid001244: Epoch: 0, Step: 357, Rank: 7, loss = 1.3574161529541016
nid001241: Epoch: 0, Step: 357, Rank: 2, loss = 1.5785256624221802
nid001241: Epoch: 0, Step: 357, Rank: 3, loss = 1.7415478229522705
nid001244: Epoch: 0, Step: 357, Rank: 5, loss = 1.5348129272460938
nid001241: Epoch: 0, Step: 357, Rank: 0, loss = 1.5416721105575562
nid001241: Epoch: 0, Step: 357, Rank: 1, loss = 1.329761266708374
nid001244: Epoch: 0, Step: 357, Rank: 4, loss = 1.6733273267745972
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 358, Rank: 2, loss = 1.6041454076766968
nid001241: Epoch: 0, Step: 358, Rank: 3, loss = 1.7990200519561768
nid001244: Epoch: 0, Step: 358, Rank: 6, loss = 1.0336732864379883
nid001241: Epoch: 0, Step: 358, Rank: 0, loss = 1.5194650888442993
nid001244: Epoch: 0, Step: 358, Rank: 7, loss = 1.5021358728408813
nid001244: Epoch: 0, Step: 358, Rank: 4, loss = 1.366920828819275
nid001244: Epoch: 0, Step: 358, Rank: 5, loss = 1.3365522623062134
nid001241: Epoch: 0, Step: 358, Rank: 1, loss = 1.3987475633621216
nid001241: Model Parameters: 8.030 B, Latency: 6.87s, TFLOPs: 4.24, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 359, Rank: 0, loss = 1.8643250465393066
nid001241: Epoch: 0, Step: 359, Rank: 3, loss = 1.5416805744171143
nid001241: Epoch: 0, Step: 359, Rank: 1, loss = 1.3774784803390503
nid001241: Epoch: 0, Step: 359, Rank: 2, loss = 1.7998740673065186
nid001244: Epoch: 0, Step: 359, Rank: 7, loss = 1.5142574310302734
nid001244: Epoch: 0, Step: 359, Rank: 4, loss = 1.7029200792312622
nid001244: Epoch: 0, Step: 359, Rank: 5, loss = 1.4654659032821655
nid001244: Epoch: 0, Step: 359, Rank: 6, loss = 1.5251717567443848
nid001241: [2024-11-12 13:13:37,743] [INFO] [logging.py:128:log_dist] [Rank 0] step=360, skipped=5, lr=[9.287772546664411e-06, 9.287772546664411e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:13:37,743] [INFO] [timer.py:264:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=4.667857187559541, CurrSamplesPerSec=4.7485763521114706, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 360, Rank: 7, loss = 1.1989080905914307
nid001241: Epoch: 0, Step: 360, Rank: 2, loss = 1.4725536108016968
nid001244: Epoch: 0, Step: 360, Rank: 5, loss = 1.553973913192749
nid001244: Epoch: 0, Step: 360, Rank: 4, loss = 1.6030902862548828
nid001244: Epoch: 0, Step: 360, Rank: 6, loss = 1.6385465860366821
nid001241: Epoch: 0, Step: 360, Rank: 3, loss = 2.0350747108459473
nid001241: Epoch: 0, Step: 360, Rank: 0, loss = 1.6953601837158203
nid001241: Epoch: 0, Step: 360, Rank: 1, loss = 1.5266188383102417
nid001241: Model Parameters: 8.030 B, Latency: 6.80s, TFLOPs: 4.28, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 361, Rank: 3, loss = 1.1065402030944824
nid001241: Epoch: 0, Step: 361, Rank: 2, loss = 1.3713665008544922
nid001244: Epoch: 0, Step: 361, Rank: 6, loss = 1.8772046566009521
nid001244: Epoch: 0, Step: 361, Rank: 7, loss = 1.665764331817627
nid001241: Epoch: 0, Step: 361, Rank: 0, loss = 1.680874228477478
nid001241: Epoch: 0, Step: 361, Rank: 1, loss = 1.3307702541351318
nid001244: Epoch: 0, Step: 361, Rank: 5, loss = 1.7775338888168335
nid001244: Epoch: 0, Step: 361, Rank: 4, loss = 1.5096492767333984
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 362, Rank: 6, loss = 1.7449029684066772
nid001244: Epoch: 0, Step: 362, Rank: 7, loss = 1.5953539609909058
nid001241: Epoch: 0, Step: 362, Rank: 2, loss = 1.7681032419204712
nid001241: Epoch: 0, Step: 362, Rank: 3, loss = 1.524512767791748
nid001241: Epoch: 0, Step: 362, Rank: 0, loss = 1.8919316530227661
nid001241: Epoch: 0, Step: 362, Rank: 1, loss = 1.4991068840026855
nid001244: Epoch: 0, Step: 362, Rank: 4, loss = 1.4607752561569214
nid001244: Epoch: 0, Step: 362, Rank: 5, loss = 1.6707735061645508
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 363, Rank: 7, loss = 1.707052230834961
nid001244: Epoch: 0, Step: 363, Rank: 4, loss = 1.349541187286377
nid001241: Epoch: 0, Step: 363, Rank: 3, loss = 1.6423760652542114
nid001244: Epoch: 0, Step: 363, Rank: 5, loss = 1.6404247283935547
nid001241: Epoch: 0, Step: 363, Rank: 2, loss = 1.8138809204101562
nid001241: Epoch: 0, Step: 363, Rank: 0, loss = 1.7725335359573364
nid001244: Epoch: 0, Step: 363, Rank: 6, loss = 1.7115427255630493
nid001241: Epoch: 0, Step: 363, Rank: 1, loss = 1.4630441665649414
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 364, Rank: 3, loss = 1.8775829076766968
nid001241: Epoch: 0, Step: 364, Rank: 2, loss = 1.626662254333496
nid001241: Epoch: 0, Step: 364, Rank: 1, loss = 1.4718343019485474
nid001244: Epoch: 0, Step: 364, Rank: 7, loss = 1.4973433017730713
nid001244: Epoch: 0, Step: 364, Rank: 6, loss = 1.6299453973770142
nid001244: Epoch: 0, Step: 364, Rank: 5, loss = 1.5698131322860718
nid001241: Epoch: 0, Step: 364, Rank: 0, loss = 1.761003851890564
nid001244: Epoch: 0, Step: 364, Rank: 4, loss = 1.6052757501602173
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 365, Rank: 2, loss = 1.5634657144546509
nid001241: Epoch: 0, Step: 365, Rank: 3, loss = 1.3244630098342896
nid001244: Epoch: 0, Step: 365, Rank: 6, loss = 1.4339696168899536
nid001241: Epoch: 0, Step: 365, Rank: 0, loss = 1.5916223526000977
nid001244: Epoch: 0, Step: 365, Rank: 7, loss = 1.438271403312683
nid001244: Epoch: 0, Step: 365, Rank: 4, loss = 0.9888467192649841
nid001244: Epoch: 0, Step: 365, Rank: 5, loss = 1.7854537963867188
nid001241: Epoch: 0, Step: 365, Rank: 1, loss = 1.5109752416610718
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 366, Rank: 7, loss = 1.37010657787323
nid001244: Epoch: 0, Step: 366, Rank: 6, loss = 1.28220796585083
nid001244: Epoch: 0, Step: 366, Rank: 5, loss = 1.4932422637939453
nid001241: Epoch: 0, Step: 366, Rank: 2, loss = 1.1598823070526123
nid001241: Epoch: 0, Step: 366, Rank: 3, loss = 1.6375030279159546
nid001244: Epoch: 0, Step: 366, Rank: 4, loss = 1.6870765686035156
nid001241: Epoch: 0, Step: 366, Rank: 1, loss = 1.8937448263168335
nid001241: Epoch: 0, Step: 366, Rank: 0, loss = 1.3149446249008179
nid001241: Model Parameters: 8.030 B, Latency: 6.75s, TFLOPs: 4.31, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 367, Rank: 6, loss = 1.560414433479309
nid001244: Epoch: 0, Step: 367, Rank: 7, loss = 1.545334815979004
nid001244: Epoch: 0, Step: 367, Rank: 4, loss = 1.9371206760406494
nid001241: Epoch: 0, Step: 367, Rank: 2, loss = 1.3766857385635376
nid001244: Epoch: 0, Step: 367, Rank: 5, loss = 1.6862181425094604
nid001241: Epoch: 0, Step: 367, Rank: 3, loss = 1.449906587600708
nid001241: Epoch: 0, Step: 367, Rank: 0, loss = 1.1995717287063599
nid001241: Epoch: 0, Step: 367, Rank: 1, loss = 1.3990559577941895
nid001241: Model Parameters: 8.030 B, Latency: 6.79s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 368, Rank: 3, loss = 1.7100650072097778
nid001244: Epoch: 0, Step: 368, Rank: 7, loss = 0.9834451675415039
nid001241: Epoch: 0, Step: 368, Rank: 1, loss = 1.8058843612670898
nid001241: Epoch: 0, Step: 368, Rank: 2, loss = 1.5421515703201294
nid001244: Epoch: 0, Step: 368, Rank: 6, loss = 1.4955761432647705
nid001244: Epoch: 0, Step: 368, Rank: 5, loss = 1.4004381895065308
nid001241: Epoch: 0, Step: 368, Rank: 0, loss = 1.8807077407836914
nid001244: Epoch: 0, Step: 368, Rank: 4, loss = 1.4448027610778809
nid001241: Model Parameters: 8.030 B, Latency: 6.89s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 369, Rank: 7, loss = 1.269472599029541
nid001241: Epoch: 0, Step: 369, Rank: 3, loss = 1.366559624671936
nid001241: Epoch: 0, Step: 369, Rank: 2, loss = 1.5289775133132935
nid001244: Epoch: 0, Step: 369, Rank: 5, loss = 1.237016201019287
nid001244: Epoch: 0, Step: 369, Rank: 6, loss = 1.6603397130966187
nid001244: Epoch: 0, Step: 369, Rank: 4, loss = 1.8402661085128784
nid001241: Epoch: 0, Step: 369, Rank: 1, loss = 1.561557412147522
nid001241: Epoch: 0, Step: 369, Rank: 0, loss = 1.804127812385559
nid001241: [2024-11-12 13:14:46,213] [INFO] [logging.py:128:log_dist] [Rank 0] step=370, skipped=5, lr=[9.267355788665073e-06, 9.267355788665073e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:14:46,213] [INFO] [timer.py:264:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=4.668102951479595, CurrSamplesPerSec=4.730540110892094, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.77s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 370, Rank: 7, loss = 1.9127877950668335
nid001241: Epoch: 0, Step: 370, Rank: 0, loss = 1.6936380863189697
nid001244: Epoch: 0, Step: 370, Rank: 4, loss = 1.8367058038711548
nid001241: Epoch: 0, Step: 370, Rank: 3, loss = 1.4207968711853027
nid001241: Epoch: 0, Step: 370, Rank: 2, loss = 1.7638752460479736
nid001241: Epoch: 0, Step: 370, Rank: 1, loss = 1.9202890396118164
nid001244: Epoch: 0, Step: 370, Rank: 5, loss = 1.8276655673980713
nid001244: Epoch: 0, Step: 370, Rank: 6, loss = 1.4411282539367676
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 371, Rank: 5, loss = 1.5062743425369263
nid001244: Epoch: 0, Step: 371, Rank: 4, loss = 1.6563096046447754
nid001241: Epoch: 0, Step: 371, Rank: 1, loss = 1.8931516408920288
nid001241: Epoch: 0, Step: 371, Rank: 0, loss = 1.6388558149337769
nid001244: Epoch: 0, Step: 371, Rank: 6, loss = 1.3676742315292358
nid001241: Epoch: 0, Step: 371, Rank: 2, loss = 1.306532859802246
nid001241: Epoch: 0, Step: 371, Rank: 3, loss = 1.77644681930542
nid001244: Epoch: 0, Step: 371, Rank: 7, loss = 1.8744196891784668
nid001241: Model Parameters: 8.030 B, Latency: 6.81s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 372, Rank: 2, loss = 1.8208067417144775
nid001244: Epoch: 0, Step: 372, Rank: 6, loss = 1.677268385887146
nid001244: Epoch: 0, Step: 372, Rank: 4, loss = 1.7629913091659546
nid001241: Epoch: 0, Step: 372, Rank: 3, loss = 1.8082549571990967
nid001244: Epoch: 0, Step: 372, Rank: 7, loss = 1.6216800212860107
nid001241: Epoch: 0, Step: 372, Rank: 0, loss = 1.645206093788147
nid001241: Epoch: 0, Step: 372, Rank: 1, loss = 1.5517711639404297
nid001244: Epoch: 0, Step: 372, Rank: 5, loss = 1.7180688381195068
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 373, Rank: 2, loss = 1.4041681289672852
nid001241: Epoch: 0, Step: 373, Rank: 3, loss = 1.554044246673584
nid001244: Epoch: 0, Step: 373, Rank: 6, loss = 1.4858580827713013
nid001241: Epoch: 0, Step: 373, Rank: 0, loss = 1.5508930683135986
nid001244: Epoch: 0, Step: 373, Rank: 7, loss = 1.4995559453964233
nid001241: Epoch: 0, Step: 373, Rank: 1, loss = 1.4289244413375854
nid001244: Epoch: 0, Step: 373, Rank: 5, loss = 1.7406067848205566
nid001244: Epoch: 0, Step: 373, Rank: 4, loss = 1.4541689157485962
nid001241: Model Parameters: 8.030 B, Latency: 6.71s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.68s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 374, Rank: 7, loss = 1.555025577545166
nid001244: Epoch: 0, Step: 374, Rank: 6, loss = 1.9144341945648193
nid001241: Epoch: 0, Step: 374, Rank: 3, loss = 1.3923829793930054
nid001244: Epoch: 0, Step: 374, Rank: 5, loss = 1.5036011934280396
nid001241: Epoch: 0, Step: 374, Rank: 2, loss = 1.2723854780197144
nid001244: Epoch: 0, Step: 374, Rank: 4, loss = 1.2200770378112793
nid001241: Epoch: 0, Step: 374, Rank: 1, loss = 1.5983308553695679
nid001241: Epoch: 0, Step: 374, Rank: 0, loss = 1.2216589450836182
nid001241: Model Parameters: 8.030 B, Latency: 7.03s, TFLOPs: 4.14, Samples/sec: 0.57, Time/seq 1.76s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 375, Rank: 7, loss = 1.4806336164474487
nid001244: Epoch: 0, Step: 375, Rank: 5, loss = 1.6943604946136475
nid001244: Epoch: 0, Step: 375, Rank: 6, loss = 1.7572070360183716
nid001241: Epoch: 0, Step: 375, Rank: 3, loss = 1.5545287132263184
nid001244: Epoch: 0, Step: 375, Rank: 4, loss = 1.7874857187271118
nid001241: Epoch: 0, Step: 375, Rank: 2, loss = 1.8266432285308838
nid001241: Epoch: 0, Step: 375, Rank: 1, loss = 1.7699460983276367
nid001241: Epoch: 0, Step: 375, Rank: 0, loss = 1.364896535873413
nid001241: Model Parameters: 8.030 B, Latency: 6.97s, TFLOPs: 4.18, Samples/sec: 0.57, Time/seq 1.74s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 376, Rank: 7, loss = 1.6589573621749878
nid001244: Epoch: 0, Step: 376, Rank: 6, loss = 1.7693032026290894
nid001244: Epoch: 0, Step: 376, Rank: 5, loss = 1.3957139253616333
nid001241: Epoch: 0, Step: 376, Rank: 3, loss = 1.5439956188201904
nid001241: Epoch: 0, Step: 376, Rank: 2, loss = 1.4553859233856201
nid001241: Epoch: 0, Step: 376, Rank: 1, loss = 1.453676700592041
nid001241: Epoch: 0, Step: 376, Rank: 0, loss = 1.6515780687332153
nid001244: Epoch: 0, Step: 376, Rank: 4, loss = 1.1904995441436768
nid001241: Model Parameters: 8.030 B, Latency: 7.02s, TFLOPs: 4.15, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 377, Rank: 4, loss = 1.5060105323791504
nid001244: Epoch: 0, Step: 377, Rank: 7, loss = 1.5044946670532227
nid001241: Epoch: 0, Step: 377, Rank: 0, loss = 1.4733240604400635
nid001241: Epoch: 0, Step: 377, Rank: 3, loss = 1.7487382888793945
nid001244: Epoch: 0, Step: 377, Rank: 5, loss = 1.5812841653823853
nid001241: Epoch: 0, Step: 377, Rank: 1, loss = 1.7824954986572266
nid001241: Epoch: 0, Step: 377, Rank: 2, loss = 1.9759992361068726
nid001244: Epoch: 0, Step: 377, Rank: 6, loss = 1.4198336601257324
nid001241: Model Parameters: 8.030 B, Latency: 6.78s, TFLOPs: 4.29, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 378, Rank: 2, loss = 1.5290820598602295
nid001244: Epoch: 0, Step: 378, Rank: 7, loss = 1.8117337226867676
nid001244: Epoch: 0, Step: 378, Rank: 5, loss = 1.650782585144043
nid001244: Epoch: 0, Step: 378, Rank: 4, loss = 1.4690673351287842Epoch: 0, Step: 378, Rank: 6, loss = 1.4707980155944824
nid001244: 
nid001241: Epoch: 0, Step: 378, Rank: 3, loss = 1.7859166860580444
nid001241: Epoch: 0, Step: 378, Rank: 0, loss = 1.4414865970611572
nid001241: Epoch: 0, Step: 378, Rank: 1, loss = 1.0448658466339111
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 379, Rank: 3, loss = 1.6202994585037231
nid001241: Epoch: 0, Step: 379, Rank: 2, loss = 1.2122702598571777
nid001241: Epoch: 0, Step: 379, Rank: 1, loss = 1.7160179615020752
nid001244: Epoch: 0, Step: 379, Rank: 7, loss = 1.8586913347244263
nid001244: Epoch: 0, Step: 379, Rank: 6, loss = 1.3628332614898682
nid001241: Epoch: 0, Step: 379, Rank: 0, loss = 1.511033058166504
nid001244: Epoch: 0, Step: 379, Rank: 5, loss = 1.5576982498168945
nid001244: Epoch: 0, Step: 379, Rank: 4, loss = 1.5948922634124756
nid001241: [2024-11-12 13:15:54,907] [INFO] [logging.py:128:log_dist] [Rank 0] step=380, skipped=5, lr=[9.24640301602705e-06, 9.24640301602705e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:15:54,907] [INFO] [timer.py:264:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=4.667935016103569, CurrSamplesPerSec=4.652730531967589, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 380, Rank: 2, loss = 1.6679805517196655
nid001241: Epoch: 0, Step: 380, Rank: 3, loss = 1.3274039030075073
nid001241: Epoch: 0, Step: 380, Rank: 0, loss = 1.367600679397583
nid001244: Epoch: 0, Step: 380, Rank: 6, loss = 1.6144888401031494
nid001244: Epoch: 0, Step: 380, Rank: 7, loss = 1.8399873971939087
nid001241: Epoch: 0, Step: 380, Rank: 1, loss = 1.56990385055542
nid001244: Epoch: 0, Step: 380, Rank: 4, loss = 1.2008044719696045
nid001244: Epoch: 0, Step: 380, Rank: 5, loss = 1.8510857820510864
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 381, Rank: 4, loss = 1.3769017457962036
nid001241: Epoch: 0, Step: 381, Rank: 3, loss = 1.4912210702896118
nid001241: Epoch: 0, Step: 381, Rank: 0, loss = 1.5243771076202393
nid001244: Epoch: 0, Step: 381, Rank: 7, loss = 1.8761518001556396
nid001241: Epoch: 0, Step: 381, Rank: 1, loss = 1.7199593782424927
nid001241: Epoch: 0, Step: 381, Rank: 2, loss = 1.6685206890106201
nid001244: Epoch: 0, Step: 381, Rank: 5, loss = 1.4681720733642578
nid001244: Epoch: 0, Step: 381, Rank: 6, loss = 1.123353123664856
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 382, Rank: 7, loss = 1.800641417503357
nid001244: Epoch: 0, Step: 382, Rank: 5, loss = 1.4890992641448975
nid001244: Epoch: 0, Step: 382, Rank: 6, loss = 1.3198705911636353
nid001241: Epoch: 0, Step: 382, Rank: 3, loss = 1.7090448141098022
nid001241: Epoch: 0, Step: 382, Rank: 2, loss = 1.794344186782837
nid001244: Epoch: 0, Step: 382, Rank: 4, loss = 1.426672339439392
nid001241: Epoch: 0, Step: 382, Rank: 1, loss = 1.3590941429138184
nid001241: Epoch: 0, Step: 382, Rank: 0, loss = 1.0316828489303589
nid001241: Model Parameters: 8.030 B, Latency: 7.13s, TFLOPs: 4.08, Samples/sec: 0.56, Time/seq 1.78s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 383, Rank: 5, loss = 1.8452067375183105
nid001244: Epoch: 0, Step: 383, Rank: 4, loss = 1.3000104427337646
nid001244: Epoch: 0, Step: 383, Rank: 6, loss = 1.6660975217819214
nid001241: Epoch: 0, Step: 383, Rank: 1, loss = 1.5220372676849365
nid001244: Epoch: 0, Step: 383, Rank: 7, loss = 1.7054989337921143
nid001241: Epoch: 0, Step: 383, Rank: 0, loss = 1.5984959602355957Epoch: 0, Step: 383, Rank: 2, loss = 1.2488932609558105
nid001241: 
nid001241: Epoch: 0, Step: 383, Rank: 3, loss = 1.9470109939575195
nid001241: Model Parameters: 8.030 B, Latency: 6.93s, TFLOPs: 4.20, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 384, Rank: 2, loss = 1.4778186082839966
nid001241: Epoch: 0, Step: 384, Rank: 3, loss = 1.4284111261367798
nid001244: Epoch: 0, Step: 384, Rank: 6, loss = 1.6686190366744995
nid001244: Epoch: 0, Step: 384, Rank: 7, loss = 1.184662103652954
nid001241: Epoch: 0, Step: 384, Rank: 0, loss = 1.4145610332489014
nid001244: Epoch: 0, Step: 384, Rank: 5, loss = 1.7450249195098877
nid001244: Epoch: 0, Step: 384, Rank: 4, loss = 1.7209490537643433
nid001241: Epoch: 0, Step: 384, Rank: 1, loss = 1.461190104484558
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 385, Rank: 5, loss = 1.9312942028045654
nid001244: Epoch: 0, Step: 385, Rank: 4, loss = 1.8423537015914917
nid001244: Epoch: 0, Step: 385, Rank: 6, loss = 1.7230089902877808
nid001241: Epoch: 0, Step: 385, Rank: 1, loss = 1.5578844547271729
nid001241: Epoch: 0, Step: 385, Rank: 0, loss = 1.4717031717300415
nid001241: Epoch: 0, Step: 385, Rank: 2, loss = 1.6120554208755493
nid001244: Epoch: 0, Step: 385, Rank: 7, loss = 1.2479642629623413
nid001241: Epoch: 0, Step: 385, Rank: 3, loss = 1.4237446784973145
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 386, Rank: 3, loss = 1.578336477279663
nid001241: Epoch: 0, Step: 386, Rank: 2, loss = 1.522179365158081
nid001241: Epoch: 0, Step: 386, Rank: 1, loss = 1.7906684875488281
nid001244: Epoch: 0, Step: 386, Rank: 7, loss = 1.3535435199737549
nid001241: Epoch: 0, Step: 386, Rank: 0, loss = 1.5967401266098022
nid001244: Epoch: 0, Step: 386, Rank: 6, loss = 1.5918190479278564
nid001244: Epoch: 0, Step: 386, Rank: 5, loss = 1.5731498003005981
nid001244: Epoch: 0, Step: 386, Rank: 4, loss = 1.4060096740722656
nid001241: Model Parameters: 8.030 B, Latency: 6.98s, TFLOPs: 4.17, Samples/sec: 0.57, Time/seq 1.75s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 387, Rank: 3, loss = 1.6178722381591797
nid001244: Epoch: 0, Step: 387, Rank: 7, loss = 1.914144515991211
nid001244: Epoch: 0, Step: 387, Rank: 6, loss = 1.284443974494934
nid001241: Epoch: 0, Step: 387, Rank: 1, loss = 1.9541620016098022
nid001241: Epoch: 0, Step: 387, Rank: 2, loss = 1.4667030572891235
nid001241: Epoch: 0, Step: 387, Rank: 0, loss = 1.5759837627410889
nid001244: Epoch: 0, Step: 387, Rank: 4, loss = 1.8068411350250244
nid001244: Epoch: 0, Step: 387, Rank: 5, loss = 1.8328278064727783
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 388, Rank: 7, loss = 1.6251925230026245
nid001244: Epoch: 0, Step: 388, Rank: 6, loss = 1.4840471744537354
nid001241: Epoch: 0, Step: 388, Rank: 3, loss = 1.3274388313293457
nid001241: Epoch: 0, Step: 388, Rank: 2, loss = 1.5442029237747192
nid001244: Epoch: 0, Step: 388, Rank: 5, loss = 1.7143229246139526
nid001241: Epoch: 0, Step: 388, Rank: 1, loss = 1.4574204683303833
nid001241: Epoch: 0, Step: 388, Rank: 0, loss = 1.7150120735168457
nid001244: Epoch: 0, Step: 388, Rank: 4, loss = 1.2223612070083618
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 389, Rank: 7, loss = 1.7438859939575195
nid001244: Epoch: 0, Step: 389, Rank: 4, loss = 1.7902708053588867
nid001244: Epoch: 0, Step: 389, Rank: 5, loss = 1.742469072341919
nid001244: Epoch: 0, Step: 389, Rank: 6, loss = 1.536213994026184
nid001241: Epoch: 0, Step: 389, Rank: 0, loss = 1.356797218322754
nid001241: Epoch: 0, Step: 389, Rank: 3, loss = 1.251592993736267
nid001241: Epoch: 0, Step: 389, Rank: 1, loss = 1.5482009649276733
nid001241: Epoch: 0, Step: 389, Rank: 2, loss = 1.5841768980026245
nid001241: [2024-11-12 13:17:03,850] [INFO] [logging.py:128:log_dist] [Rank 0] step=390, skipped=5, lr=[9.224916756911836e-06, 9.224916756911836e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:17:03,851] [INFO] [timer.py:264:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=4.667343100816396, CurrSamplesPerSec=4.682236631946968, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 390, Rank: 6, loss = 1.5082472562789917
nid001244: Epoch: 0, Step: 390, Rank: 7, loss = 1.388322353363037
nid001241: Epoch: 0, Step: 390, Rank: 2, loss = 1.4363223314285278
nid001244: Epoch: 0, Step: 390, Rank: 4, loss = 1.561219573020935
nid001241: Epoch: 0, Step: 390, Rank: 3, loss = 1.7135151624679565
nid001244: Epoch: 0, Step: 390, Rank: 5, loss = 1.3196465969085693
nid001241: Epoch: 0, Step: 390, Rank: 0, loss = 1.432238221168518
nid001241: Epoch: 0, Step: 390, Rank: 1, loss = 1.3745976686477661
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 391, Rank: 2, loss = 1.429343581199646
nid001244: Epoch: 0, Step: 391, Rank: 6, loss = 1.528201937675476
nid001244: Epoch: 0, Step: 391, Rank: 7, loss = 1.6094136238098145
nid001244: Epoch: 0, Step: 391, Rank: 4, loss = 1.85096275806427
nid001241: Epoch: 0, Step: 391, Rank: 3, loss = 1.5495109558105469
nid001241: Epoch: 0, Step: 391, Rank: 0, loss = 1.7068170309066772
nid001244: Epoch: 0, Step: 391, Rank: 5, loss = 1.6241365671157837
nid001241: Epoch: 0, Step: 391, Rank: 1, loss = 1.4217162132263184
nid001241: Model Parameters: 8.030 B, Latency: 6.85s, TFLOPs: 4.25, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 392, Rank: 7, loss = 1.5595015287399292
nid001244: Epoch: 0, Step: 392, Rank: 4, loss = 1.4582687616348267
nid001244: Epoch: 0, Step: 392, Rank: 5, loss = 1.5032925605773926
nid001244: Epoch: 0, Step: 392, Rank: 6, loss = 1.6192679405212402
nid001241: Epoch: 0, Step: 392, Rank: 0, loss = 1.476685881614685
nid001241: Epoch: 0, Step: 392, Rank: 3, loss = 1.8317549228668213
nid001241: Epoch: 0, Step: 392, Rank: 1, loss = 1.4820972681045532
nid001241: Epoch: 0, Step: 392, Rank: 2, loss = 1.3138982057571411
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 393, Rank: 6, loss = 1.7466179132461548
nid001244: Epoch: 0, Step: 393, Rank: 7, loss = 1.563681960105896
nid001241: Epoch: 0, Step: 393, Rank: 2, loss = 1.4482377767562866
nid001241: Epoch: 0, Step: 393, Rank: 3, loss = 1.710152268409729
nid001244: Epoch: 0, Step: 393, Rank: 4, loss = 1.7535085678100586
nid001244: Epoch: 0, Step: 393, Rank: 5, loss = 1.3310056924819946
nid001241: Epoch: 0, Step: 393, Rank: 0, loss = 1.4660851955413818
nid001241: Epoch: 0, Step: 393, Rank: 1, loss = 1.8591347932815552
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 394, Rank: 2, loss = 1.8403500318527222
nid001244: Epoch: 0, Step: 394, Rank: 6, loss = 1.623738408088684
nid001241: Epoch: 0, Step: 394, Rank: 3, loss = 1.3136392831802368
nid001241: Epoch: 0, Step: 394, Rank: 0, loss = 1.1825473308563232
nid001241: Epoch: 0, Step: 394, Rank: 1, loss = 1.6478632688522339
nid001244: Epoch: 0, Step: 394, Rank: 7, loss = 1.5766240358352661
nid001244: Epoch: 0, Step: 394, Rank: 4, loss = 1.0044547319412231
nid001244: Epoch: 0, Step: 394, Rank: 5, loss = 1.6978023052215576
nid001241: Model Parameters: 8.030 B, Latency: 6.91s, TFLOPs: 4.21, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 395, Rank: 4, loss = 1.4694042205810547
nid001244: Epoch: 0, Step: 395, Rank: 5, loss = 1.5651112794876099
nid001244: Epoch: 0, Step: 395, Rank: 6, loss = 1.6252291202545166
nid001241: Epoch: 0, Step: 395, Rank: 1, loss = 1.5073673725128174
nid001241: Epoch: 0, Step: 395, Rank: 0, loss = 1.6664297580718994
nid001244: Epoch: 0, Step: 395, Rank: 7, loss = 1.63572359085083
nid001241: Epoch: 0, Step: 395, Rank: 2, loss = 1.7622861862182617
nid001241: Epoch: 0, Step: 395, Rank: 3, loss = 1.680912971496582
nid001241: Model Parameters: 8.030 B, Latency: 6.83s, TFLOPs: 4.26, Samples/sec: 0.59, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 396, Rank: 7, loss = 1.2210780382156372
nid001244: Epoch: 0, Step: 396, Rank: 4, loss = 1.486175298690796
nid001241: Epoch: 0, Step: 396, Rank: 3, loss = 1.5447757244110107
nid001241: Epoch: 0, Step: 396, Rank: 2, loss = 1.421376347541809
nid001241: Epoch: 0, Step: 396, Rank: 0, loss = 1.6070218086242676
nid001241: Epoch: 0, Step: 396, Rank: 1, loss = 1.6514830589294434
nid001244: Epoch: 0, Step: 396, Rank: 5, loss = 1.6959902048110962
nid001244: Epoch: 0, Step: 396, Rank: 6, loss = 1.2502261400222778
nid001241: Model Parameters: 8.030 B, Latency: 6.76s, TFLOPs: 4.30, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 397, Rank: 7, loss = 1.297155499458313
nid001244: Epoch: 0, Step: 397, Rank: 6, loss = 1.874726414680481
nid001241: Epoch: 0, Step: 397, Rank: 3, loss = 1.7719573974609375
nid001241: Epoch: 0, Step: 397, Rank: 2, loss = 1.3994678258895874
nid001241: Epoch: 0, Step: 397, Rank: 1, loss = 1.3467357158660889
nid001241: Epoch: 0, Step: 397, Rank: 0, loss = 1.330984115600586
nid001244: Epoch: 0, Step: 397, Rank: 5, loss = 1.8282476663589478
nid001244: Epoch: 0, Step: 397, Rank: 4, loss = 1.7039822340011597
nid001241: Model Parameters: 8.030 B, Latency: 6.88s, TFLOPs: 4.23, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 398, Rank: 1, loss = 1.6443016529083252
nid001241: Epoch: 0, Step: 398, Rank: 0, loss = 1.5800330638885498
nid001241: Epoch: 0, Step: 398, Rank: 2, loss = 1.4020134210586548
nid001244: Epoch: 0, Step: 398, Rank: 4, loss = 1.5120445489883423
nid001244: Epoch: 0, Step: 398, Rank: 5, loss = 1.4999865293502808
nid001241: Epoch: 0, Step: 398, Rank: 3, loss = 1.6670701503753662
nid001244: Epoch: 0, Step: 398, Rank: 7, loss = 1.854742407798767
nid001244: Epoch: 0, Step: 398, Rank: 6, loss = 1.6216061115264893
nid001241: Model Parameters: 8.030 B, Latency: 6.74s, TFLOPs: 4.32, Samples/sec: 0.59, Time/seq 1.69s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 399, Rank: 2, loss = 1.2645071744918823
nid001241: Epoch: 0, Step: 399, Rank: 3, loss = 1.2622809410095215
nid001244: Epoch: 0, Step: 399, Rank: 7, loss = 1.4210059642791748
nid001244: Epoch: 0, Step: 399, Rank: 4, loss = 1.692526936531067
nid001244: Epoch: 0, Step: 399, Rank: 5, loss = 1.2328293323516846
nid001241: Epoch: 0, Step: 399, Rank: 0, loss = 1.6262108087539673
nid001244: Epoch: 0, Step: 399, Rank: 6, loss = 1.6576251983642578
nid001241: Epoch: 0, Step: 399, Rank: 1, loss = 1.6533777713775635
nid001241: [2024-11-12 13:18:12,122] [INFO] [logging.py:128:log_dist] [Rank 0] step=400, skipped=5, lr=[9.202899603851403e-06, 9.202899603851403e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
nid001241: [2024-11-12 13:18:12,123] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=4.667920516232669, CurrSamplesPerSec=4.779363137878829, MemAllocated=16.89GB, MaxMemAllocated=25.91GB
nid001241: Model Parameters: 8.030 B, Latency: 6.70s, TFLOPs: 4.34, Samples/sec: 0.60, Time/seq 1.67s, Batch Size: 4, Sequence Length: 1024
nid001241: Epoch: 0, Step: 400, Rank: 3, loss = 1.745560884475708
nid001241: Epoch: 0, Step: 400, Rank: 0, loss = 1.5323370695114136
nid001241: Epoch: 0, Step: 400, Rank: 1, loss = 1.734980821609497
nid001244: Epoch: 0, Step: 400, Rank: 4, loss = 1.5524123907089233
nid001244: Epoch: 0, Step: 400, Rank: 7, loss = 1.4171829223632812
nid001241: Epoch: 0, Step: 400, Rank: 2, loss = 1.7867155075073242
nid001244: Epoch: 0, Step: 400, Rank: 5, loss = 1.847511887550354
nid001244: Epoch: 0, Step: 400, Rank: 6, loss = 1.3270878791809082
nid001241: Model Parameters: 8.030 B, Latency: 6.82s, TFLOPs: 4.27, Samples/sec: 0.59, Time/seq 1.70s, Batch Size: 4, Sequence Length: 1024
nid001241: Step 400: GPU Memory Usage
nid001241: GPU 0 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36862 MB
nid001241:   Free:  4097 MB
nid001241:   Usage: 36862/40960 MB (90.00%)
nid001241: 
nid001241: GPU 1 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36862 MB
nid001241:   Free:  4097 MB
nid001241:   Usage: 36862/40960 MB (90.00%)
nid001241: 
nid001241: GPU 2 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36886 MB
nid001241:   Free:  4073 MB
nid001241:   Usage: 36886/40960 MB (90.05%)
nid001241: 
nid001241: GPU 3 Memory Usage:
nid001241:   Total: 40960 MB
nid001241:   Used:  36838 MB
nid001241:   Free:  4121 MB
nid001241:   Usage: 36838/40960 MB (89.94%)
nid001241: 
nid001244: Epoch: 0, Step: 401, Rank: 6, loss = 1.3192158937454224
nid001244: Epoch: 0, Step: 401, Rank: 7, loss = 1.6047170162200928
nid001241: Epoch: 0, Step: 401, Rank: 2, loss = 1.5898175239562988
nid001241: Epoch: 0, Step: 401, Rank: 3, loss = 1.5093271732330322
nid001244: Epoch: 0, Step: 401, Rank: 4, loss = 1.6844402551651
nid001241: Epoch: 0, Step: 401, Rank: 1, loss = 1.7016000747680664
nid001241: Epoch: 0, Step: 401, Rank: 0, loss = 1.69877290725708
nid001244: Epoch: 0, Step: 401, Rank: 5, loss = 1.413615107536316
nid001241: Model Parameters: 8.030 B, Latency: 6.84s, TFLOPs: 4.26, Samples/sec: 0.58, Time/seq 1.71s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 402, Rank: 7, loss = 1.336665153503418
nid001244: Epoch: 0, Step: 402, Rank: 5, loss = 1.5367649793624878
nid001244: Epoch: 0, Step: 402, Rank: 6, loss = 1.7520289421081543
nid001244: Epoch: 0, Step: 402, Rank: 4, loss = 1.1411349773406982
nid001241: Epoch: 0, Step: 402, Rank: 3, loss = 1.4510717391967773
nid001241: Epoch: 0, Step: 402, Rank: 2, loss = 1.574175238609314
nid001241: Epoch: 0, Step: 402, Rank: 1, loss = 1.740475058555603
nid001241: Epoch: 0, Step: 402, Rank: 0, loss = 1.5963261127471924
nid001241: Model Parameters: 8.030 B, Latency: 6.94s, TFLOPs: 4.19, Samples/sec: 0.58, Time/seq 1.73s, Batch Size: 4, Sequence Length: 1024
nid001244: Epoch: 0, Step: 403, Rank: 7, loss = 2.084561347961426
nid001244: Epoch: 0, Step: 403, Rank: 6, loss = 1.5711473226547241
nid001244: Epoch: 0, Step: 403, Rank: 5, loss = 1.4009711742401123
nid001241: Epoch: 0, Step: 403, Rank: 3, loss = 1.5270884037017822
nid001244: Epoch: 0, Step: 403, Rank: 4, loss = 1.903656244277954
nid001241: Epoch: 0, Step: 403, Rank: 2, loss = 1.8548009395599365
nid001241: Epoch: 0, Step: 403, Rank: 1, loss = 1.5634815692901611
nid001241: Epoch: 0, Step: 403, Rank: 0, loss = 1.7643615007400513
nid001244: Connection to nid001244 closed by remote host.
nid001241: Connection to nid001241 closed by remote host.
pdsh@nid001241: nid001241: ssh exited with exit code 255
pdsh@nid001241: nid001244: ssh exited with exit code 255
